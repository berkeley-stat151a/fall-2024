<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Multilinear regression as loss minimization.</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-MMK2VCM6EW"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-MMK2VCM6EW', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Multilinear regression as loss minimization.</li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../stat_bear.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/berkeley-stat151a/fall-2024" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course_policies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Policies</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lectures</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../assignments/assignments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assignments</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../datasets/data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Datasets</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../quizzes/quizzes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quizzes</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#goals" id="toc-goals" class="nav-link active" data-scroll-target="#goals">Goals</a></li>
  <li><a href="#siomple-least-squares" id="toc-siomple-least-squares" class="nav-link" data-scroll-target="#siomple-least-squares">Siomple least squares</a></li>
  <li><a href="#simple-least-squares-estimator-derivation" id="toc-simple-least-squares-estimator-derivation" class="nav-link" data-scroll-target="#simple-least-squares-estimator-derivation">Simple least squares estimator derivation</a></li>
  <li><a href="#matrix-multiplication-version" id="toc-matrix-multiplication-version" class="nav-link" data-scroll-target="#matrix-multiplication-version">Matrix multiplication version</a></li>
  <li><a href="#matrix-notation" id="toc-matrix-notation" class="nav-link" data-scroll-target="#matrix-notation">Matrix notation</a></li>
  <li><a href="#least-squares-in-matrix-notation" id="toc-least-squares-in-matrix-notation" class="nav-link" data-scroll-target="#least-squares-in-matrix-notation">Least squares in matrix notation</a></li>
  <li><a href="#some-examples" id="toc-some-examples" class="nav-link" data-scroll-target="#some-examples">Some examples</a>
  <ul class="collapse">
  <li><a href="#the-sample-mean" id="toc-the-sample-mean" class="nav-link" data-scroll-target="#the-sample-mean">The sample mean</a></li>
  <li><a href="#a-single-regressor" id="toc-a-single-regressor" class="nav-link" data-scroll-target="#a-single-regressor">A single regressor</a></li>
  <li><a href="#onehot-encodings" id="toc-onehot-encodings" class="nav-link" data-scroll-target="#onehot-encodings">One–hot encodings</a></li>
  <li><a href="#onehot-encodings-and-constants" id="toc-onehot-encodings-and-constants" class="nav-link" data-scroll-target="#onehot-encodings-and-constants">One–hot encodings and constants</a></li>
  <li><a href="#redundant-regressors" id="toc-redundant-regressors" class="nav-link" data-scroll-target="#redundant-regressors">Redundant regressors</a></li>
  <li><a href="#redundant-regressors-and-zero-eigenvalues" id="toc-redundant-regressors-and-zero-eigenvalues" class="nav-link" data-scroll-target="#redundant-regressors-and-zero-eigenvalues">Redundant regressors and zero eigenvalues</a></li>
  <li><a href="#zero-variance-regressors" id="toc-zero-variance-regressors" class="nav-link" data-scroll-target="#zero-variance-regressors">Zero variance regressors</a></li>
  <li><a href="#orthogonal-regressors" id="toc-orthogonal-regressors" class="nav-link" data-scroll-target="#orthogonal-regressors">Orthogonal regressors</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
$$

\newcommand{\mybold}[1]{\boldsymbol{#1}} 


\newcommand{\trans}{\intercal}
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\bbr}{\mathbb{R}}
\newcommand{\bbz}{\mathbb{Z}}
\newcommand{\bbc}{\mathbb{C}}
\newcommand{\gauss}[1]{\mathcal{N}\left(#1\right)}
\newcommand{\chisq}[1]{\mathcal{\chi}^2_{#1}}
\newcommand{\studentt}[1]{\mathrm{StudentT}_{#1}}
\newcommand{\fdist}[2]{\mathrm{FDist}_{#1,#2}}

\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\,}
\newcommand{\projop}[1]{\underset{#1}{\mathrm{Proj}}\,}
\newcommand{\proj}[1]{\underset{#1}{\mybold{P}}}
\newcommand{\expect}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\dens}[1]{\mathit{p}\left(#1\right)}
\newcommand{\var}[1]{\mathrm{Var}\left(#1\right)}
\newcommand{\cov}[1]{\mathrm{Cov}\left(#1\right)}
\newcommand{\sumn}{\sum_{n=1}^N}
\newcommand{\meann}{\frac{1}{N} \sumn}
\newcommand{\cltn}{\frac{1}{\sqrt{N}} \sumn}

\newcommand{\trace}[1]{\mathrm{trace}\left(#1\right)}
\newcommand{\diag}[1]{\mathrm{Diag}\left(#1\right)}
\newcommand{\grad}[2]{\nabla_{#1} \left. #2 \right.}
\newcommand{\gradat}[3]{\nabla_{#1} \left. #2 \right|_{#3}}
\newcommand{\fracat}[3]{\left. \frac{#1}{#2} \right|_{#3}}


\newcommand{\W}{\mybold{W}}
\newcommand{\w}{w}
\newcommand{\wbar}{\bar{w}}
\newcommand{\wv}{\mybold{w}}

\newcommand{\X}{\mybold{X}}
\newcommand{\x}{x}
\newcommand{\xbar}{\bar{x}}
\newcommand{\xv}{\mybold{x}}
\newcommand{\Xcov}{\Sigmam_{\X}}
\newcommand{\Xcovhat}{\hat{\Sigmam}_{\X}}
\newcommand{\Covsand}{\Sigmam_{\mathrm{sand}}}
\newcommand{\Covsandhat}{\hat{\Sigmam}_{\mathrm{sand}}}

\newcommand{\Z}{\mybold{Z}}
\newcommand{\z}{z}
\newcommand{\zv}{\mybold{z}}
\newcommand{\zbar}{\bar{z}}

\newcommand{\Y}{\mybold{Y}}
\newcommand{\Yhat}{\hat{\Y}}
\newcommand{\y}{y}
\newcommand{\yv}{\mybold{y}}
\newcommand{\yhat}{\hat{\y}}
\newcommand{\ybar}{\bar{y}}

\newcommand{\res}{\varepsilon}
\newcommand{\resv}{\mybold{\res}}
\newcommand{\resvhat}{\hat{\mybold{\res}}}
\newcommand{\reshat}{\hat{\res}}

\newcommand{\betav}{\mybold{\beta}}
\newcommand{\betavhat}{\hat{\betav}}
\newcommand{\betahat}{\hat{\beta}}
\newcommand{\betastar}{{\beta^{*}}}

\newcommand{\bv}{\mybold{\b}}
\newcommand{\bvhat}{\hat{\bv}}

\newcommand{\alphav}{\mybold{\alpha}}
\newcommand{\alphavhat}{\hat{\av}}
\newcommand{\alphahat}{\hat{\alpha}}

\newcommand{\omegav}{\mybold{\omega}}

\newcommand{\gv}{\mybold{\gamma}}
\newcommand{\gvhat}{\hat{\gv}}
\newcommand{\ghat}{\hat{\gamma}}

\newcommand{\hv}{\mybold{\h}}
\newcommand{\hvhat}{\hat{\hv}}
\newcommand{\hhat}{\hat{\h}}

\newcommand{\gammav}{\mybold{\gamma}}
\newcommand{\gammavhat}{\hat{\gammav}}
\newcommand{\gammahat}{\hat{\gamma}}

\newcommand{\new}{\mathrm{new}}
\newcommand{\zerov}{\mybold{0}}
\newcommand{\onev}{\mybold{1}}
\newcommand{\id}{\mybold{I}}

\newcommand{\sigmahat}{\hat{\sigma}}


\newcommand{\etav}{\mybold{\eta}}
\newcommand{\muv}{\mybold{\mu}}
\newcommand{\Sigmam}{\mybold{\Sigma}}

\newcommand{\rdom}[1]{\mathbb{R}^{#1}}

\newcommand{\RV}[1]{\tilde{#1}}



\def\A{\mybold{A}}

\def\A{\mybold{A}}
\def\av{\mybold{a}}
\def\a{a}

\def\B{\mybold{B}}
\def\b{b}


\def\S{\mybold{S}}
\def\sv{\mybold{s}}
\def\s{s}

\def\R{\mybold{R}}
\def\rv{\mybold{r}}
\def\r{r}

\def\V{\mybold{V}}
\def\vv{\mybold{v}}
\def\v{v}

\def\U{\mybold{U}}
\def\uv{\mybold{u}}
\def\u{u}

\def\W{\mybold{W}}
\def\wv{\mybold{w}}
\def\w{w}

\def\tv{\mybold{t}}
\def\t{t}

\def\Sc{\mathcal{S}}
\def\ev{\mybold{e}}

\def\Lammat{\mybold{\Lambda}}

\def\Q{\mybold{Q}}

$$

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Multilinear regression as loss minimization.</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><span class="math inline">\(\textcolor{white}{\LaTeX}\)</span></p>
<section id="goals" class="level1">
<h1>Goals</h1>
<ul>
<li>Derive the general form of the ordinary least squares (OLS) estimator in matrix notation
<ul>
<li>Review simple least squares derivation</li>
<li>Review matrix notation</li>
<li>Review vector calculus</li>
<li>Derive the general OLS formula and show that the simple least squares is a special case</li>
</ul></li>
</ul>
</section>
<section id="siomple-least-squares" class="level1">
<h1>Siomple least squares</h1>
<p>Recall the simple least squares model:</p>
<p><span id="eq-lm-simple"><span class="math display">\[
\begin{align*}
\y_n :={}&amp; \textrm{Response (e.g. sales price)} \\
\x_n :={}&amp; \textrm{Regressor (e.g. square footage)}\\
y_n ={}&amp; \beta_2 \x_n + \beta_1 + \res_n \textrm{ Model (straight line through data)}.
\end{align*}
\tag{1}\]</span></span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Here are some key quantities and their names:</p>
<ul>
<li><span class="math inline">\(\y_n\)</span>: The ‘response’</li>
<li><span class="math inline">\(\x_n\)</span>: The ‘regressors’ or ‘explanatory’ variables</li>
</ul>
<p>For a linear model, we also have:</p>
<ul>
<li><span class="math inline">\(\res_n\)</span>: The ‘error’ or ‘residual’</li>
<li><span class="math inline">\(\beta_2, \beta_1\)</span>: The ‘coefficients’, ‘parameters’, ‘slope and intercept’</li>
</ul>
<p>We might also have estimates of these quantities:</p>
<ul>
<li><span class="math inline">\(\betahat_p\)</span>: Estimate of <span class="math inline">\(\beta_p\)</span></li>
<li><span class="math inline">\(\reshat\)</span>: Estimate of <span class="math inline">\(\res_n\)</span></li>
<li><span class="math inline">\(\yhat_n\)</span>: A ‘prediction’ or ‘fitted value’ <span class="math inline">\(\yhat_n = \betahat_1 + \betahat_2 \x_n\)</span></li>
</ul>
<p>When we form the estimator by minimizing the estimated residuals, we might call the estimate</p>
<ul>
<li>‘Ordinary least squares’ (or ‘OLS’)</li>
<li>‘Least-squares’</li>
<li>‘Linear regression’</li>
</ul>
<p>An estimate will implicitly be least-squares estimates, but precisely what we mean by an estimate may have to come from context.</p>
</div>
</div>
<p>Note that for any value of <span class="math inline">\(\beta\)</span>, we get a value of the “error” or “residual” <span class="math inline">\(\res_n\)</span>:</p>
<p><span class="math display">\[
\res_n = \y_n - (\beta_2 \x_n + \beta_1).
\]</span></p>
<p>The “least squares fit” is called this because we choose <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> to make <span class="math inline">\(\sumn \res_n^2\)</span> as small as possible:</p>
<p><span class="math display">\[
\begin{align*}
\textrm{Choose }\betahat_2,\betahat_1\textrm{ so that }
\sumn \res_n^2 = \sumn \left(  \y_n - (\betahat_2 \x_n + \betahat_1) \right)^2
\textrm{ is as small as possible.}
\end{align*}
\]</span></p>
<p>How do we do this for the simple least squares model? And what if we have more regressors?</p>
</section>
<section id="simple-least-squares-estimator-derivation" class="level1">
<h1>Simple least squares estimator derivation</h1>
<p>The quantity we’re trying to minimize is smooth and convex, so if there is a minimum it would satisfy</p>
<p><span class="math display">\[
\begin{align*}
\fracat{\partial \sumn \res_n^2}{\partial \beta_1}{\betahat_1, \betahat_2} ={}&amp; 0 \quad\textrm{and} \\
\fracat{\partial \sumn \res_n^2}{\partial \beta_2}{\betahat_1, \betahat_2} ={}&amp; 0.
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-warning callout-titled" title="Question">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<p>When is it sufficient to set the gradient equal to zero to find a minumum?</p>
</div>
</div>
<p>These translate to (after dividing by <span class="math inline">\(-2 N\)</span>)</p>
<p><span class="math display">\[
\begin{align*}
\meann \y_n - \betahat_2 \meann \x_n - \betahat_1 ={}&amp; 0 \quad\textrm{and}\\
\meann \y_n \x_n - \betahat_2 \meann \x_n^2 - \betahat_1 \meann \x_n  ={}&amp; 0.
\end{align*}
\]</span></p>
<p>Let’s introduce the notation</p>
<p><span class="math display">\[
\begin{align*}
\overline{y} ={}&amp; \meann \y_n \\
\overline{x} ={}&amp; \meann \x_n \\
\overline{xy} ={}&amp; \meann \x_n \y_n \\
\overline{xx} ={}&amp; \meann \x_n ^2,
\end{align*}
\]</span></p>
<p>Our estimator them must satisfy</p>
<p><span class="math display">\[
\begin{align*}
\overline{x} \betahat_2  + \betahat_1 ={}&amp; \overline{y} \quad\textrm{and}\\
\overline{xx} \betahat_2  + \overline{x} \betahat_1  ={}&amp; \overline{yx}.
\end{align*}
\]</span></p>
<p>We have a linear system with two unknowns and two equations. An elegant way to solve them is to subtract <span class="math inline">\(\overline{x}\)</span> times the first equation from the second, giving:</p>
<p><span class="math display">\[
\begin{align*}
\overline{x} \betahat_1 - \overline{x} \betahat_1 +
    \overline{xx} \betahat_2 - \overline{x}^2 \betahat_2 ={}&amp;
    \overline{xy} - \overline{x} \overline{y} \Leftrightarrow\\
\betahat_2 ={}&amp;
    \frac{\overline{xy} - \overline{x} \overline{y}}
         {\overline{xx}  - \overline{x}^2},
\end{align*}
\]</span></p>
<p>as long as <span class="math inline">\(\overline{xx} - \overline{x}^2 \ne 0\)</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled" title="Question">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<p>In ordinary language, what does it mean for <span class="math inline">\(\overline{xx} - \overline{x}^2 = 0\)</span>?</p>
</div>
</div>
<p>We can then plug this into the first equation giving</p>
<p><span class="math display">\[
\betahat_1 = \overline{y} - \betahat_2 \overline{x}.
\]</span></p>
</section>
<section id="matrix-multiplication-version" class="level1">
<h1>Matrix multiplication version</h1>
<p>Alternatively, our criterion can be written in matrix form as</p>
<p><span id="eq-simple-est-as-matrix"><span class="math display">\[
\begin{pmatrix}
1  &amp; \overline{x} \\
\overline{x} &amp; \overline{xx}
\end{pmatrix}
\begin{pmatrix}
\betahat_1 \\
\betahat_2
\end{pmatrix} =
\begin{pmatrix}
\overline{y} \\
\overline{xy}
\end{pmatrix}
\tag{2}\]</span></span></p>
<p>Recall that there is a special matrix that allows us to get an expression for <span class="math inline">\(\betahat_1\)</span> and <span class="math inline">\(\betahat_2\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
\begin{pmatrix}
1  &amp; \overline{x} \\
\overline{x} &amp; \overline{xx}
\end{pmatrix}^{-1} =
\frac{1}{\overline{xx} - \overline{x}^2}
\begin{pmatrix}
\overline{xx}  &amp; - \overline{x} \\
-\overline{x} &amp; 1
\end{pmatrix}
\end{align*}
\]</span></p>
<p>This matrix is called the “inverse” because</p>
<p><span class="math display">\[
\begin{align*}
\begin{pmatrix}
1  &amp; \overline{x} \\
\overline{x} &amp; \overline{xx}
\end{pmatrix}^{-1}
\begin{pmatrix}
1  &amp; \overline{x} \\
\overline{x} &amp; \overline{xx}
\end{pmatrix} =
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix}.
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-warning callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Verify the preceding property.</p>
</div>
</div>
<p>Multiplying both sides of <a href="#eq-simple-est-as-matrix" class="quarto-xref">Equation&nbsp;2</a> by the matrix inverse gives</p>
<p><span class="math display">\[
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix}
\begin{pmatrix}
\betahat_1 \\
\betahat_2
\end{pmatrix} =
\begin{pmatrix}
\betahat_1 \\
\betahat_2
\end{pmatrix} =
\frac{1}{\overline{xx} - \overline{x}^2}
\begin{pmatrix}
\overline{xx}  &amp; - \overline{x} \\
-\overline{x} &amp; 1
\end{pmatrix}
\begin{pmatrix}
\overline{y} \\
\overline{xy}
\end{pmatrix}.
\]</span></p>
<p>From this we can read off the familiar answer</p>
<p><span class="math display">\[
\begin{align*}
\betahat_2 ={}&amp; \frac{\overline{xy} - \overline{x}\,\overline{y}}{\overline{xx} - \overline{x}^2}\\
\betahat_1 ={}&amp; \frac{\overline{xx}\,\overline{y} - \overline{xy}\,\overline{x}}{\overline{xx} - \overline{x}^2}\\
  ={}&amp; \frac{\overline{xx}\,\overline{y} -
      \overline{x}^2 \overline{y} + \overline{x}^2 \overline{y} - \overline{xy}\,\overline{x}}
    {\overline{xx} - \overline{x}^2}\\
  ={}&amp; \overline{y} - \frac{\overline{x}^2 \overline{y} - \overline{xy}\,\overline{x}}
    {\overline{xx} - \overline{x}^2} \\
  ={}&amp; \overline{y} - \betahat_1 \overline{x}.
\end{align*}
\]</span></p>
</section>
<section id="matrix-notation" class="level1">
<h1>Matrix notation</h1>
<p>The preceding formula came from combining the equations that set the univariate gradients equal to zero, and then recognizing a matrix equation. We can in fact do both at the same time! But first we need some notation</p>
<p>Here is a formal definition of the type of model that we will study for the vast majority of the semester:</p>
<p><span id="eq-lm-scalar"><span class="math display">\[
\begin{align*}
\y_n ={}&amp; \beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP} + \res_{n}, \quad\textrm{For }n=1,\ldots,N.
\end{align*}
\tag{3}\]</span></span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>I will always use <span class="math inline">\(N\)</span> for the number of observed data points, and <span class="math inline">\(P\)</span> for the dimension of the regression vector.</p>
</div>
</div>
<p><a href="#eq-lm-scalar" class="quarto-xref">Equation&nbsp;3</a> is a general form of simpler cases. For example, if we take <span class="math inline">\(\x_{n1} \equiv 1\)</span>, <span class="math inline">\(\x_{n2}= \x_n\)</span> to be some scalar, and <span class="math inline">\(P = 2\)</span>, then <a href="#eq-lm-scalar" class="quarto-xref">Equation&nbsp;3</a> becomes <a href="#eq-lm-simple" class="quarto-xref">Equation&nbsp;1</a>:</p>
<p><span class="math display">\[
\begin{align*}
\y_n ={}&amp; \beta_1  + \beta_2 \x_{n} + \res_{n}, \quad\textrm{For }n=1,\ldots,N.
\end{align*}
\]</span></p>
<p>The residuals <span class="math inline">\(\res_n\)</span> measure the “misfit” of the line. If you know <span class="math inline">\(\beta_1, \ldots, \beta_P\)</span>, then you can compute</p>
<p><span class="math display">\[
\begin{align*}
\res_n ={}&amp; \y_n -  (\beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP}).
\end{align*}
\]</span></p>
<p>But in general we only observe <span class="math inline">\(\y_n\)</span> and <span class="math inline">\(\x_{n1}, \ldots, \x_{nP}\)</span>, and we choose <span class="math inline">\(\beta_1, \ldots, \beta_P\)</span> to make the residuals small. (How we do this precisely will be something we talk about at great length.)</p>
<p>The general form of <a href="#eq-lm-scalar" class="quarto-xref">Equation&nbsp;3</a> can be written more compactly using matrix and vector notation. Specifically, if we let</p>
<p><span class="math display">\[
\begin{align*}
\xv_n :=
\begin{pmatrix}
  \x_{n1} \\ \x_{n2} \\ \vdots \\ \x_{nP}
\end{pmatrix}
\quad
\textrm{and}
\quad
\betav :=
\begin{pmatrix}
  \beta_{1} \\ \beta_2 \\ \vdots \\ \beta_{P}
\end{pmatrix}
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Bold lowercase variables are column vectors (unless otherwise specified).</p>
</div>
</div>
<p>Recall that the “transpose” operator <span class="math inline">\((\cdot)^\trans\)</span> flips the row and columns of a matrix. For example,</p>
<p><span class="math display">\[
\begin{align*}
\xv_n ^\trans =
\begin{pmatrix}
  \x_{n1} &amp; \x_{n2} &amp; \ldots &amp; \x_{nP}
\end{pmatrix}.
\end{align*}
\]</span></p>
<p>By matrix multiplication rules,</p>
<p><span class="math display">\[
\begin{align*}
\xv_n^\trans \betav =
\begin{pmatrix}
  \x_{n1} &amp; \x_{n2} &amp; \ldots &amp; \x_{nP}
\end{pmatrix}
\quad\quad\quad
\begin{pmatrix}
  \beta_{1} \\ \beta_2 \\ \vdots \\ \beta_{P}
\end{pmatrix}
= \beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP}.
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>I have written <span class="math inline">\(\xv_n^\trans \betav\)</span> for the “dot product” or “inner product” between <span class="math inline">\(\xv_n\)</span> and <span class="math inline">\(\betav\)</span>. Writing it in this way clarifies the relationship with matrix notation below.</p>
<p>There are many other ways to denote inner products in the literature, including <span class="math inline">\(\xv_n \cdot \betav\)</span> and <span class="math inline">\(&lt;\xv_n, \betav&gt;\)</span>.</p>
</div>
</div>
<p>Then we can compactly write</p>
<p><span class="math display">\[
\begin{align*}
\y_n ={}&amp; \xv_n ^\trans \betav + \res_{n}, \quad\textrm{For }n=1,\ldots,N.
\end{align*}
\]</span></p>
<p>We can compactify it even further if we stack the <span class="math inline">\(n\)</span> observations: % <span class="math display">\[
\begin{align*}
\y_1 ={}&amp; \xv_1 ^\trans \betav + \res_{1} \\
\y_2 ={}&amp; \xv_2 ^\trans \betav + \res_{2} \\
\vdots\\
\y_N ={}&amp; \xv_N ^\trans \betav + \res_{N} \\
\end{align*}
\]</span></p>
<p>As before we can stack the responses and residuals:</p>
<p><span class="math display">\[
\begin{align*}
\Y :=
\begin{pmatrix}
  \y_{1} \\ \y_2 \\ \vdots \\ \y_{P}
\end{pmatrix}
\quad
\textrm{and}
\quad
\resv :=
\begin{pmatrix}
  \res_{1} \\ \res_2 \\ \vdots \\ \res_{P}
\end{pmatrix}
\end{align*}
\]</span></p>
<p>We can also stack the regressors:</p>
<p><span class="math display">\[
\begin{align*}
\X :=
\begin{pmatrix}
  \x_{11} &amp; \x_{12} &amp; \ldots &amp; \x_{1P}\\
  \x_{21} &amp; \x_{22} &amp; \ldots &amp; \x_{2P}\\
  \vdots\\
  \x_{n1} &amp; \x_{n2} &amp; \ldots &amp; \x_{nP}\\
  \vdots\\
    \x_{N1} &amp; \x_{N2} &amp; \ldots &amp; \x_{NP}
\end{pmatrix}
=
\begin{pmatrix}
  \xv_{1}^\trans \\ \xv_{2}^\trans \\ \vdots \\ \xv_n^\trans \\ \vdots \\ \xv_{N}^\trans
\end{pmatrix}
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>I will use upper case bold letters for multi-dimensional matrices like <span class="math inline">\(\X\)</span>. But I may also use upper case bold letters even when the quantity could also be a column vector, when I think it’s more useful to think of the quantity as a matrix with a single column. Examples are <span class="math inline">\(\Y\)</span> above, or <span class="math inline">\(\X\)</span> when <span class="math inline">\(P = 1\)</span>.</p>
</div>
</div>
<p>Note that by matrix multiplication rules,</p>
<p><span class="math display">\[
\begin{align*}
\X  =
\begin{pmatrix}
  \xv_{1}^\trans \\ \xv_{2}^\trans \\ \vdots \\ \xv_n^\trans \\ \vdots \\ \xv_{N}^\trans
\end{pmatrix}
\quad\quad\quad
\X \betav
=
\begin{pmatrix}
  \xv_{1}^\trans\betav \\ \xv_{2}^\trans\betav \\ \vdots \\ \xv_n^\trans\betav \\ \vdots \\ \xv_{N}^\trans\betav
\end{pmatrix}
\end{align*}
\]</span></p>
<p>so we end up with the extremely tidy expression</p>
<p><span id="eq-y-matrix"><span class="math display">\[
\begin{align*}
\y_n ={}&amp; \beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP} + \res_{n}, \quad\textrm{For }n=1,\ldots,N
\\\\&amp;\textrm{is the same as}\quad\\\\
\Y ={}&amp; \X \betav + \resv.
\end{align*}
\tag{4}\]</span></span></p>
<p>In the case of simple least squares, we can write</p>
<p><span id="eq-simple_matrix"><span class="math display">\[
\begin{align*}
\X :=
\begin{pmatrix}
  1 &amp; \x_{1}\\
  1 &amp; \x_{2}\\
  \vdots &amp; \vdots\\
  1 &amp; \x_{N}\\
\end{pmatrix},
\end{align*}
\tag{5}\]</span></span></p>
<p>and verify that the <span class="math inline">\(n\)</span>–th row of <a href="#eq-y-matrix" class="quarto-xref">Equation&nbsp;4</a> is the same as <a href="#eq-lm-simple" class="quarto-xref">Equation&nbsp;1</a>.</p>
</section>
<section id="least-squares-in-matrix-notation" class="level1">
<h1>Least squares in matrix notation</h1>
<p>Using our tidy expression <a href="#eq-y-matrix" class="quarto-xref">Equation&nbsp;4</a>, we can easily write out the sum of the squared errors as</p>
<p><span class="math display">\[
\begin{aligned}
\sumn \res_n^2 ={}&amp;
    \resv^\trans \resv = (\Y - \X \betav)^\trans (\Y - \X \betav)
    \\={}&amp;
    \Y^\trans \Y - \betav^\trans \X^\trans \Y - \Y^\trans \X \betav + \betav^\trans \X^\trans \X \betav
    \\={}&amp;
    \Y^\trans \Y - 2  \Y^\trans \X \betav + \betav^\trans \X^\trans \X \betav.
\end{aligned}
\]</span></p>
<p>This is a quadratic function of the vector <span class="math inline">\(\betav\)</span>. We wish to find the minimum of this quantity as a function of <span class="math inline">\(\betav\)</span>. We might hope that the minimum occurs at a point where the gradient of this expression is zero.</p>
<p>Rather than compute the <em>univariate</em> derivative with respect to each component, we can compute the <em>multivariate gradient</em> with respect to the vector.</p>
<p>Let’s recall some facts from vector calculus.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Take <span class="math inline">\(\zv \in \mathbb{R}^P\)</span> to be a <span class="math inline">\(P\)</span>–vector. and let <span class="math inline">\(f(\zv)\)</span> a scalar–valued function of the vector <span class="math inline">\(\z\)</span>. We write</p>
<p><span class="math display">\[
\frac{\partial f(\zv)}{\partial \zv}  =
\begin{pmatrix}
\frac{\partial}{\partial \z_1} f(\zv) \\
\vdots\\
\frac{\partial}{\partial \z_P} f(\zv) \\
\end{pmatrix}.
\]</span></p>
<p>That is, the partial <span class="math inline">\(\frac{\partial f(\zv)}{\partial \zv}\)</span> is a <span class="math inline">\(P\)</span>–vector of the stacked univariate dervatives.</p>
</div>
</div>
<p>Recall a couple rules from vector calculus. Let <span class="math inline">\(\vv\)</span> denote a <span class="math inline">\(P\)</span>–vector and <span class="math inline">\(\A\)</span> a symmetric matrix. Then</p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial \vv^\trans \zv}{\partial \zv}  = \vv
\quad\textrm{and}\quad
\frac{\partial \zv^\trans \A \zv}{\partial \zv}  = 2 \A \zv.
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-warning callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Prove these results above using univariate derivatives and our stacking convention.</p>
</div>
</div>
<p>Applying these two rules to our least squares objective,</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial\resv^\trans \resv }{\partial \betav} ={}&amp;
    \frac{\partial }{\partial \betav} \Y^\trans \Y -
    2 \frac{\partial }{\partial \betav}  \Y^\trans \X \betav +
    \frac{\partial }{\partial \betav} \betav^\trans \X^\trans \X \betav
    \\ ={}&amp;
    0 - 2 \X^\trans \Y + 2 \X^\trans \X \betav.
\end{aligned}
\]</span></p>
<p>Assuming our estimator <span class="math inline">\(\betahat\)</span> sets these partial derivatives are equal to zero, we then get</p>
<p><span id="eq-ols-esteq"><span class="math display">\[
\begin{align*}
\X^\trans \X \betavhat ={}&amp; \X^\trans \Y.
\end{align*}
\tag{6}\]</span></span></p>
<p>This is a set of <span class="math inline">\(P\)</span> equations in <span class="math inline">\(P\)</span> unknowns. If it is not degenerate, one can solve for <span class="math inline">\(\betavhat\)</span>. That is, if the matrix <span class="math inline">\(\X^\trans \X\)</span> is invertible, then we can multiply both sides of <a href="#eq-ols-esteq" class="quarto-xref">Equation&nbsp;6</a> by <span class="math inline">\((\X^\trans \X)^{-1}\)</span> to get</p>
<p><span id="eq-ols-betahat"><span class="math display">\[
\betavhat = (\X^\trans \X)^{-1} \X^\trans \Y
\tag{7}\]</span></span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>We’re going to be talking again again about the “ordinary least squares” (“OLS”) problem <span class="math display">\[
\betavhat = \argmin{\beta} \resv^\trans \resv
\quad\textrm{where}\quad
\Y = \X \betav + \resv
\quad\textrm{where}\quad
\y_n = \xv_n^\trans \betav + \res_n \textrm{ for all }n=1,\ldots,N.
\]</span></p>
<p>It will be nice to have some shorthand for this problem so I don’t have to write this out every time. All of the following will be understood as shorthand for the preceding problem. In each, the fact that <span class="math inline">\(\betavhat\)</span> minimizes the sum of squared residuals is implicit.</p>
<p><span class="math display">\[
\begin{aligned}
\Y \sim{}&amp; \X \betav + \resv&amp; \textrm{Only the least squares criterion is implicit}\\
\Y \sim{}&amp; \X \betav &amp; \textrm{$\resv$ implicit}\\
\Y \sim{}&amp; \X &amp; \textrm{$\resv$, $\betav$ implicit}\\
\y_n \sim{}&amp; \xv_n^\trans \betav &amp; \textrm{$\res_n$, $N$ implicit}\\
\y_n \sim{}&amp; \xv_n^\trans \betav + \res_n &amp; \textrm{$N$ implicit}\\
\y_n \sim{}&amp; \xv_n &amp; \textrm{$\res_n$, $\betav$, $N$ implicit}\\
\y_n \sim{}&amp; \x_{n1} + \x_{n2} + \ldots + \x_{nP} &amp; \textrm{$\res_n$, $\betav$, $N$ implicit}\\
\end{aligned}
\]</span></p>
<p>(The final shorthand is closest to the notation for the <code>R</code> <code>lm</code> function.) This is convenient because, for example, certain properties of the regression <span class="math inline">\(\y_n \sim \xv_n\)</span> don’t necessarily need to commit to which symbol we use for the coefficients. Symbols for the missing pieces will hopefuly be clear from context as necessary.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Unlike many other regression texts (and the <code>lm</code> function), I will <em>not</em> necessarily assume that a constant is included in the regression. One can always take a generic regression <span class="math inline">\(\y_n \sim \xv_n\)</span> to include a constant by assuming that one of the entries of <span class="math inline">\(\xv_n\)</span> is one. At some points my convention of not including a constant by default will lead to formulas that may be at odds with some textbooks. But these differences are superficial, and are, in my mind, more than made up for by the generality and simplicity of treating constants as just another regressor.</p>
</div>
</div>
</section>
<section id="some-examples" class="level1">
<h1>Some examples</h1>
<p>The formulas given in <a href="#eq-ols-esteq" class="quarto-xref">Equation&nbsp;6</a> and <a href="#eq-ols-betahat" class="quarto-xref">Equation&nbsp;7</a> are enough to calculate every least squares estimator we’ll encounter. But we’d like to have intuition for the meaning of the formulas, and for that it will be useful to start by applying the formulas in some familiar settings.</p>
<p>Recall that, any fit of the form <span class="math inline">\(\X \betav\)</span> that minimizes the sum of squared errors must take the form <span class="math inline">\(\X \betavhat\)</span> where <span class="math inline">\(\betavhat\)</span> satisfies</p>
<p><span class="math display">\[
\begin{align*}
\X^\trans \X \betavhat ={}&amp; \X^\trans \Y.
\end{align*}
\]</span></p>
<p>However, there may in general be many <span class="math inline">\(\betavhat\)</span> that satisfy the preceding equation. But if <span class="math inline">\(\X^\trans \X\)</span> is invertible, then <span class="math display">\[
\betavhat = (\X^\trans \X)^{-1} \X^\trans \Y,
\]</span></p>
<p>and there is only one <span class="math inline">\(\betavhat\)</span> that minimizes the sum of squared error.</p>
<p>Let’s take a careful look at what <span class="math inline">\(\X^\trans \X\)</span> is measuring, what it means for it to be invertible, as well as what it means when it is <em>not</em> invertible.</p>
<section id="the-sample-mean" class="level2">
<h2 class="anchored" data-anchor-id="the-sample-mean">The sample mean</h2>
<div class="callout callout-style-default callout-tip callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>I will use <span class="math inline">\(\onev\)</span> to denote a vector full of ones. Usually it will be an <span class="math inline">\(N\)</span>–vector, but sometimes its dimension will just be implicit. Similarly, <span class="math inline">\(\zerov\)</span> is a vector of zeros.</p>
</div>
</div>
<p>We showed earlier that the sample mean is a special case of the regression <span class="math inline">\(\y_n \sim 1 \cdot \beta\)</span>. This can be expressed in matrix notation by taking <span class="math inline">\(\X = \onev\)</span> as a <span class="math inline">\(N\times 1\)</span> vector. We then have</p>
<p><span class="math display">\[
\X^\trans \X = \onev^\trans \onev = \sumn 1 \cdot 1 = N,
\]</span></p>
<p>so <span class="math inline">\(\X^\trans \X\)</span> is invertible as long as <span class="math inline">\(N &gt; 0\)</span> (i.e., if you have at least one datapoint), with <span class="math inline">\((\X^\trans \X)^{-1} = 1/N\)</span>. We also have</p>
<p><span class="math display">\[
\X^\trans \Y = \onev^\trans \Y = \sumn 1 \cdot \y_n = N \ybar,
\]</span></p>
<p>and so</p>
<p><span class="math display">\[
\betahat = (\X^\trans \X)^{-1}  \X^\trans \Y = (\onev^\trans \onev)^{-1} \onev^\trans \Y = \frac{N \ybar}{N} = \ybar,
\]</span></p>
<p>as expected.</p>
</section>
<section id="a-single-regressor" class="level2">
<h2 class="anchored" data-anchor-id="a-single-regressor">A single regressor</h2>
<p>Suppose that we regress <span class="math inline">\(\y_n \sim \x_n\)</span> where <span class="math inline">\(\x_n\)</span> is a scalar. Let’s suppose that <span class="math inline">\(\expect{\x_n} = 0\)</span> and <span class="math inline">\(\var{\x_n} = \sigma^2 &gt; 0\)</span>. We have</p>
<p><span class="math display">\[
\X = \begin{pmatrix}
\x_1 \\
\x_2 \\
\vdots\\
\x_N
\end{pmatrix}
\]</span></p>
<p>so</p>
<p><span class="math display">\[
\X^\trans \X = \sumn \x_n^2.
\]</span></p>
<p>Depending on the distribution of <span class="math inline">\(\x_n\)</span>, it may be possible for <span class="math inline">\(\X^\trans \X\)</span> to be non–invertible!</p>
<div class="callout callout-style-default callout-warning callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Produce a distribution for <span class="math inline">\(\x_n\)</span> where <span class="math inline">\(\X^\trans \X\)</span> is non–invertible with positive probability for any <span class="math inline">\(N\)</span>.</p>
</div>
</div>
<p>However, as <span class="math inline">\(N \rightarrow \infty\)</span>, <span class="math inline">\(\frac{1}{N} \X^\trans \X \rightarrow \sigma^2\)</span> by the LLN, and since <span class="math inline">\(\sigma^2 &gt; 0\)</span>, <span class="math inline">\(\frac{1}{N} \X^\trans \X\)</span> will be invertible with probability approaching one as <span class="math inline">\(N\)</span> goes to infinity.</p>
</section>
<section id="onehot-encodings" class="level2">
<h2 class="anchored" data-anchor-id="onehot-encodings">One–hot encodings</h2>
<p>We discussed one-hot encodings in the context of the Ames housing data. Suppose we have a columns <span class="math inline">\(k_n \in \{ g, e\}\)</span> indicating whether a kitchen is “good” or “excellent”. A one–hot encoding of this categorical variable is given by</p>
<p><span class="math display">\[
\begin{aligned}
\x_{ng} =
\begin{cases}
1 &amp; \textrm{ if }k_n = g \\
0 &amp; \textrm{ if }k_n \ne g \\
\end{cases}
&amp;&amp;
\x_{ne} =
\begin{cases}
1 &amp; \textrm{ if }k_n = e \\
0 &amp; \textrm{ if }k_n \ne e \\
\end{cases}
\end{aligned}.
\]</span></p>
<p>We can then regress <span class="math inline">\(\y_n \sim \beta_g \x_{ng} + \beta_e \x_{ne} = \x_n^\trans \betav\)</span>. The corresponding <span class="math inline">\(\X\)</span> matrix might look like</p>
<p><span class="math display">\[
\begin{aligned}
\mybold{k} =
\begin{pmatrix}
g \\
e \\
g \\
g \\
\vdots
\end{pmatrix}
&amp;&amp;
\X = (\xv_g \quad \xv_e) =
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
1 &amp; 0 \\
1 &amp; 0 \\
\vdots
\end{pmatrix}
\end{aligned}
\]</span></p>
<p>Note that <span class="math inline">\(\xv_g^\trans \xv_g\)</span> is just the number of entries with <span class="math inline">\(k_n = g\)</span>, and <span class="math inline">\(\xv_g^\trans \xv_e = 0\)</span> because a kitchen is either good or excellent but never both.</p>
<p>We then have</p>
<p><span class="math display">\[
\X^\trans \X =
\begin{pmatrix}
\xv_g^\trans \xv_g  &amp; \xv_g^\trans \xv_e \\
\xv_e^\trans \xv_g  &amp; \xv_e^\trans \xv_e \\
\end{pmatrix} =
\begin{pmatrix}
N_g  &amp; 0 \\
0  &amp; N_e \\
\end{pmatrix}.
\]</span></p>
<p>Then <span class="math inline">\(\X^\trans \X\)</span> is invertible as long as <span class="math inline">\(N_g &gt; 0\)</span> and <span class="math inline">\(N_e &gt; 0\)</span>, that is, as long as we have at least one observation of each kitchen type, and</p>
<p><span class="math display">\[
\left(\X^\trans \X\right)^{-1} =
\begin{pmatrix}
\frac{1}{N_g}  &amp; 0 \\
0  &amp; \frac{1}{N_e} \\
\end{pmatrix}.
\]</span></p>
<p>Similarly, <span class="math inline">\(\xv_g^\trans \Y\)</span> is just the sum of entries of <span class="math inline">\(\Y\)</span> where <span class="math inline">\(k_n = g\)</span>, with the analogous conclusion for <span class="math inline">\(\xv_e\)</span>. From this we recover the result that</p>
<p><span class="math display">\[
\betavhat = (\X^\trans \X)^{-1} \X^\trans \Y
=
\begin{pmatrix}
\frac{1}{N_g}  &amp; 0 \\
0  &amp; \frac{1}{N_e} \\
\end{pmatrix}
\begin{pmatrix}
\sum_{n: k_n=g} \y_n \\
\sum_{n: k_n=e} \y_n
\end{pmatrix}
=
\begin{pmatrix}
\frac{1}{N_g} \sum_{n: k_n=g} \y_n \\
\frac{1}{N_e} \sum_{n: k_n=e} \y_n
\end{pmatrix}.
\]</span></p>
<p>If we let <span class="math inline">\(\ybar_e\)</span> and <span class="math inline">\(\ybar_g\)</span> denote the sample means within each group, we have shows that <span class="math inline">\(\betahat_g = \ybar_g\)</span> and <span class="math inline">\(\betahat_e = \ybar_e\)</span>, as we proved before without using the matrix formulation.</p>
</section>
<section id="onehot-encodings-and-constants" class="level2">
<h2 class="anchored" data-anchor-id="onehot-encodings-and-constants">One–hot encodings and constants</h2>
<p>Recall in the Ames housing data, we ran the following two regressions:</p>
<p><span class="math display">\[
\begin{aligned}
\y_n \sim{}&amp; \beta_e \x_{ne} + \beta_g \x_{ng}  \\
\y_n \sim{}&amp; \gamma_0  + \gamma_g \x_{ng} + \res_n = \z_n^\trans \gammav,
\end{aligned}
\]</span> where I take <span class="math inline">\(\gammav = (\gamma_0, \gamma_g)^\trans\)</span> and <span class="math inline">\(\z_n = (1, \x_{ng})^\trans\)</span>.</p>
<p>We found using <code>R</code> that the best fits were given by</p>
<p><span class="math display">\[
\begin{aligned}
\betahat_e =&amp; \ybar_e  &amp; \betahat_g =&amp; \ybar_g \\
\gammahat_0 =&amp; \ybar_e  &amp; \gammahat_g =&amp; \ybar_g - \ybar_e \\
\end{aligned}
\]</span></p>
<p>We can compute the latter by constructing the <span class="math inline">\(\Z\)</span> matrix whose rows are <span class="math inline">\(\z_n^\trans\)</span>. (We use <span class="math inline">\(\Z\)</span> to differentiate the <span class="math inline">\(\X\)</span> matrix from the previous example.) Using similar reasoning to the one–hot encoding, we see that</p>
<p><span class="math display">\[
\Z^\trans \Z =
\begin{pmatrix}
N &amp; N_g \\
N_g &amp; N_g
\end{pmatrix}.
\]</span></p>
<p>This is invertible as long as <span class="math inline">\(N_g \ne N\)</span>, i.e., as long as there is at least one <span class="math inline">\(k_n = e\)</span>. We have</p>
<p><span class="math display">\[
(\Z^\trans \Z)^{-1} =
\frac{1}{N_g (N - N_g)}
\begin{pmatrix}
N_g &amp; -N_g \\
-N_g &amp; N
\end{pmatrix}
\quad\textrm{and}\quad
\Z^\trans \Y =
\begin{pmatrix}
\sumn \y_n \\
\sum_{n: k_n=g} \y_n \\
\end{pmatrix}
\]</span></p>
<p>It is possible (but a little tedious) to prove <span class="math inline">\(\gammahat_0 = \ybar_e\)</span> and <span class="math inline">\(\gammahat_g = \ybar_g - \ybar_e\)</span> using these formulas. But an easier way to see it is as follows.</p>
<p>Note that <span class="math inline">\(\x_{ne} + \x_{ng} = 1\)</span>. That means we can always re-write the regression with a constant as</p>
<p><span class="math display">\[
\y_n \sim \gamma_0 + \gamma_g \x_{ng} = \gamma_0 (\x_{ne} + \x_{ng}) + \gamma_g \x_{ng} =
\gamma_0 \x_{ne} + (\gamma_0 + \gamma_g) \x_{ng}.
\]</span></p>
<p>Now, we already know from the one–hot encoding case that the sum of squared residuals is minimized by setting <span class="math inline">\(\gammahat_0 = \ybar_e\)</span> and <span class="math inline">\(\gammahat_0 + \gammahat_g = \ybar_g\)</span>. We can then solve for <span class="math inline">\(\gammahat_g = \ybar_g - \ybar_e\)</span>, as expected.</p>
<p>This is case where we have two regressions whose regressors are invertible linear combinations of one another:</p>
<p><span class="math display">\[
\zv_n =
\begin{pmatrix}
1 \\
\x_{ng}
\end{pmatrix} =
\begin{pmatrix}
\x_{ne} + \x_{ng} \\
\x_{ng}
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; 0\\
\end{pmatrix}
\begin{pmatrix}
\x_{ng} \\
\x_{ne}
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; 0\\
\end{pmatrix}
\xv_n.
\]</span></p>
<p>It follows that if you can acheive a least squares fit with <span class="math inline">\(\xv_n^\trans \betavhat\)</span>, you can achieve exactly the same fit with</p>
<p><span class="math display">\[
\betavhat^\trans \xv_n =  
\betavhat^\trans
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; 0\\
\end{pmatrix}^{-1} \zv_n,
\]</span></p>
<p>which can be achieved by taking</p>
<p><span class="math display">\[
\gammavhat^\trans =
\betavhat^\trans
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; 0\\
\end{pmatrix}^{-1} \Rightarrow
\gammavhat =
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; 0\\
\end{pmatrix}^{-T} \betavhat
=
\frac{1}{-1}
\begin{pmatrix}
0 &amp; -1 \\
-1 &amp; 1\\
\end{pmatrix} \betavhat
=
\begin{pmatrix}
\betahat_2 \\
\betahat_1 - \betahat_2 \\
\end{pmatrix},
\]</span></p>
<p>exactly as expected.</p>
<p>We will see this is an entirely general result: when regressions are related by invertible linear transformations of regressors, the fit does not change, but the optimal coefficients are linear transforms of one another.</p>
</section>
<section id="redundant-regressors" class="level2">
<h2 class="anchored" data-anchor-id="redundant-regressors">Redundant regressors</h2>
<p>Suppose we run the (silly) regression <span class="math inline">\(\y \sim \alpha \cdot 1 + \gamma \cdot 3 + \res_n\)</span>. That is, we regress on both the constant <span class="math inline">\(1\)</span> and the constant <span class="math inline">\(3\)</span>. We have</p>
<p><span class="math display">\[
\X =
\begin{pmatrix}
1 &amp; 3 \\
1 &amp; 3 \\
1 &amp; 3 \\
\vdots
\end{pmatrix}
=
(\onev \quad 3 \onev)
\]</span></p>
<p>and so</p>
<p><span class="math display">\[
\X^\trans \X =
\begin{pmatrix}
\onev^\trans \onev &amp; 3 \onev^\trans \onev \\
3 \onev^\trans \onev &amp; 9 \onev^\trans \onev \\
\end{pmatrix}
=
N \begin{pmatrix}
1 &amp; 3  \\
3 &amp; 9  \\
\end{pmatrix}
\]</span></p>
<p>This is not invertible (the second row is <span class="math inline">\(3\)</span> times the first, and the determinant is <span class="math inline">\(9 - 3 \cdot 3 = 0\)</span>). So <span class="math inline">\(\betavhat\)</span> is not defined. What went wrong?</p>
<p>One way to see this is to define <span class="math inline">\(\beta = \alpha + 3 \gamma\)</span> and write</p>
<p><span class="math display">\[
\y_n = (\alpha + 3 \gamma) + \res_n = \beta + \res_n.
\]</span></p>
<p>There is obviously only one <span class="math inline">\(\betahat\)</span> that minimizes <span class="math inline">\(\sumn \res_n^2\)</span>, <span class="math inline">\(\betahat = \ybar\)</span>. But there are an infinite set of choices for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\gamma\)</span> satisfying</p>
<p><span class="math display">\[
\alpha + 3 \gamma = \betahat = \ybar.
\]</span></p>
<p>Specifically, for any value of <span class="math inline">\(\gamma\)</span> we can take <span class="math inline">\(\alpha = \ybar - 3 \gamma\)</span>, leaving <span class="math inline">\(\beta\)</span> unchanged. All of these choices for <span class="math inline">\(\alpha,\gamma\)</span> acheive the same <span class="math inline">\(\sumn \res_n^2\)</span>! So the least squares criterion cannot distinguish among them.</p>
<p>In general, this is what it means for <span class="math inline">\(\X^\trans \X\)</span> to be non–invertibile. It happens precisely when there are redundant regressors, and many regression coefficients that result in the same fit.</p>
</section>
<section id="redundant-regressors-and-zero-eigenvalues" class="level2">
<h2 class="anchored" data-anchor-id="redundant-regressors-and-zero-eigenvalues">Redundant regressors and zero eigenvalues</h2>
<p>In fact, <span class="math inline">\(\X^\trans \X\)</span> is invertible precisely when <span class="math inline">\(\X^\trans \X\)</span> has a zero eigenvalue. In the preceding example, we can see that</p>
<p><span class="math display">\[
\X^\trans \X
\begin{pmatrix}
3 \\ -1
\end{pmatrix}
=
N \begin{pmatrix}
1 &amp; 3  \\
3 &amp; 9  \\
\end{pmatrix}
\begin{pmatrix}
3 \\ -1
\end{pmatrix}
=
\begin{pmatrix}
0 \\ 0
\end{pmatrix},
\]</span></p>
<p>so <span class="math inline">\((3, -1)^\trans\)</span> is a zero eigenvector. (In general you might find this by numerical eigenvalue decomposition, but in this case you can just guess the zero eigenvalue.)</p>
<p>Going back to <a href="#eq-ols-esteq" class="quarto-xref">Equation&nbsp;6</a>, we see that this means that</p>
<p><span class="math display">\[
\X^\trans \Y =
(\X^\trans \X)
\begin{pmatrix}
\alphahat \\ \gammahat
\end{pmatrix}
=
N \begin{pmatrix}
1 &amp; 3  \\
3 &amp; 9  \\
\end{pmatrix}
\begin{pmatrix}
\alphahat \\ \gammahat
\end{pmatrix}
=
N \begin{pmatrix}
1 &amp; 3  \\
3 &amp; 9  \\
\end{pmatrix}
\left(
\begin{pmatrix}
\alphahat \\ \gammahat
\end{pmatrix}
+ C
\begin{pmatrix}
3 \\ -1
\end{pmatrix}
\right)
\]</span></p>
<p>for <em>any</em> value of <span class="math inline">\(C\)</span>. This means there are an infinite set of “optimal” values, all of which set the gradient of the loss to zero, and all of which have the same value of the loss function (i.e.&nbsp;acheive the same fit). And you can check that these family of values are exactly the ones that satisfy <span class="math inline">\(\alpha + 3 \gamma = \betahat = \ybar\)</span>, since</p>
<p><span class="math display">\[
\alpha + 3 \gamma =
\begin{pmatrix}
1 &amp; 3
\end{pmatrix}
\begin{pmatrix}
\alpha \\ \gamma
\end{pmatrix}
\quad\quad\textrm{and}\quad\quad
\begin{pmatrix}
1 &amp; 3
\end{pmatrix}
\begin{pmatrix}
3 \\ -1
\end{pmatrix} = 0.
\]</span></p>
<p>Soon, we will see that this is a general result: when <span class="math inline">\(\X^\trans \X\)</span> is not invertible, that means there are many equivalent least squares fits, all characterized precisely by the zero eigenvectors of <span class="math inline">\(\X^\trans \X\)</span>.</p>
</section>
<section id="zero-variance-regressors" class="level2">
<h2 class="anchored" data-anchor-id="zero-variance-regressors">Zero variance regressors</h2>
<p>An example of redundant regressors occurs when the sample variance of <span class="math inline">\(\x_n\)</span> is zero and a constant is included in the regression. Specifically, suppose that <span class="math inline">\(\overline{xx} - \overline{x}^2 = 0\)</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Prove that <span class="math inline">\(\overline{xx} - \overline{x}^2 = 0\)</span> means <span class="math inline">\(\x_n\)</span> is a constant with <span class="math inline">\(\x_n = \xbar\)</span>. Hint: look at the sample variance of <span class="math inline">\(\x_n\)</span>.</p>
</div>
</div>
<p>Let’s regress <span class="math inline">\(\y_n \sim \beta_1 + \beta_2 \x_n\)</span>.</p>
<p>For simplicity, let’s take <span class="math inline">\(\x_n = 3\)</span>. In that case we can rewrite our estimating equation as</p>
<p><span class="math display">\[
\y_n = \beta_1 + \beta_2 \x_n + \res_n
     = (\beta_1 + \beta_2 \xbar) + \res_n.
\]</span></p>
<p>We’re thus in the previous setting with <span class="math inline">\(\xbar\)</span> in place of the number <span class="math inline">\(3\)</span>.</p>
</section>
<section id="orthogonal-regressors" class="level2">
<h2 class="anchored" data-anchor-id="orthogonal-regressors">Orthogonal regressors</h2>
<p>Suppose we have regressors such that the columns of <span class="math inline">\(\X\)</span> are orthonormal. This seems strange at first, since we usually specify the <em>rows</em> of the regressors, not the columns. But in fact we have seen a near–example with one–hot encodings, which are defined row–wise, but which produce orthogonal column vectors in <span class="math inline">\(\X\)</span>. If we divide a one–hot encoding by the square root of the number of ones in the whole dataset, we produce an normal column vector.</p>
<p>If <span class="math inline">\(\X\)</span> has orthonormal columns, then <span class="math inline">\(\X^\trans \X = \id\)</span>, the identity matrix, and so</p>
<p><span class="math display">\[
\betavhat = (\X^\trans \X)^{-1} \X^\trans \Y = \X^\trans \Y.
\]</span></p>
<p>This is of course the same answer we would have gotten if we had tried to write <span class="math inline">\(\Y\)</span> in the basis of the column vectors of <span class="math inline">\(\X\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\Y ={}&amp; \betahat_1 \X_{\cdot 1} + \ldots + \betahat_P \X_{\cdot P} = \X \betavhat \Rightarrow \\
\X^\trans \Y ={}&amp; \X^\trans \X \betavhat = \betavhat
\end{aligned}
\]</span></p>
<p>This regression is particularly simple — each component of <span class="math inline">\(\betavhat\)</span> depends only on its corresponding column of <span class="math inline">\(\X\)</span>.</p>
<p>Note that if each entry of <span class="math inline">\(\xv_n\)</span> is mean zero, unit variance, and uncorrelated with the other entries, then <span class="math inline">\(\frac{1}{N} \X^\trans \X \rightarrow \id\)</span> by the LLN. Such a regressor matrix is not typically orthogonal for any particular <span class="math inline">\(N\)</span>, but it approaches orthogonality as <span class="math inline">\(N\)</span> grows.</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Multilinear regression as loss minimization."</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    include-before-body:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">     - file: ../macros.md</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>$\textcolor{white}{\LaTeX}$ </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu"># Goals</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Derive the general form of the ordinary least squares (OLS) estimator in matrix notation</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Review simple least squares derivation</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Review matrix notation</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Review vector calculus</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Derive the general OLS formula and show that the simple least squares is a special case</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="fu"># Siomple least squares </span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>Recall the simple least squares model:</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>\y_n :={}&amp; \textrm{Response (e.g. sales price)} <span class="sc">\\</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>\x_n :={}&amp; \textrm{Regressor (e.g. square footage)}<span class="sc">\\</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>y_n ={}&amp; \beta_2 \x_n + \beta_1 + \res_n \textrm{ Model (straight line through data)}.</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>$${#eq-lm-simple}</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'} </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>Here are some key quantities and their names:</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\y_n$: The 'response'</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\x_n$: The 'regressors' or 'explanatory' variables</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>For a linear model, we also have:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\res_n$: The 'error' or 'residual'</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\beta_2, \beta_1$: The 'coefficients', 'parameters', 'slope and intercept'</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>We might also have estimates of these quantities:</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\betahat_p$: Estimate of $\beta_p$</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\reshat$: Estimate of $\res_n$</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\yhat_n$: A 'prediction' or 'fitted value'</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>     $\yhat_n = \betahat_1 + \betahat_2 \x_n$</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>When we form the estimator by minimizing the estimated residuals, we might call the estimate</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>'Ordinary least squares' (or 'OLS')</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>'Least-squares'</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>'Linear regression'</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>An estimate will implicitly be least-squares estimates, but precisely what we mean by an estimate may have to come from context.</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>Note that for any value of $\beta$, we get a value of the "error"</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>or "residual" $\res_n$:</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>\res_n = \y_n - (\beta_2 \x_n + \beta_1).</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>The "least squares fit" is called this because we choose $\beta_1$ and $\beta_2$</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>to make $\sumn \res_n^2$ as small as possible:</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>\textrm{Choose }\betahat_2,\betahat_1\textrm{ so that }</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>\sumn \res_n^2 = \sumn \left(  \y_n - (\betahat_2 \x_n + \betahat_1) \right)^2</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>\textrm{ is as small as possible.}</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>How do we do this for the simple least squares model?  And what</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>if we have more regressors?</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="fu"># Simple least squares estimator derivation</span></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>The quantity we're trying to minimize is smooth and convex, so if there</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>is a minimum it would satisfy</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>\fracat{\partial \sumn \res_n^2}{\partial \beta_1}{\betahat_1, \betahat_2} ={}&amp; 0 \quad\textrm{and} <span class="sc">\\</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>\fracat{\partial \sumn \res_n^2}{\partial \beta_2}{\betahat_1, \betahat_2} ={}&amp; 0.</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Question'} </span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>When is it sufficient to set the gradient equal to zero to find a minumum?</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>These translate to (after dividing by $-2 N$)</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>\meann \y_n - \betahat_2 \meann \x_n - \betahat_1 ={}&amp; 0 \quad\textrm{and}<span class="sc">\\</span></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>\meann \y_n \x_n - \betahat_2 \meann \x_n^2 - \betahat_1 \meann \x_n  ={}&amp; 0.</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>Let's introduce the notation</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>\overline{y} ={}&amp; \meann \y_n <span class="sc">\\</span></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>\overline{x} ={}&amp; \meann \x_n <span class="sc">\\</span></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>\overline{xy} ={}&amp; \meann \x_n \y_n <span class="sc">\\</span></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>\overline{xx} ={}&amp; \meann \x_n ^2,</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>Our estimator them must satisfy</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a> \overline{x} \betahat_2  + \betahat_1 ={}&amp; \overline{y} \quad\textrm{and}<span class="sc">\\</span></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a> \overline{xx} \betahat_2  + \overline{x} \betahat_1  ={}&amp; \overline{yx}.</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>We have a linear system with two unknowns and two equations.  An elegant way to solve them is to subtract $\overline{x}$ times the first equation from the second, giving:</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>\overline{x} \betahat_1 - \overline{x} \betahat_1 + </span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>    \overline{xx} \betahat_2 - \overline{x}^2 \betahat_2 ={}&amp;</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>    \overline{xy} - \overline{x} \overline{y} \Leftrightarrow<span class="sc">\\</span></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>\betahat_2 ={}&amp;</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>    \frac{\overline{xy} - \overline{x} \overline{y}}</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>         {\overline{xx}  - \overline{x}^2},</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>as long as $\overline{xx} - \overline{x}^2 \ne 0$.  </span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Question'} </span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>In ordinary language, what does it mean for $\overline{xx} - \overline{x}^2 = 0$?</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>We can then plug this into the first equation giving</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>\betahat_1 = \overline{y} - \betahat_2 \overline{x}.</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a><span class="fu"># Matrix multiplication version</span></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>Alternatively, our criterion can be written in matrix form as</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>1  &amp; \overline{x} <span class="sc">\\</span></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>\overline{x} &amp; \overline{xx} </span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>\betahat_1 <span class="sc">\\</span></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>\betahat_2</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} =</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>\overline{y} <span class="sc">\\</span></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>\overline{xy}</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>$${#eq-simple-est-as-matrix}</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>Recall that there is a special matrix that allows us to get an</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>expression for $\betahat_1$ and $\betahat_2$:</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>1  &amp; \overline{x} <span class="sc">\\</span></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>\overline{x} &amp; \overline{xx} </span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}^{-1} =</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>\frac{1}{\overline{xx} - \overline{x}^2}</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>\overline{xx}  &amp; - \overline{x} <span class="sc">\\</span></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>-\overline{x} &amp; 1 </span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>This matrix is called the "inverse" because</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>1  &amp; \overline{x} <span class="sc">\\</span></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>\overline{x} &amp; \overline{xx} </span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}^{-1}</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>1  &amp; \overline{x} <span class="sc">\\</span></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>\overline{x} &amp; \overline{xx} </span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} =</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>0 &amp; 1</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Exercise'} </span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>Verify the preceding property.</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>Multiplying both sides of @eq-simple-est-as-matrix by</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>the matrix inverse gives</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>0 &amp; 1</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>\betahat_1 <span class="sc">\\</span></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>\betahat_2</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} =</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>\betahat_1 <span class="sc">\\</span></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>\betahat_2</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} =</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>\frac{1}{\overline{xx} - \overline{x}^2}</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>\overline{xx}  &amp; - \overline{x} <span class="sc">\\</span></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>-\overline{x} &amp; 1 </span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>\overline{y} <span class="sc">\\</span></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>\overline{xy}</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>From this we can read off the familiar answer</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>\betahat_2 ={}&amp; \frac{\overline{xy} - \overline{x}\,\overline{y}}{\overline{xx} - \overline{x}^2}<span class="sc">\\</span></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>\betahat_1 ={}&amp; \frac{\overline{xx}\,\overline{y} - \overline{xy}\,\overline{x}}{\overline{xx} - \overline{x}^2}<span class="sc">\\</span></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>  ={}&amp; \frac{\overline{xx}\,\overline{y} - </span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>      \overline{x}^2 \overline{y} + \overline{x}^2 \overline{y} - \overline{xy}\,\overline{x}}</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>    {\overline{xx} - \overline{x}^2}<span class="sc">\\</span></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>  ={}&amp; \overline{y} - \frac{\overline{x}^2 \overline{y} - \overline{xy}\,\overline{x}}</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>    {\overline{xx} - \overline{x}^2} <span class="sc">\\</span></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>  ={}&amp; \overline{y} - \betahat_1 \overline{x}.</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a><span class="fu"># Matrix notation</span></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>The preceding formula came from combining the equations that set the univariate</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>gradients equal to zero, and then recognizing a matrix equation.  We can in</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>fact do both at the same time!  But first we need some notation</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>Here is a formal definition of the type of model that we will study for the vast</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a>majority of the semester:</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>\y_n ={}&amp; \beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP} + \res_{n}, \quad\textrm{For }n=1,\ldots,N.</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>$${#eq-lm-scalar}</span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'}  </span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>I will always use $N$ for the number of observed data points, and $P$ </span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>for the dimension of the regression vector.</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>@eq-lm-scalar is a general form of simpler cases.  For example,</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>if we take $\x_{n1} \equiv 1$, $\x_{n2}= \x_n$ to be some scalar, and $P = 2$, then</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>@eq-lm-scalar becomes @eq-lm-simple:</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>\y_n ={}&amp; \beta_1  + \beta_2 \x_{n} + \res_{n}, \quad\textrm{For }n=1,\ldots,N.</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>The residuals $\res_n$ measure the "misfit" of the line.  If you know $\beta_1, \ldots, \beta_P$,</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>then you can compute</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>\res_n ={}&amp; \y_n -  (\beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP}).</span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>But in general we only observe $\y_n$ and $\x_{n1}, \ldots, \x_{nP}$, and we</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>choose $\beta_1, \ldots, \beta_P$ to make the residuals small.  (How we do</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>this precisely will be something we talk about at great length.)</span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>The general form of @eq-lm-scalar can be written more compactly using</span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>matrix and vector notation. Specifically, if we let</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a>\xv_n := </span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>  \x_{n1} <span class="sc">\\</span> \x_{n2} <span class="sc">\\</span> \vdots <span class="sc">\\</span> \x_{nP}</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>\textrm{and}</span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>\betav := </span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>  \beta_{1} <span class="sc">\\</span> \beta_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \beta_{P}</span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'}  </span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>Bold lowercase variables are column vectors (unless otherwise specified).</span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a>Recall that the "transpose" operator $(\cdot)^\trans$ flips the row and</span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a>columns of a matrix.  For example,</span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>\xv_n ^\trans = </span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a>  \x_{n1} &amp; \x_{n2} &amp; \ldots &amp; \x_{nP}</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a>By matrix multiplication rules,</span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a>\xv_n^\trans \betav = </span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a>  \x_{n1} &amp; \x_{n2} &amp; \ldots &amp; \x_{nP}</span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a>\quad\quad\quad</span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a>  \beta_{1} <span class="sc">\\</span> \beta_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \beta_{P}</span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a>= \beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP}.</span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'}  </span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a>I have written $\xv_n^\trans \betav$ for the "dot product" or "inner product"</span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a>between $\xv_n$ and $\betav$.  Writing it in this way clarifies the relationship</span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a>with matrix notation below.</span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a>There are many other ways to denote inner products in the literature,</span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a>including $\xv_n \cdot \betav$ and $&lt;\xv_n, \betav&gt;$.</span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a>Then we can compactly write</span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a>\y_n ={}&amp; \xv_n ^\trans \betav + \res_{n}, \quad\textrm{For }n=1,\ldots,N.</span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a>We can compactify it even further if we stack the $n$ observations:</span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a>%</span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a>\y_1 ={}&amp; \xv_1 ^\trans \betav + \res_{1} <span class="sc">\\</span></span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a>\y_2 ={}&amp; \xv_2 ^\trans \betav + \res_{2} <span class="sc">\\</span></span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a>\y_N ={}&amp; \xv_N ^\trans \betav + \res_{N} <span class="sc">\\</span></span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a>As before we can stack the responses and residuals:</span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a>\Y := </span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a>  \y_{1} <span class="sc">\\</span> \y_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \y_{P}</span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a>\textrm{and}</span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a>\resv := </span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a>  \res_{1} <span class="sc">\\</span> \res_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \res_{P}</span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a>We can also stack the regressors:</span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a>\X := </span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a>  \x_{11} &amp; \x_{12} &amp; \ldots &amp; \x_{1P}<span class="sc">\\</span></span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a>  \x_{21} &amp; \x_{22} &amp; \ldots &amp; \x_{2P}<span class="sc">\\</span></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a>  \vdots<span class="sc">\\</span></span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a>  \x_{n1} &amp; \x_{n2} &amp; \ldots &amp; \x_{nP}<span class="sc">\\</span></span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a>  \vdots<span class="sc">\\</span></span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a>    \x_{N1} &amp; \x_{N2} &amp; \ldots &amp; \x_{NP}</span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a>  \xv_{1}^\trans <span class="sc">\\</span> \xv_{2}^\trans <span class="sc">\\</span> \vdots <span class="sc">\\</span> \xv_n^\trans <span class="sc">\\</span> \vdots <span class="sc">\\</span> \xv_{N}^\trans</span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'}  </span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a>I will use upper case bold letters for multi-dimensional matrices like $\X$.  But</span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a>I may also use upper case bold letters even when the quantity could also be</span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a>a column vector, when I think it's more useful to think of the quantity</span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a>as a matrix with a single column.  Examples are $\Y$ above, or $\X$ when $P = 1$.</span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a>Note that by matrix multiplication rules,</span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a>\X  = </span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a>  \xv_{1}^\trans <span class="sc">\\</span> \xv_{2}^\trans <span class="sc">\\</span> \vdots <span class="sc">\\</span> \xv_n^\trans <span class="sc">\\</span> \vdots <span class="sc">\\</span> \xv_{N}^\trans</span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a>\quad\quad\quad</span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a>\X \betav</span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a>  \xv_{1}^\trans\betav <span class="sc">\\</span> \xv_{2}^\trans\betav <span class="sc">\\</span> \vdots <span class="sc">\\</span> \xv_n^\trans\betav <span class="sc">\\</span> \vdots <span class="sc">\\</span> \xv_{N}^\trans\betav</span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a>so we end up with the extremely tidy expression</span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a>\y_n ={}&amp; \beta_1 \x_{n1} + \beta_2 \x_{n2} + \ldots + \x_{nP} + \res_{n}, \quad\textrm{For }n=1,\ldots,N</span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a><span class="sc">\\\\</span>&amp;\textrm{is the same as}\quad<span class="sc">\\\\</span></span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a>\Y ={}&amp; \X \betav + \resv.</span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a>$${#eq-y-matrix}</span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a>In the case of simple least squares, we can write</span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a>\X := </span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a>  1 &amp; \x_{1}<span class="sc">\\</span></span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a>  1 &amp; \x_{2}<span class="sc">\\</span></span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a>  \vdots &amp; \vdots<span class="sc">\\</span></span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a>  1 &amp; \x_{N}<span class="sc">\\</span></span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a>\end{pmatrix},</span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a>$${#eq-simple_matrix}</span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-480"><a href="#cb1-480" aria-hidden="true" tabindex="-1"></a>and verify that the $n$--th row of @eq-y-matrix is the same as @eq-lm-simple.</span>
<span id="cb1-481"><a href="#cb1-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-482"><a href="#cb1-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-483"><a href="#cb1-483" aria-hidden="true" tabindex="-1"></a><span class="fu"># Least squares in matrix notation</span></span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a>Using our tidy expression @eq-y-matrix, we can easily write out the</span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a>sum of the squared errors as</span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a>\sumn \res_n^2 ={}&amp;</span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a>    \resv^\trans \resv = (\Y - \X \betav)^\trans (\Y - \X \betav) </span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a>    <span class="sc">\\</span>={}&amp;</span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a>    \Y^\trans \Y - \betav^\trans \X^\trans \Y - \Y^\trans \X \betav + \betav^\trans \X^\trans \X \betav </span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a>    <span class="sc">\\</span>={}&amp;</span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a>    \Y^\trans \Y - 2  \Y^\trans \X \betav + \betav^\trans \X^\trans \X \betav.</span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a>This is a quadratic function of the vector $\betav$.  We wish to find the minimum</span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a>of this quantity as a function of $\betav$.  We might hope that</span>
<span id="cb1-502"><a href="#cb1-502" aria-hidden="true" tabindex="-1"></a>the minimum occurs at a point where the gradient of this expression</span>
<span id="cb1-503"><a href="#cb1-503" aria-hidden="true" tabindex="-1"></a>is zero.</span>
<span id="cb1-504"><a href="#cb1-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-505"><a href="#cb1-505" aria-hidden="true" tabindex="-1"></a>Rather than compute the</span>
<span id="cb1-506"><a href="#cb1-506" aria-hidden="true" tabindex="-1"></a>*univariate* derivative with respect to each component, we can compute</span>
<span id="cb1-507"><a href="#cb1-507" aria-hidden="true" tabindex="-1"></a>the *multivariate gradient* with respect to the vector.</span>
<span id="cb1-508"><a href="#cb1-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-509"><a href="#cb1-509" aria-hidden="true" tabindex="-1"></a>Let's recall some facts from vector calculus.  </span>
<span id="cb1-510"><a href="#cb1-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-511"><a href="#cb1-511" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'}  </span>
<span id="cb1-512"><a href="#cb1-512" aria-hidden="true" tabindex="-1"></a>Take $\zv \in \mathbb{R}^P$</span>
<span id="cb1-513"><a href="#cb1-513" aria-hidden="true" tabindex="-1"></a>to be a $P$--vector.</span>
<span id="cb1-514"><a href="#cb1-514" aria-hidden="true" tabindex="-1"></a>and let $f(\zv)$ a scalar--valued function of the vector $\z$.  We write</span>
<span id="cb1-515"><a href="#cb1-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-516"><a href="#cb1-516" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-517"><a href="#cb1-517" aria-hidden="true" tabindex="-1"></a>\frac{\partial f(\zv)}{\partial \zv}  = </span>
<span id="cb1-518"><a href="#cb1-518" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-519"><a href="#cb1-519" aria-hidden="true" tabindex="-1"></a>\frac{\partial}{\partial \z_1} f(\zv) <span class="sc">\\</span> </span>
<span id="cb1-520"><a href="#cb1-520" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb1-521"><a href="#cb1-521" aria-hidden="true" tabindex="-1"></a>\frac{\partial}{\partial \z_P} f(\zv) <span class="sc">\\</span> </span>
<span id="cb1-522"><a href="#cb1-522" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-523"><a href="#cb1-523" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-524"><a href="#cb1-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-525"><a href="#cb1-525" aria-hidden="true" tabindex="-1"></a>That is, the partial $\frac{\partial f(\zv)}{\partial \zv}$ is</span>
<span id="cb1-526"><a href="#cb1-526" aria-hidden="true" tabindex="-1"></a>a $P$--vector of the stacked univariate dervatives.</span>
<span id="cb1-527"><a href="#cb1-527" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-528"><a href="#cb1-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-529"><a href="#cb1-529" aria-hidden="true" tabindex="-1"></a>Recall a couple rules from vector calculus.  Let $\vv$ denote a $P$--vector</span>
<span id="cb1-530"><a href="#cb1-530" aria-hidden="true" tabindex="-1"></a>and $\A$ a symmetric matrix.  Then</span>
<span id="cb1-531"><a href="#cb1-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-532"><a href="#cb1-532" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-533"><a href="#cb1-533" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-534"><a href="#cb1-534" aria-hidden="true" tabindex="-1"></a>\frac{\partial \vv^\trans \zv}{\partial \zv}  = \vv</span>
<span id="cb1-535"><a href="#cb1-535" aria-hidden="true" tabindex="-1"></a>\quad\textrm{and}\quad</span>
<span id="cb1-536"><a href="#cb1-536" aria-hidden="true" tabindex="-1"></a>\frac{\partial \zv^\trans \A \zv}{\partial \zv}  = 2 \A \zv.</span>
<span id="cb1-537"><a href="#cb1-537" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-538"><a href="#cb1-538" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-539"><a href="#cb1-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-540"><a href="#cb1-540" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Exercise'} </span>
<span id="cb1-541"><a href="#cb1-541" aria-hidden="true" tabindex="-1"></a>Prove these results above using univariate derivatives and our stacking convention.</span>
<span id="cb1-542"><a href="#cb1-542" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-543"><a href="#cb1-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-544"><a href="#cb1-544" aria-hidden="true" tabindex="-1"></a>Applying these two rules to our least squares objective,</span>
<span id="cb1-545"><a href="#cb1-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-546"><a href="#cb1-546" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-547"><a href="#cb1-547" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-548"><a href="#cb1-548" aria-hidden="true" tabindex="-1"></a>\frac{\partial\resv^\trans \resv }{\partial \betav} ={}&amp;</span>
<span id="cb1-549"><a href="#cb1-549" aria-hidden="true" tabindex="-1"></a>    \frac{\partial }{\partial \betav} \Y^\trans \Y - </span>
<span id="cb1-550"><a href="#cb1-550" aria-hidden="true" tabindex="-1"></a>    2 \frac{\partial }{\partial \betav}  \Y^\trans \X \betav + </span>
<span id="cb1-551"><a href="#cb1-551" aria-hidden="true" tabindex="-1"></a>    \frac{\partial }{\partial \betav} \betav^\trans \X^\trans \X \betav</span>
<span id="cb1-552"><a href="#cb1-552" aria-hidden="true" tabindex="-1"></a>    <span class="sc">\\</span> ={}&amp;</span>
<span id="cb1-553"><a href="#cb1-553" aria-hidden="true" tabindex="-1"></a>    0 - 2 \X^\trans \Y + 2 \X^\trans \X \betav.</span>
<span id="cb1-554"><a href="#cb1-554" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-555"><a href="#cb1-555" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-556"><a href="#cb1-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-557"><a href="#cb1-557" aria-hidden="true" tabindex="-1"></a>Assuming our estimator $\betahat$ sets these partial derivatives are equal to zero, we then</span>
<span id="cb1-558"><a href="#cb1-558" aria-hidden="true" tabindex="-1"></a>get</span>
<span id="cb1-559"><a href="#cb1-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-560"><a href="#cb1-560" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-561"><a href="#cb1-561" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-562"><a href="#cb1-562" aria-hidden="true" tabindex="-1"></a>\X^\trans \X \betavhat ={}&amp; \X^\trans \Y.</span>
<span id="cb1-563"><a href="#cb1-563" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-564"><a href="#cb1-564" aria-hidden="true" tabindex="-1"></a>$${#eq-ols-esteq}</span>
<span id="cb1-565"><a href="#cb1-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-566"><a href="#cb1-566" aria-hidden="true" tabindex="-1"></a>This is a set of $P$ equations in $P$ unknowns.  If it is not degenerate,</span>
<span id="cb1-567"><a href="#cb1-567" aria-hidden="true" tabindex="-1"></a>one can solve for $\betavhat$.  That is, if the matrix $\X^\trans \X$ is invertible, then we</span>
<span id="cb1-568"><a href="#cb1-568" aria-hidden="true" tabindex="-1"></a>can multiply both sides of @eq-ols-esteq by $(\X^\trans \X)^{-1}$</span>
<span id="cb1-569"><a href="#cb1-569" aria-hidden="true" tabindex="-1"></a>to get</span>
<span id="cb1-570"><a href="#cb1-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-571"><a href="#cb1-571" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-572"><a href="#cb1-572" aria-hidden="true" tabindex="-1"></a>\betavhat = (\X^\trans \X)^{-1} \X^\trans \Y</span>
<span id="cb1-573"><a href="#cb1-573" aria-hidden="true" tabindex="-1"></a>$${#eq-ols-betahat}</span>
<span id="cb1-574"><a href="#cb1-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-575"><a href="#cb1-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-576"><a href="#cb1-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-577"><a href="#cb1-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-578"><a href="#cb1-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-579"><a href="#cb1-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-580"><a href="#cb1-580" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'}  </span>
<span id="cb1-581"><a href="#cb1-581" aria-hidden="true" tabindex="-1"></a>We're going to be talking again again about the "ordinary least squares" ("OLS") problem</span>
<span id="cb1-582"><a href="#cb1-582" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-583"><a href="#cb1-583" aria-hidden="true" tabindex="-1"></a>\betavhat = \argmin{\beta} \resv^\trans \resv</span>
<span id="cb1-584"><a href="#cb1-584" aria-hidden="true" tabindex="-1"></a>\quad\textrm{where}\quad</span>
<span id="cb1-585"><a href="#cb1-585" aria-hidden="true" tabindex="-1"></a>\Y = \X \betav + \resv</span>
<span id="cb1-586"><a href="#cb1-586" aria-hidden="true" tabindex="-1"></a>\quad\textrm{where}\quad</span>
<span id="cb1-587"><a href="#cb1-587" aria-hidden="true" tabindex="-1"></a>\y_n = \xv_n^\trans \betav + \res_n \textrm{ for all }n=1,\ldots,N.</span>
<span id="cb1-588"><a href="#cb1-588" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-589"><a href="#cb1-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-590"><a href="#cb1-590" aria-hidden="true" tabindex="-1"></a>It will be nice to have some shorthand for this problem so I don't</span>
<span id="cb1-591"><a href="#cb1-591" aria-hidden="true" tabindex="-1"></a>have to write this out every time.  All of the following will be understood</span>
<span id="cb1-592"><a href="#cb1-592" aria-hidden="true" tabindex="-1"></a>as shorthand for the preceding problem.  In each, the fact that</span>
<span id="cb1-593"><a href="#cb1-593" aria-hidden="true" tabindex="-1"></a>$\betavhat$ minimizes the sum of squared residuals is implicit.</span>
<span id="cb1-594"><a href="#cb1-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-595"><a href="#cb1-595" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-596"><a href="#cb1-596" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-597"><a href="#cb1-597" aria-hidden="true" tabindex="-1"></a>\Y \sim{}&amp; \X \betav + \resv&amp; \textrm{Only the least squares criterion is implicit}<span class="sc">\\</span></span>
<span id="cb1-598"><a href="#cb1-598" aria-hidden="true" tabindex="-1"></a>\Y \sim{}&amp; \X \betav &amp; \textrm{$\resv$ implicit}<span class="sc">\\</span></span>
<span id="cb1-599"><a href="#cb1-599" aria-hidden="true" tabindex="-1"></a>\Y \sim{}&amp; \X &amp; \textrm{$\resv$, $\betav$ implicit}<span class="sc">\\</span></span>
<span id="cb1-600"><a href="#cb1-600" aria-hidden="true" tabindex="-1"></a>\y_n \sim{}&amp; \xv_n^\trans \betav &amp; \textrm{$\res_n$, $N$ implicit}<span class="sc">\\</span></span>
<span id="cb1-601"><a href="#cb1-601" aria-hidden="true" tabindex="-1"></a>\y_n \sim{}&amp; \xv_n^\trans \betav + \res_n &amp; \textrm{$N$ implicit}<span class="sc">\\</span></span>
<span id="cb1-602"><a href="#cb1-602" aria-hidden="true" tabindex="-1"></a>\y_n \sim{}&amp; \xv_n &amp; \textrm{$\res_n$, $\betav$, $N$ implicit}<span class="sc">\\</span></span>
<span id="cb1-603"><a href="#cb1-603" aria-hidden="true" tabindex="-1"></a>\y_n \sim{}&amp; \x_{n1} + \x_{n2} + \ldots + \x_{nP} &amp; \textrm{$\res_n$, $\betav$, $N$ implicit}<span class="sc">\\</span></span>
<span id="cb1-604"><a href="#cb1-604" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-605"><a href="#cb1-605" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-606"><a href="#cb1-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-607"><a href="#cb1-607" aria-hidden="true" tabindex="-1"></a>(The final shorthand is closest to the notation for the <span class="in">`R`</span> <span class="in">`lm`</span> function.)  This is convenient</span>
<span id="cb1-608"><a href="#cb1-608" aria-hidden="true" tabindex="-1"></a>because, for example, certain properties of the regression $\y_n \sim \xv_n$ don't necessarily</span>
<span id="cb1-609"><a href="#cb1-609" aria-hidden="true" tabindex="-1"></a>need to commit to which symbol we use for the coefficients.  Symbols for</span>
<span id="cb1-610"><a href="#cb1-610" aria-hidden="true" tabindex="-1"></a>the missing pieces will hopefuly be clear from context as necessary.  </span>
<span id="cb1-611"><a href="#cb1-611" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-612"><a href="#cb1-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-613"><a href="#cb1-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-614"><a href="#cb1-614" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'}  </span>
<span id="cb1-615"><a href="#cb1-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-616"><a href="#cb1-616" aria-hidden="true" tabindex="-1"></a>Unlike many other regression texts (and the <span class="in">`lm`</span> function), I will *not* necessarily</span>
<span id="cb1-617"><a href="#cb1-617" aria-hidden="true" tabindex="-1"></a>assume that a constant is included in the regression.  One can always take a generic</span>
<span id="cb1-618"><a href="#cb1-618" aria-hidden="true" tabindex="-1"></a>regression $\y_n \sim \xv_n$ to include a constant by assuming that one of the entries</span>
<span id="cb1-619"><a href="#cb1-619" aria-hidden="true" tabindex="-1"></a>of $\xv_n$ is one.  At some points my convention of not including a constant by default</span>
<span id="cb1-620"><a href="#cb1-620" aria-hidden="true" tabindex="-1"></a>will lead to formulas that may be at odds with some textbooks.  But these differences</span>
<span id="cb1-621"><a href="#cb1-621" aria-hidden="true" tabindex="-1"></a>are superficial, and are, in my mind, more than made up for by the generality and simplicity</span>
<span id="cb1-622"><a href="#cb1-622" aria-hidden="true" tabindex="-1"></a>of treating constants as just another regressor.</span>
<span id="cb1-623"><a href="#cb1-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-624"><a href="#cb1-624" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-625"><a href="#cb1-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-626"><a href="#cb1-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-627"><a href="#cb1-627" aria-hidden="true" tabindex="-1"></a><span class="fu"># Some examples</span></span>
<span id="cb1-628"><a href="#cb1-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-629"><a href="#cb1-629" aria-hidden="true" tabindex="-1"></a>The formulas given in @eq-ols-esteq and @eq-ols-betahat are enough to calculate every</span>
<span id="cb1-630"><a href="#cb1-630" aria-hidden="true" tabindex="-1"></a>least squares estimator we'll encounter.  But we'd like to have intuition for</span>
<span id="cb1-631"><a href="#cb1-631" aria-hidden="true" tabindex="-1"></a>the meaning of the formulas, and for that it will be useful to start</span>
<span id="cb1-632"><a href="#cb1-632" aria-hidden="true" tabindex="-1"></a>by applying the formulas in some familiar settings.</span>
<span id="cb1-633"><a href="#cb1-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-634"><a href="#cb1-634" aria-hidden="true" tabindex="-1"></a>Recall that, any fit of the form $\X \betav$ that minimizes the sum of squared errors </span>
<span id="cb1-635"><a href="#cb1-635" aria-hidden="true" tabindex="-1"></a>must take the form $\X \betavhat$ where $\betavhat$ satisfies</span>
<span id="cb1-636"><a href="#cb1-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-637"><a href="#cb1-637" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-638"><a href="#cb1-638" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-639"><a href="#cb1-639" aria-hidden="true" tabindex="-1"></a>\X^\trans \X \betavhat ={}&amp; \X^\trans \Y.</span>
<span id="cb1-640"><a href="#cb1-640" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-641"><a href="#cb1-641" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-642"><a href="#cb1-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-643"><a href="#cb1-643" aria-hidden="true" tabindex="-1"></a>However, there may in general be many $\betavhat$ that satisfy the</span>
<span id="cb1-644"><a href="#cb1-644" aria-hidden="true" tabindex="-1"></a>preceding equation.  But if $\X^\trans \X$ is invertible, then</span>
<span id="cb1-645"><a href="#cb1-645" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-646"><a href="#cb1-646" aria-hidden="true" tabindex="-1"></a>\betavhat = (\X^\trans \X)^{-1} \X^\trans \Y,</span>
<span id="cb1-647"><a href="#cb1-647" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-648"><a href="#cb1-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-649"><a href="#cb1-649" aria-hidden="true" tabindex="-1"></a>and there is only one $\betavhat$ that minimizes the sum of</span>
<span id="cb1-650"><a href="#cb1-650" aria-hidden="true" tabindex="-1"></a>squared error.</span>
<span id="cb1-651"><a href="#cb1-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-652"><a href="#cb1-652" aria-hidden="true" tabindex="-1"></a>Let's take a careful look at what $\X^\trans \X$ is measuring,</span>
<span id="cb1-653"><a href="#cb1-653" aria-hidden="true" tabindex="-1"></a>what it means for it to be invertible, as well as what</span>
<span id="cb1-654"><a href="#cb1-654" aria-hidden="true" tabindex="-1"></a>it means when it is *not* invertible.</span>
<span id="cb1-655"><a href="#cb1-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-656"><a href="#cb1-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-657"><a href="#cb1-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-658"><a href="#cb1-658" aria-hidden="true" tabindex="-1"></a><span class="fu">## The sample mean</span></span>
<span id="cb1-659"><a href="#cb1-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-660"><a href="#cb1-660" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title='Notation'}  </span>
<span id="cb1-661"><a href="#cb1-661" aria-hidden="true" tabindex="-1"></a>I will use $\onev$ to denote a vector full of ones.  Usually it</span>
<span id="cb1-662"><a href="#cb1-662" aria-hidden="true" tabindex="-1"></a>will be an $N$--vector, but sometimes its dimension will just be</span>
<span id="cb1-663"><a href="#cb1-663" aria-hidden="true" tabindex="-1"></a>implicit.  Similarly, $\zerov$ is a vector of zeros.</span>
<span id="cb1-664"><a href="#cb1-664" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-665"><a href="#cb1-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-666"><a href="#cb1-666" aria-hidden="true" tabindex="-1"></a>We showed earlier that the sample mean is a special case of the</span>
<span id="cb1-667"><a href="#cb1-667" aria-hidden="true" tabindex="-1"></a>regression $\y_n \sim 1 \cdot \beta$.  This can be expressed in matrix notation</span>
<span id="cb1-668"><a href="#cb1-668" aria-hidden="true" tabindex="-1"></a>by taking $\X = \onev$ as a $N\times 1$ vector.  We then have</span>
<span id="cb1-669"><a href="#cb1-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-670"><a href="#cb1-670" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-671"><a href="#cb1-671" aria-hidden="true" tabindex="-1"></a>\X^\trans \X = \onev^\trans \onev = \sumn 1 \cdot 1 = N,</span>
<span id="cb1-672"><a href="#cb1-672" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-673"><a href="#cb1-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-674"><a href="#cb1-674" aria-hidden="true" tabindex="-1"></a>so $\X^\trans \X$ is invertible as long as $N &gt; 0$ (i.e., if you</span>
<span id="cb1-675"><a href="#cb1-675" aria-hidden="true" tabindex="-1"></a>have at least one datapoint), with $(\X^\trans \X)^{-1} = 1/N$.   We also have</span>
<span id="cb1-676"><a href="#cb1-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-677"><a href="#cb1-677" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-678"><a href="#cb1-678" aria-hidden="true" tabindex="-1"></a>\X^\trans \Y = \onev^\trans \Y = \sumn 1 \cdot \y_n = N \ybar,</span>
<span id="cb1-679"><a href="#cb1-679" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-680"><a href="#cb1-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-681"><a href="#cb1-681" aria-hidden="true" tabindex="-1"></a>and so</span>
<span id="cb1-682"><a href="#cb1-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-683"><a href="#cb1-683" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-684"><a href="#cb1-684" aria-hidden="true" tabindex="-1"></a>\betahat = (\X^\trans \X)^{-1}  \X^\trans \Y = (\onev^\trans \onev)^{-1} \onev^\trans \Y = \frac{N \ybar}{N} = \ybar,</span>
<span id="cb1-685"><a href="#cb1-685" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-686"><a href="#cb1-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-687"><a href="#cb1-687" aria-hidden="true" tabindex="-1"></a>as expected.</span>
<span id="cb1-688"><a href="#cb1-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-689"><a href="#cb1-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-690"><a href="#cb1-690" aria-hidden="true" tabindex="-1"></a><span class="fu">## A single regressor</span></span>
<span id="cb1-691"><a href="#cb1-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-692"><a href="#cb1-692" aria-hidden="true" tabindex="-1"></a>Suppose that we regress $\y_n \sim \x_n$ where $\x_n$ is a scalar.  Let's suppose</span>
<span id="cb1-693"><a href="#cb1-693" aria-hidden="true" tabindex="-1"></a>that $\expect{\x_n} = 0$ and $\var{\x_n} = \sigma^2 &gt; 0$.  We have</span>
<span id="cb1-694"><a href="#cb1-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-695"><a href="#cb1-695" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-696"><a href="#cb1-696" aria-hidden="true" tabindex="-1"></a>\X = \begin{pmatrix}</span>
<span id="cb1-697"><a href="#cb1-697" aria-hidden="true" tabindex="-1"></a>\x_1 <span class="sc">\\</span></span>
<span id="cb1-698"><a href="#cb1-698" aria-hidden="true" tabindex="-1"></a>\x_2 <span class="sc">\\</span></span>
<span id="cb1-699"><a href="#cb1-699" aria-hidden="true" tabindex="-1"></a>\vdots<span class="sc">\\</span></span>
<span id="cb1-700"><a href="#cb1-700" aria-hidden="true" tabindex="-1"></a>\x_N</span>
<span id="cb1-701"><a href="#cb1-701" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-702"><a href="#cb1-702" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-703"><a href="#cb1-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-704"><a href="#cb1-704" aria-hidden="true" tabindex="-1"></a>so </span>
<span id="cb1-705"><a href="#cb1-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-706"><a href="#cb1-706" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-707"><a href="#cb1-707" aria-hidden="true" tabindex="-1"></a>\X^\trans \X = \sumn \x_n^2.</span>
<span id="cb1-708"><a href="#cb1-708" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-709"><a href="#cb1-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-710"><a href="#cb1-710" aria-hidden="true" tabindex="-1"></a>Depending on the distribution of $\x_n$, it may be possible for $\X^\trans \X$ to be</span>
<span id="cb1-711"><a href="#cb1-711" aria-hidden="true" tabindex="-1"></a>non--invertible!</span>
<span id="cb1-712"><a href="#cb1-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-713"><a href="#cb1-713" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Exercise'} </span>
<span id="cb1-714"><a href="#cb1-714" aria-hidden="true" tabindex="-1"></a>Produce a distribution for $\x_n$ where $\X^\trans \X$ is non--invertible with</span>
<span id="cb1-715"><a href="#cb1-715" aria-hidden="true" tabindex="-1"></a>positive probability for any $N$.</span>
<span id="cb1-716"><a href="#cb1-716" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-717"><a href="#cb1-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-718"><a href="#cb1-718" aria-hidden="true" tabindex="-1"></a>However, as $N \rightarrow \infty$, $\frac{1}{N} \X^\trans \X \rightarrow \sigma^2$</span>
<span id="cb1-719"><a href="#cb1-719" aria-hidden="true" tabindex="-1"></a>by the LLN, and since $\sigma^2 &gt; 0$, $\frac{1}{N} \X^\trans \X$ will be invertible</span>
<span id="cb1-720"><a href="#cb1-720" aria-hidden="true" tabindex="-1"></a>with probability approaching one as $N$ goes to infinity.</span>
<span id="cb1-721"><a href="#cb1-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-722"><a href="#cb1-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-723"><a href="#cb1-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-724"><a href="#cb1-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-725"><a href="#cb1-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-726"><a href="#cb1-726" aria-hidden="true" tabindex="-1"></a><span class="fu">## One--hot encodings</span></span>
<span id="cb1-727"><a href="#cb1-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-728"><a href="#cb1-728" aria-hidden="true" tabindex="-1"></a>We discussed one-hot encodings in the context of the Ames housing data.  Suppose we have </span>
<span id="cb1-729"><a href="#cb1-729" aria-hidden="true" tabindex="-1"></a>a columns $k_n \in <span class="sc">\{</span> g, e<span class="sc">\}</span>$ indicating whether a kitchen is "good" or "excellent".</span>
<span id="cb1-730"><a href="#cb1-730" aria-hidden="true" tabindex="-1"></a>A one--hot encoding of this categorical variable is given by</span>
<span id="cb1-731"><a href="#cb1-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-732"><a href="#cb1-732" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-733"><a href="#cb1-733" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-734"><a href="#cb1-734" aria-hidden="true" tabindex="-1"></a>\x_{ng} = </span>
<span id="cb1-735"><a href="#cb1-735" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-736"><a href="#cb1-736" aria-hidden="true" tabindex="-1"></a>1 &amp; \textrm{ if }k_n = g <span class="sc">\\</span></span>
<span id="cb1-737"><a href="#cb1-737" aria-hidden="true" tabindex="-1"></a>0 &amp; \textrm{ if }k_n \ne g <span class="sc">\\</span></span>
<span id="cb1-738"><a href="#cb1-738" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-739"><a href="#cb1-739" aria-hidden="true" tabindex="-1"></a>&amp;&amp;</span>
<span id="cb1-740"><a href="#cb1-740" aria-hidden="true" tabindex="-1"></a>\x_{ne} = </span>
<span id="cb1-741"><a href="#cb1-741" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-742"><a href="#cb1-742" aria-hidden="true" tabindex="-1"></a>1 &amp; \textrm{ if }k_n = e <span class="sc">\\</span></span>
<span id="cb1-743"><a href="#cb1-743" aria-hidden="true" tabindex="-1"></a>0 &amp; \textrm{ if }k_n \ne e <span class="sc">\\</span></span>
<span id="cb1-744"><a href="#cb1-744" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-745"><a href="#cb1-745" aria-hidden="true" tabindex="-1"></a>\end{aligned}.</span>
<span id="cb1-746"><a href="#cb1-746" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-747"><a href="#cb1-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-748"><a href="#cb1-748" aria-hidden="true" tabindex="-1"></a>We can then regress $\y_n \sim \beta_g \x_{ng} + \beta_e \x_{ne} = \x_n^\trans \betav$.  The corresponding $\X$ matrix</span>
<span id="cb1-749"><a href="#cb1-749" aria-hidden="true" tabindex="-1"></a>might look like</span>
<span id="cb1-750"><a href="#cb1-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-751"><a href="#cb1-751" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-752"><a href="#cb1-752" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-753"><a href="#cb1-753" aria-hidden="true" tabindex="-1"></a>\mybold{k} = </span>
<span id="cb1-754"><a href="#cb1-754" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-755"><a href="#cb1-755" aria-hidden="true" tabindex="-1"></a>g <span class="sc">\\</span></span>
<span id="cb1-756"><a href="#cb1-756" aria-hidden="true" tabindex="-1"></a>e <span class="sc">\\</span></span>
<span id="cb1-757"><a href="#cb1-757" aria-hidden="true" tabindex="-1"></a>g <span class="sc">\\</span></span>
<span id="cb1-758"><a href="#cb1-758" aria-hidden="true" tabindex="-1"></a>g <span class="sc">\\</span></span>
<span id="cb1-759"><a href="#cb1-759" aria-hidden="true" tabindex="-1"></a>\vdots</span>
<span id="cb1-760"><a href="#cb1-760" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-761"><a href="#cb1-761" aria-hidden="true" tabindex="-1"></a>&amp;&amp;</span>
<span id="cb1-762"><a href="#cb1-762" aria-hidden="true" tabindex="-1"></a>\X = (\xv_g \quad \xv_e) =</span>
<span id="cb1-763"><a href="#cb1-763" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-764"><a href="#cb1-764" aria-hidden="true" tabindex="-1"></a>1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-765"><a href="#cb1-765" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-766"><a href="#cb1-766" aria-hidden="true" tabindex="-1"></a>1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-767"><a href="#cb1-767" aria-hidden="true" tabindex="-1"></a>1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-768"><a href="#cb1-768" aria-hidden="true" tabindex="-1"></a>\vdots</span>
<span id="cb1-769"><a href="#cb1-769" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-770"><a href="#cb1-770" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-771"><a href="#cb1-771" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-772"><a href="#cb1-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-773"><a href="#cb1-773" aria-hidden="true" tabindex="-1"></a>Note that $\xv_g^\trans \xv_g$ is just the number of entries with $k_n = g$, and $\xv_g^\trans \xv_e = 0$</span>
<span id="cb1-774"><a href="#cb1-774" aria-hidden="true" tabindex="-1"></a>because a kitchen is either good or excellent but never both.</span>
<span id="cb1-775"><a href="#cb1-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-776"><a href="#cb1-776" aria-hidden="true" tabindex="-1"></a>We then have </span>
<span id="cb1-777"><a href="#cb1-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-778"><a href="#cb1-778" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-779"><a href="#cb1-779" aria-hidden="true" tabindex="-1"></a>\X^\trans \X =</span>
<span id="cb1-780"><a href="#cb1-780" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-781"><a href="#cb1-781" aria-hidden="true" tabindex="-1"></a>\xv_g^\trans \xv_g  &amp; \xv_g^\trans \xv_e <span class="sc">\\</span></span>
<span id="cb1-782"><a href="#cb1-782" aria-hidden="true" tabindex="-1"></a>\xv_e^\trans \xv_g  &amp; \xv_e^\trans \xv_e <span class="sc">\\</span></span>
<span id="cb1-783"><a href="#cb1-783" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} =</span>
<span id="cb1-784"><a href="#cb1-784" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-785"><a href="#cb1-785" aria-hidden="true" tabindex="-1"></a>N_g  &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-786"><a href="#cb1-786" aria-hidden="true" tabindex="-1"></a>0  &amp; N_e <span class="sc">\\</span></span>
<span id="cb1-787"><a href="#cb1-787" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-788"><a href="#cb1-788" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-789"><a href="#cb1-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-790"><a href="#cb1-790" aria-hidden="true" tabindex="-1"></a>Then $\X^\trans \X$ is invertible as long as $N_g &gt; 0$ and $N_e &gt; 0$, that is, as</span>
<span id="cb1-791"><a href="#cb1-791" aria-hidden="true" tabindex="-1"></a>long as we have at least one observation of each kitchen type, and</span>
<span id="cb1-792"><a href="#cb1-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-793"><a href="#cb1-793" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-794"><a href="#cb1-794" aria-hidden="true" tabindex="-1"></a>\left(\X^\trans \X\right)^{-1} =</span>
<span id="cb1-795"><a href="#cb1-795" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-796"><a href="#cb1-796" aria-hidden="true" tabindex="-1"></a>\frac{1}{N_g}  &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-797"><a href="#cb1-797" aria-hidden="true" tabindex="-1"></a>0  &amp; \frac{1}{N_e} <span class="sc">\\</span></span>
<span id="cb1-798"><a href="#cb1-798" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-799"><a href="#cb1-799" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-800"><a href="#cb1-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-801"><a href="#cb1-801" aria-hidden="true" tabindex="-1"></a>Similarly, $\xv_g^\trans \Y$ is just the sum of entries of $\Y$ where $k_n = g$,</span>
<span id="cb1-802"><a href="#cb1-802" aria-hidden="true" tabindex="-1"></a>with the analogous conclusion for $\xv_e$.  From this we recover the result that</span>
<span id="cb1-803"><a href="#cb1-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-804"><a href="#cb1-804" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-805"><a href="#cb1-805" aria-hidden="true" tabindex="-1"></a>\betavhat = (\X^\trans \X)^{-1} \X^\trans \Y </span>
<span id="cb1-806"><a href="#cb1-806" aria-hidden="true" tabindex="-1"></a> =</span>
<span id="cb1-807"><a href="#cb1-807" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-808"><a href="#cb1-808" aria-hidden="true" tabindex="-1"></a>\frac{1}{N_g}  &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-809"><a href="#cb1-809" aria-hidden="true" tabindex="-1"></a>0  &amp; \frac{1}{N_e} <span class="sc">\\</span></span>
<span id="cb1-810"><a href="#cb1-810" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-811"><a href="#cb1-811" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-812"><a href="#cb1-812" aria-hidden="true" tabindex="-1"></a>\sum_{n: k_n=g} \y_n <span class="sc">\\</span></span>
<span id="cb1-813"><a href="#cb1-813" aria-hidden="true" tabindex="-1"></a>\sum_{n: k_n=e} \y_n</span>
<span id="cb1-814"><a href="#cb1-814" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-815"><a href="#cb1-815" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-816"><a href="#cb1-816" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-817"><a href="#cb1-817" aria-hidden="true" tabindex="-1"></a>\frac{1}{N_g} \sum_{n: k_n=g} \y_n <span class="sc">\\</span></span>
<span id="cb1-818"><a href="#cb1-818" aria-hidden="true" tabindex="-1"></a>\frac{1}{N_e} \sum_{n: k_n=e} \y_n</span>
<span id="cb1-819"><a href="#cb1-819" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-820"><a href="#cb1-820" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-821"><a href="#cb1-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-822"><a href="#cb1-822" aria-hidden="true" tabindex="-1"></a>If we let $\ybar_e$ and $\ybar_g$ denote the sample means within each group, we</span>
<span id="cb1-823"><a href="#cb1-823" aria-hidden="true" tabindex="-1"></a>have shows that $\betahat_g = \ybar_g$ and $\betahat_e = \ybar_e$, as we</span>
<span id="cb1-824"><a href="#cb1-824" aria-hidden="true" tabindex="-1"></a>proved before without using the matrix formulation.</span>
<span id="cb1-825"><a href="#cb1-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-826"><a href="#cb1-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-827"><a href="#cb1-827" aria-hidden="true" tabindex="-1"></a><span class="fu">## One--hot encodings and constants</span></span>
<span id="cb1-828"><a href="#cb1-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-829"><a href="#cb1-829" aria-hidden="true" tabindex="-1"></a>Recall in the Ames housing data, we ran the following two regressions:</span>
<span id="cb1-830"><a href="#cb1-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-831"><a href="#cb1-831" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-832"><a href="#cb1-832" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-833"><a href="#cb1-833" aria-hidden="true" tabindex="-1"></a>\y_n \sim{}&amp; \beta_e \x_{ne} + \beta_g \x_{ng}  <span class="sc">\\</span></span>
<span id="cb1-834"><a href="#cb1-834" aria-hidden="true" tabindex="-1"></a>\y_n \sim{}&amp; \gamma_0  + \gamma_g \x_{ng} + \res_n = \z_n^\trans \gammav,</span>
<span id="cb1-835"><a href="#cb1-835" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-836"><a href="#cb1-836" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-837"><a href="#cb1-837" aria-hidden="true" tabindex="-1"></a>where I take $\gammav = (\gamma_0, \gamma_g)^\trans$ and $\z_n = (1, \x_{ng})^\trans$.</span>
<span id="cb1-838"><a href="#cb1-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-839"><a href="#cb1-839" aria-hidden="true" tabindex="-1"></a>We found using <span class="in">`R`</span> that the best fits were given by</span>
<span id="cb1-840"><a href="#cb1-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-841"><a href="#cb1-841" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-842"><a href="#cb1-842" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-843"><a href="#cb1-843" aria-hidden="true" tabindex="-1"></a>\betahat_e =&amp; \ybar_e  &amp; \betahat_g =&amp; \ybar_g <span class="sc">\\</span></span>
<span id="cb1-844"><a href="#cb1-844" aria-hidden="true" tabindex="-1"></a>\gammahat_0 =&amp; \ybar_e  &amp; \gammahat_g =&amp; \ybar_g - \ybar_e <span class="sc">\\</span></span>
<span id="cb1-845"><a href="#cb1-845" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-846"><a href="#cb1-846" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-847"><a href="#cb1-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-848"><a href="#cb1-848" aria-hidden="true" tabindex="-1"></a>We can compute the latter by constructing the $\Z$ matrix whose rows</span>
<span id="cb1-849"><a href="#cb1-849" aria-hidden="true" tabindex="-1"></a>are $\z_n^\trans$.  (We use $\Z$ to differentiate the $\X$ matrix from</span>
<span id="cb1-850"><a href="#cb1-850" aria-hidden="true" tabindex="-1"></a>the previous example.)  Using similar reasoning to the one--hot encoding,</span>
<span id="cb1-851"><a href="#cb1-851" aria-hidden="true" tabindex="-1"></a>we see that</span>
<span id="cb1-852"><a href="#cb1-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-853"><a href="#cb1-853" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-854"><a href="#cb1-854" aria-hidden="true" tabindex="-1"></a>\Z^\trans \Z =</span>
<span id="cb1-855"><a href="#cb1-855" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-856"><a href="#cb1-856" aria-hidden="true" tabindex="-1"></a>N &amp; N_g <span class="sc">\\</span></span>
<span id="cb1-857"><a href="#cb1-857" aria-hidden="true" tabindex="-1"></a>N_g &amp; N_g </span>
<span id="cb1-858"><a href="#cb1-858" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb1-859"><a href="#cb1-859" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-860"><a href="#cb1-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-861"><a href="#cb1-861" aria-hidden="true" tabindex="-1"></a>This is invertible as long as $N_g \ne N$, i.e., as long as there is at least</span>
<span id="cb1-862"><a href="#cb1-862" aria-hidden="true" tabindex="-1"></a>one $k_n = e$.  We have</span>
<span id="cb1-863"><a href="#cb1-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-864"><a href="#cb1-864" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-865"><a href="#cb1-865" aria-hidden="true" tabindex="-1"></a>(\Z^\trans \Z)^{-1} =</span>
<span id="cb1-866"><a href="#cb1-866" aria-hidden="true" tabindex="-1"></a>\frac{1}{N_g (N - N_g)}</span>
<span id="cb1-867"><a href="#cb1-867" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-868"><a href="#cb1-868" aria-hidden="true" tabindex="-1"></a>N_g &amp; -N_g <span class="sc">\\</span></span>
<span id="cb1-869"><a href="#cb1-869" aria-hidden="true" tabindex="-1"></a>-N_g &amp; N </span>
<span id="cb1-870"><a href="#cb1-870" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-871"><a href="#cb1-871" aria-hidden="true" tabindex="-1"></a>\quad\textrm{and}\quad</span>
<span id="cb1-872"><a href="#cb1-872" aria-hidden="true" tabindex="-1"></a>\Z^\trans \Y = </span>
<span id="cb1-873"><a href="#cb1-873" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-874"><a href="#cb1-874" aria-hidden="true" tabindex="-1"></a>\sumn \y_n <span class="sc">\\</span></span>
<span id="cb1-875"><a href="#cb1-875" aria-hidden="true" tabindex="-1"></a>\sum_{n: k_n=g} \y_n <span class="sc">\\</span></span>
<span id="cb1-876"><a href="#cb1-876" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-877"><a href="#cb1-877" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-878"><a href="#cb1-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-879"><a href="#cb1-879" aria-hidden="true" tabindex="-1"></a>It is possible (but a little tedious) to prove $\gammahat_0 = \ybar_e$ and $\gammahat_g = \ybar_g - \ybar_e$</span>
<span id="cb1-880"><a href="#cb1-880" aria-hidden="true" tabindex="-1"></a>using these formulas.  But an easier way to see it is as follows. </span>
<span id="cb1-881"><a href="#cb1-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-882"><a href="#cb1-882" aria-hidden="true" tabindex="-1"></a>Note that $\x_{ne} + \x_{ng} = 1$.  That means we can always re-write the regression with a constant as</span>
<span id="cb1-883"><a href="#cb1-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-884"><a href="#cb1-884" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-885"><a href="#cb1-885" aria-hidden="true" tabindex="-1"></a>\y_n \sim \gamma_0 + \gamma_g \x_{ng} = \gamma_0 (\x_{ne} + \x_{ng}) + \gamma_g \x_{ng} =</span>
<span id="cb1-886"><a href="#cb1-886" aria-hidden="true" tabindex="-1"></a>\gamma_0 \x_{ne} + (\gamma_0 + \gamma_g) \x_{ng}.</span>
<span id="cb1-887"><a href="#cb1-887" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-888"><a href="#cb1-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-889"><a href="#cb1-889" aria-hidden="true" tabindex="-1"></a>Now, we already know from the one--hot encoding case that the sum of squared residuals</span>
<span id="cb1-890"><a href="#cb1-890" aria-hidden="true" tabindex="-1"></a>is minimized by setting $\gammahat_0 = \ybar_e$ and $\gammahat_0 + \gammahat_g = \ybar_g$.  We</span>
<span id="cb1-891"><a href="#cb1-891" aria-hidden="true" tabindex="-1"></a>can then solve for $\gammahat_g = \ybar_g - \ybar_e$, as expected.</span>
<span id="cb1-892"><a href="#cb1-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-893"><a href="#cb1-893" aria-hidden="true" tabindex="-1"></a>This is case where we have two regressions whose regressors are invertible linear combinations</span>
<span id="cb1-894"><a href="#cb1-894" aria-hidden="true" tabindex="-1"></a>of one another:</span>
<span id="cb1-895"><a href="#cb1-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-896"><a href="#cb1-896" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-897"><a href="#cb1-897" aria-hidden="true" tabindex="-1"></a>\zv_n =</span>
<span id="cb1-898"><a href="#cb1-898" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-899"><a href="#cb1-899" aria-hidden="true" tabindex="-1"></a>1 <span class="sc">\\</span></span>
<span id="cb1-900"><a href="#cb1-900" aria-hidden="true" tabindex="-1"></a>\x_{ng}</span>
<span id="cb1-901"><a href="#cb1-901" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} =</span>
<span id="cb1-902"><a href="#cb1-902" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-903"><a href="#cb1-903" aria-hidden="true" tabindex="-1"></a>\x_{ne} + \x_{ng} <span class="sc">\\</span></span>
<span id="cb1-904"><a href="#cb1-904" aria-hidden="true" tabindex="-1"></a>\x_{ng}</span>
<span id="cb1-905"><a href="#cb1-905" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-906"><a href="#cb1-906" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-907"><a href="#cb1-907" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-908"><a href="#cb1-908" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-909"><a href="#cb1-909" aria-hidden="true" tabindex="-1"></a>1 &amp; 0<span class="sc">\\</span></span>
<span id="cb1-910"><a href="#cb1-910" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-911"><a href="#cb1-911" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-912"><a href="#cb1-912" aria-hidden="true" tabindex="-1"></a>\x_{ng} <span class="sc">\\</span></span>
<span id="cb1-913"><a href="#cb1-913" aria-hidden="true" tabindex="-1"></a>\x_{ne}</span>
<span id="cb1-914"><a href="#cb1-914" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-915"><a href="#cb1-915" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-916"><a href="#cb1-916" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-917"><a href="#cb1-917" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-918"><a href="#cb1-918" aria-hidden="true" tabindex="-1"></a>1 &amp; 0<span class="sc">\\</span></span>
<span id="cb1-919"><a href="#cb1-919" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-920"><a href="#cb1-920" aria-hidden="true" tabindex="-1"></a>\xv_n.</span>
<span id="cb1-921"><a href="#cb1-921" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-922"><a href="#cb1-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-923"><a href="#cb1-923" aria-hidden="true" tabindex="-1"></a>It follows that if you can acheive a least squares fit with $\xv_n^\trans \betavhat$, you can</span>
<span id="cb1-924"><a href="#cb1-924" aria-hidden="true" tabindex="-1"></a>achieve exactly the same fit with </span>
<span id="cb1-925"><a href="#cb1-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-926"><a href="#cb1-926" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-927"><a href="#cb1-927" aria-hidden="true" tabindex="-1"></a>\betavhat^\trans \xv_n =  </span>
<span id="cb1-928"><a href="#cb1-928" aria-hidden="true" tabindex="-1"></a>\betavhat^\trans </span>
<span id="cb1-929"><a href="#cb1-929" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-930"><a href="#cb1-930" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-931"><a href="#cb1-931" aria-hidden="true" tabindex="-1"></a>1 &amp; 0<span class="sc">\\</span></span>
<span id="cb1-932"><a href="#cb1-932" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}^{-1} \zv_n,</span>
<span id="cb1-933"><a href="#cb1-933" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-934"><a href="#cb1-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-935"><a href="#cb1-935" aria-hidden="true" tabindex="-1"></a>which can be achieved by taking</span>
<span id="cb1-936"><a href="#cb1-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-937"><a href="#cb1-937" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-938"><a href="#cb1-938" aria-hidden="true" tabindex="-1"></a>\gammavhat^\trans = </span>
<span id="cb1-939"><a href="#cb1-939" aria-hidden="true" tabindex="-1"></a>\betavhat^\trans </span>
<span id="cb1-940"><a href="#cb1-940" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-941"><a href="#cb1-941" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-942"><a href="#cb1-942" aria-hidden="true" tabindex="-1"></a>1 &amp; 0<span class="sc">\\</span></span>
<span id="cb1-943"><a href="#cb1-943" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}^{-1} \Rightarrow</span>
<span id="cb1-944"><a href="#cb1-944" aria-hidden="true" tabindex="-1"></a>\gammavhat = </span>
<span id="cb1-945"><a href="#cb1-945" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-946"><a href="#cb1-946" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-947"><a href="#cb1-947" aria-hidden="true" tabindex="-1"></a>1 &amp; 0<span class="sc">\\</span></span>
<span id="cb1-948"><a href="#cb1-948" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}^{-T} \betavhat</span>
<span id="cb1-949"><a href="#cb1-949" aria-hidden="true" tabindex="-1"></a>= </span>
<span id="cb1-950"><a href="#cb1-950" aria-hidden="true" tabindex="-1"></a>\frac{1}{-1}</span>
<span id="cb1-951"><a href="#cb1-951" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-952"><a href="#cb1-952" aria-hidden="true" tabindex="-1"></a>0 &amp; -1 <span class="sc">\\</span></span>
<span id="cb1-953"><a href="#cb1-953" aria-hidden="true" tabindex="-1"></a>-1 &amp; 1<span class="sc">\\</span></span>
<span id="cb1-954"><a href="#cb1-954" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} \betavhat</span>
<span id="cb1-955"><a href="#cb1-955" aria-hidden="true" tabindex="-1"></a>= </span>
<span id="cb1-956"><a href="#cb1-956" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-957"><a href="#cb1-957" aria-hidden="true" tabindex="-1"></a>\betahat_2 <span class="sc">\\</span></span>
<span id="cb1-958"><a href="#cb1-958" aria-hidden="true" tabindex="-1"></a>\betahat_1 - \betahat_2 <span class="sc">\\</span></span>
<span id="cb1-959"><a href="#cb1-959" aria-hidden="true" tabindex="-1"></a>\end{pmatrix},</span>
<span id="cb1-960"><a href="#cb1-960" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-961"><a href="#cb1-961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-962"><a href="#cb1-962" aria-hidden="true" tabindex="-1"></a>exactly as expected.</span>
<span id="cb1-963"><a href="#cb1-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-964"><a href="#cb1-964" aria-hidden="true" tabindex="-1"></a>We will see this is an entirely general result: when regressions are related</span>
<span id="cb1-965"><a href="#cb1-965" aria-hidden="true" tabindex="-1"></a>by invertible linear transformations of regressors, the fit does not change,</span>
<span id="cb1-966"><a href="#cb1-966" aria-hidden="true" tabindex="-1"></a>but the optimal coefficients are linear transforms of one another.</span>
<span id="cb1-967"><a href="#cb1-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-968"><a href="#cb1-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-969"><a href="#cb1-969" aria-hidden="true" tabindex="-1"></a><span class="fu">## Redundant regressors</span></span>
<span id="cb1-970"><a href="#cb1-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-971"><a href="#cb1-971" aria-hidden="true" tabindex="-1"></a>Suppose we run the (silly) regression $\y \sim \alpha \cdot 1 + \gamma \cdot 3 + \res_n$.  That is,</span>
<span id="cb1-972"><a href="#cb1-972" aria-hidden="true" tabindex="-1"></a>we regress on both the constant $1$ and the constant $3$.  We have</span>
<span id="cb1-973"><a href="#cb1-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-974"><a href="#cb1-974" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-975"><a href="#cb1-975" aria-hidden="true" tabindex="-1"></a>\X =</span>
<span id="cb1-976"><a href="#cb1-976" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-977"><a href="#cb1-977" aria-hidden="true" tabindex="-1"></a>1 &amp; 3 <span class="sc">\\</span></span>
<span id="cb1-978"><a href="#cb1-978" aria-hidden="true" tabindex="-1"></a>1 &amp; 3 <span class="sc">\\</span></span>
<span id="cb1-979"><a href="#cb1-979" aria-hidden="true" tabindex="-1"></a>1 &amp; 3 <span class="sc">\\</span></span>
<span id="cb1-980"><a href="#cb1-980" aria-hidden="true" tabindex="-1"></a>\vdots</span>
<span id="cb1-981"><a href="#cb1-981" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-982"><a href="#cb1-982" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-983"><a href="#cb1-983" aria-hidden="true" tabindex="-1"></a>(\onev \quad 3 \onev)</span>
<span id="cb1-984"><a href="#cb1-984" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-985"><a href="#cb1-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-986"><a href="#cb1-986" aria-hidden="true" tabindex="-1"></a>and so</span>
<span id="cb1-987"><a href="#cb1-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-988"><a href="#cb1-988" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-989"><a href="#cb1-989" aria-hidden="true" tabindex="-1"></a>\X^\trans \X = </span>
<span id="cb1-990"><a href="#cb1-990" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-991"><a href="#cb1-991" aria-hidden="true" tabindex="-1"></a>\onev^\trans \onev &amp; 3 \onev^\trans \onev <span class="sc">\\</span></span>
<span id="cb1-992"><a href="#cb1-992" aria-hidden="true" tabindex="-1"></a>3 \onev^\trans \onev &amp; 9 \onev^\trans \onev <span class="sc">\\</span></span>
<span id="cb1-993"><a href="#cb1-993" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-994"><a href="#cb1-994" aria-hidden="true" tabindex="-1"></a>= </span>
<span id="cb1-995"><a href="#cb1-995" aria-hidden="true" tabindex="-1"></a>N \begin{pmatrix}</span>
<span id="cb1-996"><a href="#cb1-996" aria-hidden="true" tabindex="-1"></a>1 &amp; 3  <span class="sc">\\</span></span>
<span id="cb1-997"><a href="#cb1-997" aria-hidden="true" tabindex="-1"></a>3 &amp; 9  <span class="sc">\\</span></span>
<span id="cb1-998"><a href="#cb1-998" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-999"><a href="#cb1-999" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1000"><a href="#cb1-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1001"><a href="#cb1-1001" aria-hidden="true" tabindex="-1"></a>This is not invertible (the second row is $3$ times the first, and the determinant is</span>
<span id="cb1-1002"><a href="#cb1-1002" aria-hidden="true" tabindex="-1"></a>$9 - 3 \cdot 3 = 0$).  So $\betavhat$ is not defined.  What went wrong?</span>
<span id="cb1-1003"><a href="#cb1-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1004"><a href="#cb1-1004" aria-hidden="true" tabindex="-1"></a>One way to see this is to define $\beta = \alpha + 3 \gamma$ and write</span>
<span id="cb1-1005"><a href="#cb1-1005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1006"><a href="#cb1-1006" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1007"><a href="#cb1-1007" aria-hidden="true" tabindex="-1"></a>\y_n = (\alpha + 3 \gamma) + \res_n = \beta + \res_n.</span>
<span id="cb1-1008"><a href="#cb1-1008" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1009"><a href="#cb1-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1010"><a href="#cb1-1010" aria-hidden="true" tabindex="-1"></a>There is obviously only one $\betahat$ that minimizes $\sumn \res_n^2$, $\betahat = \ybar$.  But</span>
<span id="cb1-1011"><a href="#cb1-1011" aria-hidden="true" tabindex="-1"></a>there are an infinite set of choices for $\alpha$ and $\gamma$ satisfying</span>
<span id="cb1-1012"><a href="#cb1-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1013"><a href="#cb1-1013" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1014"><a href="#cb1-1014" aria-hidden="true" tabindex="-1"></a>\alpha + 3 \gamma = \betahat = \ybar.</span>
<span id="cb1-1015"><a href="#cb1-1015" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1016"><a href="#cb1-1016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1017"><a href="#cb1-1017" aria-hidden="true" tabindex="-1"></a>Specifically, for any value of $\gamma$ we can take $\alpha = \ybar - 3 \gamma$, leaving $\beta$ unchanged.  All</span>
<span id="cb1-1018"><a href="#cb1-1018" aria-hidden="true" tabindex="-1"></a>of these choices for $\alpha,\gamma$ acheive the same $\sumn \res_n^2$!  So the least squares criterion</span>
<span id="cb1-1019"><a href="#cb1-1019" aria-hidden="true" tabindex="-1"></a>cannot distinguish among them.</span>
<span id="cb1-1020"><a href="#cb1-1020" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1021"><a href="#cb1-1021" aria-hidden="true" tabindex="-1"></a>In general, this is what it means for $\X^\trans \X$ to be non--invertibile.  It happens precisely</span>
<span id="cb1-1022"><a href="#cb1-1022" aria-hidden="true" tabindex="-1"></a>when there are redundant regressors, and many regression coefficients that result in the same fit.</span>
<span id="cb1-1023"><a href="#cb1-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1024"><a href="#cb1-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1025"><a href="#cb1-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1026"><a href="#cb1-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1027"><a href="#cb1-1027" aria-hidden="true" tabindex="-1"></a><span class="fu">## Redundant regressors and zero eigenvalues</span></span>
<span id="cb1-1028"><a href="#cb1-1028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1029"><a href="#cb1-1029" aria-hidden="true" tabindex="-1"></a>In fact, $\X^\trans \X$ is invertible precisely when $\X^\trans \X$ has a zero eigenvalue.  In</span>
<span id="cb1-1030"><a href="#cb1-1030" aria-hidden="true" tabindex="-1"></a>the preceding example, we can see that</span>
<span id="cb1-1031"><a href="#cb1-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1032"><a href="#cb1-1032" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1033"><a href="#cb1-1033" aria-hidden="true" tabindex="-1"></a>\X^\trans \X </span>
<span id="cb1-1034"><a href="#cb1-1034" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-1035"><a href="#cb1-1035" aria-hidden="true" tabindex="-1"></a>3 <span class="sc">\\</span> -1</span>
<span id="cb1-1036"><a href="#cb1-1036" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-1037"><a href="#cb1-1037" aria-hidden="true" tabindex="-1"></a>= </span>
<span id="cb1-1038"><a href="#cb1-1038" aria-hidden="true" tabindex="-1"></a>N \begin{pmatrix}</span>
<span id="cb1-1039"><a href="#cb1-1039" aria-hidden="true" tabindex="-1"></a>1 &amp; 3  <span class="sc">\\</span></span>
<span id="cb1-1040"><a href="#cb1-1040" aria-hidden="true" tabindex="-1"></a>3 &amp; 9  <span class="sc">\\</span></span>
<span id="cb1-1041"><a href="#cb1-1041" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-1042"><a href="#cb1-1042" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-1043"><a href="#cb1-1043" aria-hidden="true" tabindex="-1"></a>3 <span class="sc">\\</span> -1</span>
<span id="cb1-1044"><a href="#cb1-1044" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-1045"><a href="#cb1-1045" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-1046"><a href="#cb1-1046" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-1047"><a href="#cb1-1047" aria-hidden="true" tabindex="-1"></a>0 <span class="sc">\\</span> 0</span>
<span id="cb1-1048"><a href="#cb1-1048" aria-hidden="true" tabindex="-1"></a>\end{pmatrix},</span>
<span id="cb1-1049"><a href="#cb1-1049" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1050"><a href="#cb1-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1051"><a href="#cb1-1051" aria-hidden="true" tabindex="-1"></a>so $(3, -1)^\trans$ is a zero eigenvector.  (In general you might find this</span>
<span id="cb1-1052"><a href="#cb1-1052" aria-hidden="true" tabindex="-1"></a>by numerical eigenvalue decomposition, but in this case you can just guess the</span>
<span id="cb1-1053"><a href="#cb1-1053" aria-hidden="true" tabindex="-1"></a>zero eigenvalue.)</span>
<span id="cb1-1054"><a href="#cb1-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1055"><a href="#cb1-1055" aria-hidden="true" tabindex="-1"></a>Going back to @eq-ols-esteq, we see that this means that</span>
<span id="cb1-1056"><a href="#cb1-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1057"><a href="#cb1-1057" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1058"><a href="#cb1-1058" aria-hidden="true" tabindex="-1"></a>\X^\trans \Y =</span>
<span id="cb1-1059"><a href="#cb1-1059" aria-hidden="true" tabindex="-1"></a>(\X^\trans \X) </span>
<span id="cb1-1060"><a href="#cb1-1060" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-1061"><a href="#cb1-1061" aria-hidden="true" tabindex="-1"></a>\alphahat <span class="sc">\\</span> \gammahat</span>
<span id="cb1-1062"><a href="#cb1-1062" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-1063"><a href="#cb1-1063" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-1064"><a href="#cb1-1064" aria-hidden="true" tabindex="-1"></a>N \begin{pmatrix}</span>
<span id="cb1-1065"><a href="#cb1-1065" aria-hidden="true" tabindex="-1"></a>1 &amp; 3  <span class="sc">\\</span></span>
<span id="cb1-1066"><a href="#cb1-1066" aria-hidden="true" tabindex="-1"></a>3 &amp; 9  <span class="sc">\\</span></span>
<span id="cb1-1067"><a href="#cb1-1067" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-1068"><a href="#cb1-1068" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-1069"><a href="#cb1-1069" aria-hidden="true" tabindex="-1"></a>\alphahat <span class="sc">\\</span> \gammahat</span>
<span id="cb1-1070"><a href="#cb1-1070" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-1071"><a href="#cb1-1071" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb1-1072"><a href="#cb1-1072" aria-hidden="true" tabindex="-1"></a>N \begin{pmatrix}</span>
<span id="cb1-1073"><a href="#cb1-1073" aria-hidden="true" tabindex="-1"></a>1 &amp; 3  <span class="sc">\\</span></span>
<span id="cb1-1074"><a href="#cb1-1074" aria-hidden="true" tabindex="-1"></a>3 &amp; 9  <span class="sc">\\</span></span>
<span id="cb1-1075"><a href="#cb1-1075" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-1076"><a href="#cb1-1076" aria-hidden="true" tabindex="-1"></a>\left( </span>
<span id="cb1-1077"><a href="#cb1-1077" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-1078"><a href="#cb1-1078" aria-hidden="true" tabindex="-1"></a>\alphahat <span class="sc">\\</span> \gammahat</span>
<span id="cb1-1079"><a href="#cb1-1079" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-1080"><a href="#cb1-1080" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>C</span>
<span id="cb1-1081"><a href="#cb1-1081" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-1082"><a href="#cb1-1082" aria-hidden="true" tabindex="-1"></a>3 <span class="sc">\\</span> -1</span>
<span id="cb1-1083"><a href="#cb1-1083" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-1084"><a href="#cb1-1084" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb1-1085"><a href="#cb1-1085" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1086"><a href="#cb1-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1087"><a href="#cb1-1087" aria-hidden="true" tabindex="-1"></a>for *any* value of $C$.  This means there are an infinite set of "optimal"</span>
<span id="cb1-1088"><a href="#cb1-1088" aria-hidden="true" tabindex="-1"></a>values, all of which set the gradient of the loss to zero, and all of</span>
<span id="cb1-1089"><a href="#cb1-1089" aria-hidden="true" tabindex="-1"></a>which have the same value of the loss function (i.e. acheive the same fit).  And</span>
<span id="cb1-1090"><a href="#cb1-1090" aria-hidden="true" tabindex="-1"></a>you can check that these family of values are exactly the ones that</span>
<span id="cb1-1091"><a href="#cb1-1091" aria-hidden="true" tabindex="-1"></a>satisfy $\alpha + 3 \gamma = \betahat = \ybar$, since </span>
<span id="cb1-1092"><a href="#cb1-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1093"><a href="#cb1-1093" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1094"><a href="#cb1-1094" aria-hidden="true" tabindex="-1"></a>\alpha + 3 \gamma = </span>
<span id="cb1-1095"><a href="#cb1-1095" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-1096"><a href="#cb1-1096" aria-hidden="true" tabindex="-1"></a>1 &amp; 3</span>
<span id="cb1-1097"><a href="#cb1-1097" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-1098"><a href="#cb1-1098" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-1099"><a href="#cb1-1099" aria-hidden="true" tabindex="-1"></a>\alpha <span class="sc">\\</span> \gamma</span>
<span id="cb1-1100"><a href="#cb1-1100" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-1101"><a href="#cb1-1101" aria-hidden="true" tabindex="-1"></a>\quad\quad\textrm{and}\quad\quad</span>
<span id="cb1-1102"><a href="#cb1-1102" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-1103"><a href="#cb1-1103" aria-hidden="true" tabindex="-1"></a>1 &amp; 3</span>
<span id="cb1-1104"><a href="#cb1-1104" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb1-1105"><a href="#cb1-1105" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb1-1106"><a href="#cb1-1106" aria-hidden="true" tabindex="-1"></a>3 <span class="sc">\\</span> -1</span>
<span id="cb1-1107"><a href="#cb1-1107" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} = 0.</span>
<span id="cb1-1108"><a href="#cb1-1108" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1109"><a href="#cb1-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1110"><a href="#cb1-1110" aria-hidden="true" tabindex="-1"></a>Soon, we will see that this is a general result: when $\X^\trans \X$</span>
<span id="cb1-1111"><a href="#cb1-1111" aria-hidden="true" tabindex="-1"></a>is not invertible, that means there are many equivalent least</span>
<span id="cb1-1112"><a href="#cb1-1112" aria-hidden="true" tabindex="-1"></a>squares fits, all characterized precisely by the zero eigenvectors</span>
<span id="cb1-1113"><a href="#cb1-1113" aria-hidden="true" tabindex="-1"></a>of $\X^\trans \X$.</span>
<span id="cb1-1114"><a href="#cb1-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1115"><a href="#cb1-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1116"><a href="#cb1-1116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1117"><a href="#cb1-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1118"><a href="#cb1-1118" aria-hidden="true" tabindex="-1"></a><span class="fu">## Zero variance regressors</span></span>
<span id="cb1-1119"><a href="#cb1-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1120"><a href="#cb1-1120" aria-hidden="true" tabindex="-1"></a>An example of redundant regressors occurs when the sample variance</span>
<span id="cb1-1121"><a href="#cb1-1121" aria-hidden="true" tabindex="-1"></a>of $\x_n$ is zero and a constant is included in the regression.</span>
<span id="cb1-1122"><a href="#cb1-1122" aria-hidden="true" tabindex="-1"></a>Specifically, suppose that $\overline{xx} - \overline{x}^2 = 0$.</span>
<span id="cb1-1123"><a href="#cb1-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1124"><a href="#cb1-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1125"><a href="#cb1-1125" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title='Exercise'} </span>
<span id="cb1-1126"><a href="#cb1-1126" aria-hidden="true" tabindex="-1"></a>Prove that $\overline{xx} - \overline{x}^2 = 0$ </span>
<span id="cb1-1127"><a href="#cb1-1127" aria-hidden="true" tabindex="-1"></a>means $\x_n$ is a constant with $\x_n = \xbar$.  Hint: look at the sample</span>
<span id="cb1-1128"><a href="#cb1-1128" aria-hidden="true" tabindex="-1"></a>variance of $\x_n$.</span>
<span id="cb1-1129"><a href="#cb1-1129" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1130"><a href="#cb1-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1131"><a href="#cb1-1131" aria-hidden="true" tabindex="-1"></a>Let's regress $\y_n \sim \beta_1 + \beta_2 \x_n$.</span>
<span id="cb1-1132"><a href="#cb1-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1133"><a href="#cb1-1133" aria-hidden="true" tabindex="-1"></a>For simplicity, let's take $\x_n = 3$.  In that case</span>
<span id="cb1-1134"><a href="#cb1-1134" aria-hidden="true" tabindex="-1"></a>we can rewrite our estimating equation as</span>
<span id="cb1-1135"><a href="#cb1-1135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1136"><a href="#cb1-1136" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1137"><a href="#cb1-1137" aria-hidden="true" tabindex="-1"></a>\y_n = \beta_1 + \beta_2 \x_n + \res_n </span>
<span id="cb1-1138"><a href="#cb1-1138" aria-hidden="true" tabindex="-1"></a>     = (\beta_1 + \beta_2 \xbar) + \res_n.</span>
<span id="cb1-1139"><a href="#cb1-1139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1140"><a href="#cb1-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1141"><a href="#cb1-1141" aria-hidden="true" tabindex="-1"></a>We're thus in the previous setting with $\xbar$ in place of the number $3$.</span>
<span id="cb1-1142"><a href="#cb1-1142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1143"><a href="#cb1-1143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1144"><a href="#cb1-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1145"><a href="#cb1-1145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1146"><a href="#cb1-1146" aria-hidden="true" tabindex="-1"></a><span class="fu">## Orthogonal regressors</span></span>
<span id="cb1-1147"><a href="#cb1-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1148"><a href="#cb1-1148" aria-hidden="true" tabindex="-1"></a>Suppose we have regressors such that the columns of $\X$ are orthonormal.  This</span>
<span id="cb1-1149"><a href="#cb1-1149" aria-hidden="true" tabindex="-1"></a>seems strange at first, since we usually specify the *rows* of the regressors,</span>
<span id="cb1-1150"><a href="#cb1-1150" aria-hidden="true" tabindex="-1"></a>not the columns.  But in fact we have seen a near--example with one--hot encodings,</span>
<span id="cb1-1151"><a href="#cb1-1151" aria-hidden="true" tabindex="-1"></a>which are defined row--wise, but which produce orthogonal column vectors in $\X$.</span>
<span id="cb1-1152"><a href="#cb1-1152" aria-hidden="true" tabindex="-1"></a>If we divide a one--hot encoding by the square root of the number of ones in the</span>
<span id="cb1-1153"><a href="#cb1-1153" aria-hidden="true" tabindex="-1"></a>whole dataset, we produce an normal column vector.</span>
<span id="cb1-1154"><a href="#cb1-1154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1155"><a href="#cb1-1155" aria-hidden="true" tabindex="-1"></a>If $\X$ has orthonormal columns, then $\X^\trans \X = \id$, the identity matrix,</span>
<span id="cb1-1156"><a href="#cb1-1156" aria-hidden="true" tabindex="-1"></a>and so </span>
<span id="cb1-1157"><a href="#cb1-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1158"><a href="#cb1-1158" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1159"><a href="#cb1-1159" aria-hidden="true" tabindex="-1"></a>\betavhat = (\X^\trans \X)^{-1} \X^\trans \Y = \X^\trans \Y.</span>
<span id="cb1-1160"><a href="#cb1-1160" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1161"><a href="#cb1-1161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1162"><a href="#cb1-1162" aria-hidden="true" tabindex="-1"></a>This is of course the same answer we would have gotten if we</span>
<span id="cb1-1163"><a href="#cb1-1163" aria-hidden="true" tabindex="-1"></a>had tried to write $\Y$ in the basis of the column vectors of $\X$:</span>
<span id="cb1-1164"><a href="#cb1-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1165"><a href="#cb1-1165" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1166"><a href="#cb1-1166" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-1167"><a href="#cb1-1167" aria-hidden="true" tabindex="-1"></a>\Y ={}&amp; \betahat_1 \X_{\cdot 1} + \ldots + \betahat_P \X_{\cdot P} = \X \betavhat \Rightarrow <span class="sc">\\</span></span>
<span id="cb1-1168"><a href="#cb1-1168" aria-hidden="true" tabindex="-1"></a>\X^\trans \Y ={}&amp; \X^\trans \X \betavhat = \betavhat</span>
<span id="cb1-1169"><a href="#cb1-1169" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-1170"><a href="#cb1-1170" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1171"><a href="#cb1-1171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1172"><a href="#cb1-1172" aria-hidden="true" tabindex="-1"></a>This regression is particularly simple --- each component of $\betavhat$ depends only on its corresponding</span>
<span id="cb1-1173"><a href="#cb1-1173" aria-hidden="true" tabindex="-1"></a>column of $\X$.</span>
<span id="cb1-1174"><a href="#cb1-1174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1175"><a href="#cb1-1175" aria-hidden="true" tabindex="-1"></a>Note that if each entry of $\xv_n$ is mean zero, unit variance, and uncorrelated with the other entries, then</span>
<span id="cb1-1176"><a href="#cb1-1176" aria-hidden="true" tabindex="-1"></a>$\frac{1}{N} \X^\trans \X \rightarrow \id$ by the LLN.  Such a regressor matrix is not typically orthogonal</span>
<span id="cb1-1177"><a href="#cb1-1177" aria-hidden="true" tabindex="-1"></a>for any particular $N$, but it approaches orthogonality as $N$ grows.</span>
<span id="cb1-1178"><a href="#cb1-1178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1179"><a href="#cb1-1179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1180"><a href="#cb1-1180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1181"><a href="#cb1-1181" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>