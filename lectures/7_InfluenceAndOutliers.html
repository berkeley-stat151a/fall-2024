<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Influence and outliers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-MMK2VCM6EW"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-MMK2VCM6EW', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Influence and outliers</li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../stat_bear.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/berkeley-stat151a/fall-2024" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course_policies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Policies</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/lectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lectures</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../assignments/assignments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assignments</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../datasets/data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Datasets</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../quizzes/quizzes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quizzes</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#goals" id="toc-goals" class="nav-link active" data-scroll-target="#goals">Goals</a></li>
  <li><a href="#births-data" id="toc-births-data" class="nav-link" data-scroll-target="#births-data">Births data</a>
  <ul class="collapse">
  <li><a href="#outliers-in-the-births-data" id="toc-outliers-in-the-births-data" class="nav-link" data-scroll-target="#outliers-in-the-births-data">Outliers in the births data</a></li>
  </ul></li>
  <li><a href="#outliers" id="toc-outliers" class="nav-link" data-scroll-target="#outliers">Outliers</a></li>
  <li><a href="#unusual-responses" id="toc-unusual-responses" class="nav-link" data-scroll-target="#unusual-responses">Unusual responses</a>
  <ul class="collapse">
  <li><a href="#unusual-responses-look-at-residuals" id="toc-unusual-responses-look-at-residuals" class="nav-link" data-scroll-target="#unusual-responses-look-at-residuals">Unusual responses (look at residuals)</a></li>
  <li><a href="#microcredit-data" id="toc-microcredit-data" class="nav-link" data-scroll-target="#microcredit-data">Microcredit data</a></li>
  </ul></li>
  <li><a href="#unusual-regressors" id="toc-unusual-regressors" class="nav-link" data-scroll-target="#unusual-regressors">Unusual regressors</a>
  <ul class="collapse">
  <li><a href="#effect-of-high-leverage" id="toc-effect-of-high-leverage" class="nav-link" data-scroll-target="#effect-of-high-leverage">Effect of high leverage</a></li>
  <li><a href="#kleibers-whale-revisited" id="toc-kleibers-whale-revisited" class="nav-link" data-scroll-target="#kleibers-whale-revisited">Kleiber’s whale revisited</a></li>
  </ul></li>
  <li><a href="#data-dropping" id="toc-data-dropping" class="nav-link" data-scroll-target="#data-dropping">Data dropping</a>
  <ul class="collapse">
  <li><a href="#bonus-content-data-dropping-generalization-to-nonlinear-estimators" id="toc-bonus-content-data-dropping-generalization-to-nonlinear-estimators" class="nav-link" data-scroll-target="#bonus-content-data-dropping-generalization-to-nonlinear-estimators">Bonus content: Data dropping (generalization to nonlinear estimators)</a></li>
  <li><a href="#bonus-content-rank-one-updates-for-linear-regression-woodbury-formula" id="toc-bonus-content-rank-one-updates-for-linear-regression-woodbury-formula" class="nav-link" data-scroll-target="#bonus-content-rank-one-updates-for-linear-regression-woodbury-formula">Bonus content: Rank-one updates for linear regression (Woodbury formula)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
$$

\newcommand{\mybold}[1]{\boldsymbol{#1}} 


\newcommand{\trans}{\intercal}
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\bbr}{\mathbb{R}}
\newcommand{\bbz}{\mathbb{Z}}
\newcommand{\bbc}{\mathbb{C}}
\newcommand{\gauss}[1]{\mathcal{N}\left(#1\right)}
\newcommand{\chisq}[1]{\mathcal{\chi}^2_{#1}}
\newcommand{\studentt}[1]{\mathrm{StudentT}_{#1}}
\newcommand{\fdist}[2]{\mathrm{FDist}_{#1,#2}}

\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}}\,}
\newcommand{\projop}[1]{\underset{#1}{\mathrm{Proj}}\,}
\newcommand{\proj}[1]{\underset{#1}{\mybold{P}}}
\newcommand{\expect}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\dens}[1]{\mathit{p}\left(#1\right)}
\newcommand{\var}[1]{\mathrm{Var}\left(#1\right)}
\newcommand{\cov}[1]{\mathrm{Cov}\left(#1\right)}
\newcommand{\sumn}{\sum_{n=1}^N}
\newcommand{\meann}{\frac{1}{N} \sumn}
\newcommand{\cltn}{\frac{1}{\sqrt{N}} \sumn}

\newcommand{\trace}[1]{\mathrm{trace}\left(#1\right)}
\newcommand{\diag}[1]{\mathrm{Diag}\left(#1\right)}
\newcommand{\grad}[2]{\nabla_{#1} \left. #2 \right.}
\newcommand{\gradat}[3]{\nabla_{#1} \left. #2 \right|_{#3}}
\newcommand{\fracat}[3]{\left. \frac{#1}{#2} \right|_{#3}}


\newcommand{\W}{\mybold{W}}
\newcommand{\w}{w}
\newcommand{\wbar}{\bar{w}}
\newcommand{\wv}{\mybold{w}}

\newcommand{\X}{\mybold{X}}
\newcommand{\x}{x}
\newcommand{\xbar}{\bar{x}}
\newcommand{\xv}{\mybold{x}}
\newcommand{\Xcov}{\Sigmam_{\X}}
\newcommand{\Xcovhat}{\hat{\Sigmam}_{\X}}
\newcommand{\Covsand}{\Sigmam_{\mathrm{sand}}}
\newcommand{\Covsandhat}{\hat{\Sigmam}_{\mathrm{sand}}}

\newcommand{\Z}{\mybold{Z}}
\newcommand{\z}{z}
\newcommand{\zv}{\mybold{z}}
\newcommand{\zbar}{\bar{z}}

\newcommand{\Y}{\mybold{Y}}
\newcommand{\Yhat}{\hat{\Y}}
\newcommand{\y}{y}
\newcommand{\yv}{\mybold{y}}
\newcommand{\yhat}{\hat{\y}}
\newcommand{\ybar}{\bar{y}}

\newcommand{\res}{\varepsilon}
\newcommand{\resv}{\mybold{\res}}
\newcommand{\resvhat}{\hat{\mybold{\res}}}
\newcommand{\reshat}{\hat{\res}}

\newcommand{\betav}{\mybold{\beta}}
\newcommand{\betavhat}{\hat{\betav}}
\newcommand{\betahat}{\hat{\beta}}
\newcommand{\betastar}{{\beta^{*}}}


\newcommand{\f}{f}
\newcommand{\fhat}{\hat{f}}

\newcommand{\bv}{\mybold{\b}}
\newcommand{\bvhat}{\hat{\bv}}

\newcommand{\alphav}{\mybold{\alpha}}
\newcommand{\alphavhat}{\hat{\av}}
\newcommand{\alphahat}{\hat{\alpha}}

\newcommand{\omegav}{\mybold{\omega}}

\newcommand{\gv}{\mybold{\gamma}}
\newcommand{\gvhat}{\hat{\gv}}
\newcommand{\ghat}{\hat{\gamma}}

\newcommand{\hv}{\mybold{\h}}
\newcommand{\hvhat}{\hat{\hv}}
\newcommand{\hhat}{\hat{\h}}

\newcommand{\gammav}{\mybold{\gamma}}
\newcommand{\gammavhat}{\hat{\gammav}}
\newcommand{\gammahat}{\hat{\gamma}}

\newcommand{\new}{\mathrm{new}}
\newcommand{\zerov}{\mybold{0}}
\newcommand{\onev}{\mybold{1}}
\newcommand{\id}{\mybold{I}}

\newcommand{\sigmahat}{\hat{\sigma}}


\newcommand{\etav}{\mybold{\eta}}
\newcommand{\muv}{\mybold{\mu}}
\newcommand{\Sigmam}{\mybold{\Sigma}}

\newcommand{\rdom}[1]{\mathbb{R}^{#1}}

\newcommand{\RV}[1]{\tilde{#1}}



\def\A{\mybold{A}}

\def\A{\mybold{A}}
\def\av{\mybold{a}}
\def\a{a}

\def\B{\mybold{B}}
\def\b{b}


\def\S{\mybold{S}}
\def\sv{\mybold{s}}
\def\s{s}

\def\R{\mybold{R}}
\def\rv{\mybold{r}}
\def\r{r}

\def\V{\mybold{V}}
\def\vv{\mybold{v}}
\def\v{v}

\def\U{\mybold{U}}
\def\uv{\mybold{u}}
\def\u{u}

\def\W{\mybold{W}}
\def\wv{\mybold{w}}
\def\w{w}

\def\tv{\mybold{t}}
\def\t{t}

\def\Sc{\mathcal{S}}
\def\ev{\mybold{e}}

\def\Lammat{\mybold{\Lambda}}

\def\Q{\mybold{Q}}


\def\eps{\varepsilon}

$$

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Influence and outliers</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="goals" class="level1">
<h1>Goals</h1>
<ul>
<li>Discuss some was that extreme data can influence regression
<ul>
<li>There is no clear definition of an outlier</li>
<li>The (unbounded) influence of outlying <span class="math inline">\(y_n\)</span>
<ul>
<li>Look at residuals using <code>fit$residuals</code></li>
</ul></li>
<li>The influence of outyling <span class="math inline">\(x_n\)</span> and the leverage score
<ul>
<li>Look at leverage scores using <code>hatvalues</code></li>
</ul></li>
<li>The influence of removing a point (both leverage and residual)
<ul>
<li>Look at the influence function using <code>influence</code></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="births-data" class="level1">
<h1>Births data</h1>
<p>Let’s look at the <code>births14</code> dataset, a random selection of 1000 observations from the US government</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(births_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  fage mage      mature weeks    premie visits gained weight lowbirthweight
1   34   34 younger mom    37 full term     14     28   6.96        not low
2   36   31 younger mom    41 full term     12     41   8.86        not low
3   37   36  mature mom    37 full term     10     28   7.51        not low
4   32   31 younger mom    36    premie     12     48   6.75        not low
5   32   26 younger mom    39 full term     14     45   6.69        not low
6   37   36  mature mom    36    premie     10     20   6.13        not low
     sex     habit marital  whitemom
1   male nonsmoker married     white
2 female nonsmoker married     white
3 female nonsmoker married not white
4 female nonsmoker married     white
5 female nonsmoker married     white
6 female nonsmoker married     white</code></pre>
</div>
</div>
<p>Although this is not a randomized controlled trial, we might look in the data for suggestive patterns to guide or support future research. This is an <strong>inference</strong> problem.</p>
<p>In particular, let’s ask how father’s age might affect birth weight.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>lm_form <span class="ot">&lt;-</span> <span class="fu">formula</span>(weight <span class="sc">~</span> fage )</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>reg_all <span class="ot">&lt;-</span> <span class="fu">lm</span>(lm_form, births_df)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_all)<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               Estimate  Std. Error    t value      Pr(&gt;|t|)
(Intercept) 7.104174083 0.193584607 36.6980319 7.158765e-180
fage        0.004726717 0.006064235  0.7794416  4.359283e-01</code></pre>
</div>
</div>
<p><strong>How can we interpret this?</strong></p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="7_InfluenceAndOutliers_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<section id="outliers-in-the-births-data" class="level2">
<h2 class="anchored" data-anchor-id="outliers-in-the-births-data">Outliers in the births data</h2>
<p>We get pretty different slopes with and without those three very old fathers!</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>age_threshold <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>reg_drop <span class="ot">&lt;-</span> <span class="fu">lm</span>(lm_form, births_df <span class="sc">%&gt;%</span> <span class="fu">filter</span>(fage <span class="sc">&lt;</span> age_threshold))</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_all)<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               Estimate  Std. Error    t value      Pr(&gt;|t|)
(Intercept) 7.104174083 0.193584607 36.6980319 7.158765e-180
fage        0.004726717 0.006064235  0.7794416  4.359283e-01</code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_drop)<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              Estimate  Std. Error   t value      Pr(&gt;|t|)
(Intercept) 6.94399958 0.202478955 34.294920 2.086328e-164
fage        0.01010549 0.006382714  1.583259  1.137214e-01</code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="7_InfluenceAndOutliers_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="outliers" class="level1">
<h1>Outliers</h1>
<p>What should we do?</p>
<ul>
<li>Remove the points with old fathers?</li>
<li>Regress on them separately?</li>
<li>Regress on <span class="math inline">\(\log \textrm{fage}\)</span>?</li>
</ul>
<p>It depends on what we’re trying to do, and why we think those observations are so extreme.</p>
<p>Data can be an “outlier” because:</p>
<ul>
<li>It’s an extreme (but important) value the data can actually take</li>
<li>It’s an extreme (and unimportant) value the data can actually take</li>
<li>The data was entered incorrectly or corrupted</li>
<li>Data from a different source mixed in with more typical data</li>
<li>Adversaries trying to mess with your data to produce some desired conclusion</li>
</ul>
<p><strong>There are no good general answers or definitions of outliers.</strong></p>
<p>Today we will be studying only how and why extreme values can change a regression.</p>
</section>
<section id="unusual-responses" class="level1">
<h1>Unusual responses</h1>
<p>Recall that <span class="math inline">\(\betavhat = (\X^\trans \X)^{-1} \X^ \trans \Y\)</span>. This can be written as a weighted sum of <span class="math inline">\(\y_n\)</span>:</p>
<p><span class="math display">\[
\betavhat = (\X^\trans \X)^{-1} \X^\trans \Y =
  \sumn (\X^\trans \X)^{-1} \xv_n \y_n =: \sumn \omegav_n \y_n.
\]</span></p>
<p>It is clear that we can produce <strong>arbitrarily large changes in <span class="math inline">\(\betavhat\)</span></strong> by producing arbitrarily large changes <strong>in only a single <span class="math inline">\(\y_n\)</span></strong>.</p>
<p>We can make the births fit arbitrarily crazy by changing a single observation:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>fit_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>()</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>modify_row <span class="ot">&lt;-</span> <span class="fu">which</span>(births_df<span class="sc">$</span>fage <span class="sc">==</span> <span class="dv">40</span>)[<span class="dv">1</span>]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (res <span class="cf">in</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">100</span>, <span class="dv">1000</span>)) {</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  births_modified_df <span class="ot">&lt;-</span> births_df</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  births_modified_df[modify_row, <span class="st">"weight"</span>] <span class="ot">&lt;-</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    births_modified_df[modify_row, <span class="st">"weight"</span>] <span class="sc">+</span> res</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  reg_mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(lm_form, births_modified_df)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>  fit_df <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    fit_df,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(births_modified_df, fage, weight) <span class="sc">%&gt;%</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">yhat=</span><span class="fu">fitted</span>(reg_mod), <span class="at">res=</span>res)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="7_InfluenceAndOutliers_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<section id="unusual-responses-look-at-residuals" class="level2">
<h2 class="anchored" data-anchor-id="unusual-responses-look-at-residuals">Unusual responses (look at residuals)</h2>
<p>Outlier <span class="math inline">\(\y_n\)</span> will also tend to have outlier residuals.</p>
<p>To see this, let’s suppose there is one abberant value, <span class="math inline">\(\y_*\)</span>, which is very large, and which we enumerate separately from the well-behaved <span class="math inline">\(\y_n\)</span>.</p>
<p>Assume that <span class="math inline">\(\xv_*\)</span> is not an outlier (i.e.&nbsp;the response is an outlier but the regressor is not). That means <span class="math inline">\(\frac{1}{N} (\X^\trans \X + \xv_* \xv_*^\trans) \approx \frac{1}{N} \X^\trans \X\)</span>. Let <span class="math inline">\(\betavhat_*= (\X^\trans \X)^{-1} \X^\trans \Y\)</span> denote the estimated coefficient without the outlier.</p>
<p>We can write the data together with the outlier as</p>
<p><span class="math display">\[
\begin{pmatrix}
\Y \\
\y_*
\end{pmatrix}
\quad\quad
\begin{pmatrix}
\X \\
\xv_*^\trans
\end{pmatrix}
\]</span> We have</p>
<p><span class="math display">\[
\begin{aligned}
\reshat_* ={}&amp; \y_* - \yhat_*
\\={}&amp; \y^* - \xv_*^\trans \betahat
\\={}&amp; \y^* - \xv_*^\trans (\X^\trans \X + \xv_* \xv_*^\trans)^{-1} (\X^\trans \Y + \xv_* \y_*)
\\={}&amp; \y^* - \xv_*^\trans \left(
    \frac{1}{N} \left(
    \X^\trans \X + \xv_* \xv_*^\trans
    \right)
  \right)^{-1}
  \frac{1}{N} (\X^\trans \Y + \xv_* \y_*)
\\\approx{}&amp; \y^* - \xv_*^\trans \left(
    \frac{1}{N} \left(
    \X^\trans \X
    \right)
  \right)^{-1}
  \frac{1}{N} (\X^\trans \Y + \xv_* \y_*)
\\=&amp; \y^* - \xv_*^\trans \betavhat_* +
  \frac{1}{N} \xv_*^\trans
  \left(\frac{1}{N} \X^\trans \X \right)^{-1} \xv_* \y_*
\\\approx&amp; \y^* - \xv_*^\trans \betavhat_*
\end{aligned}
\]</span></p>
<p>This means that although the <span class="math inline">\(\y_*\)</span> causes the <span class="math inline">\(\betahat\)</span> to grow very large in an attempt to fit it, its residual remains large, and with the same sign as <span class="math inline">\(\y_*\)</span>.</p>
<p>This means you may be able to identify outlier responses by looking at a residual plot, e.g., a histogram of residuals, and seeing if any fitted residuals are atypical.</p>
</section>
<section id="microcredit-data" class="level2">
<h2 class="anchored" data-anchor-id="microcredit-data">Microcredit data</h2>
<p>Let’s take a look at some data from a study of microcredit in Mexico. The goal of the study is to estimate the effect of microcredit, which was randomly allocated.</p>
<p>For example, we might try to measure the effect of the treatment on “temptation goods.” We run the model <span class="math inline">\(\textrm{Temptation spend} ~ 1 + \textrm{treatment}\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>reg_mx <span class="ot">&lt;-</span> <span class="fu">lm</span>(temptation <span class="sc">~</span> treatment, mx_df)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_mx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = temptation ~ treatment, data = mx_df)

Residuals:
    Min      1Q  Median      3Q     Max 
 -4.656  -3.985  -1.925   1.667 101.306 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  4.65605    0.06635  70.177   &lt;2e-16 ***
treatment   -0.09604    0.09393  -1.022    0.307    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.044 on 16558 degrees of freedom
Multiple R-squared:  6.314e-05, Adjusted R-squared:  2.746e-06 
F-statistic: 1.045 on 1 and 16558 DF,  p-value: 0.3066</code></pre>
</div>
</div>
<p>However, we see that there are huge residuals:</p>
<div class="cell">
<div class="cell-output cell-output-stderr">
<pre><code>`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="7_InfluenceAndOutliers_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This is because spending has a very heavy tail!</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="7_InfluenceAndOutliers_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We can produce big changes in the regression by dropping the 5 largest residuals (which is only 0.0301932 percent of the data):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"With all datapoints:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "With all datapoints:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_mx)<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               Estimate Std. Error   t value  Pr(&gt;|t|)
(Intercept)  4.65605325 0.06634731 70.176975 0.0000000
treatment   -0.09604309 0.09393141 -1.022481 0.3065682</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Without the largest residual datapoints:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Without the largest residual datapoints:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_drop_mx)<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              Estimate Std. Error   t value  Pr(&gt;|t|)
(Intercept)  4.6359381 0.06444850 71.932439 0.0000000
treatment   -0.1053137 0.09124597 -1.154174 0.2484456</code></pre>
</div>
</div>
</section>
</section>
<section id="unusual-regressors" class="level1">
<h1>Unusual regressors</h1>
<p>Unusually large regressor values are called “high leverage points,” since small changes in <span class="math inline">\(\beta\)</span> produce large changes in the fitted values at the corresponding points. Since <span class="math inline">\(\xv_n\)</span> is a vector, measuring what it means for <span class="math inline">\(\xv_n\)</span> to be an outlier is a little more subtle than measuring what it means for a scalar like <span class="math inline">\(\y_n\)</span> to be an outlier. But a sensible thing to do is to measure the size of <span class="math inline">\(\xv_n\)</span> relative to <span class="math inline">\(\X^\trans \X\)</span>, which estimates the spread of the <span class="math inline">\(\xv_n\)</span> values (if they are centered, it is an estimate of the covariance). We define the “leverage” score <span class="math display">\[
h_n := \xv_n^\trans (\X^\trans \X)^{-1} \xv_n,
\]</span></p>
<p>and we can check for unusual <span class="math inline">\(\xv_n\)</span> by looking for high leverage scores.</p>
<p>Note that <span class="math inline">\(h_n\)</span> is the <span class="math inline">\(n\)</span>–th diagonal entry of the “hat” matrix <span class="math inline">\(\proj{\X} = \X (\X^\trans \X)^{-1} \X^\trans\)</span>, so called because it “puts the hat on <span class="math inline">\(\Y\)</span>”, since <span class="math inline">\(\Yhat = \proj{\X} \Y\)</span>. There are a few useful consequences of this fact.</p>
<ul>
<li><span class="math inline">\(0 \le h_n \le 1\)</span></li>
</ul>
<p><em>Proof:</em> Take the vector <span class="math inline">\(\ev_n\)</span> that is <span class="math inline">\(1\)</span> in entry <span class="math inline">\(n\)</span> and <span class="math inline">\(0\)</span> elsewhere.</p>
<p><span class="math display">\[
h_n = \ev_n^\trans \proj{\X} \ev_n = \ev_n^\trans \proj{\X} \ev_n = \norm{\proj{\X} \ev_n}^2
\]</span></p>
<p>Since <span class="math inline">\(\norm{h_n}^2 = 1\)</span>, and <span class="math inline">\(\norm{\proj{\X} \ev_n}^2 \le \norm{\ev_n}\)</span>, it follows that <span class="math inline">\(h_n \le 1\)</span>. And since <span class="math inline">\(\norm{\proj{\X} \ev_n}^2 \ge 0\)</span>, it follows that <span class="math inline">\(h_n \ge 0\)</span>.</p>
<ul>
<li><span class="math inline">\(\sumn h_n = P\)</span></li>
</ul>
<p>There are <span class="math inline">\(P\)</span> linearly independent unit vectors in the column span of <span class="math inline">\(\X\)</span>. Call them <span class="math inline">\(\uv_1, \ldots, \uv_P\)</span>. Since <span class="math inline">\(\proj{\X} \uv_p = \uv_p\)</span>, the <span class="math inline">\(\uv_p\)</span> are all eigenvectors with eigenvalues one. Similarly, the <span class="math inline">\(N - P\)</span> unit vectors spanning the space orthogonal to <span class="math inline">\(\X\)</span> are eigenvectors with eigenvalue <span class="math inline">\(0\)</span>.</p>
<p>Finally, the <span class="math inline">\(\trace{\proj{\X}} = \sum_n h_n = P\)</span> since the trace is the sum of the eigenvalues.</p>
<ul>
<li>At least some <span class="math inline">\(h_n &gt; 0\)</span>.</li>
</ul>
<p>This follows directly from <span class="math inline">\(h_n \ge 0\)</span> and <span class="math inline">\(\sum_n h_n = P\)</span>.</p>
<p>Additionally, we can see that <span class="math inline">\(\frac{d\yhat_n}{d \y_n}  = h_n\)</span>. (This fact that one can use to define “leverage scores” in settings beyond linear regression.)</p>
<p>Putting this together, we can see that</p>
<ul>
<li>Since <span class="math inline">\(\sumn h_n = P\)</span> and <span class="math inline">\(0 \le h_n \le 1\)</span>, not too many leverage scores can be large.<br>
</li>
<li>On average, a typical <span class="math inline">\(h_n \approx P / N\)</span> if the data is well-behaved.</li>
<li>If a leverage score is large, it means that the value of <span class="math inline">\(\y_n\)</span> has a high influence on its own fit, <span class="math inline">\(\yhat_n\)</span>.</li>
<li>If a leverage score is large, <span class="math inline">\(\xv_n\)</span> is large relative to the estimated “covariance” <span class="math inline">\(\meann \X^\trans \X\)</span>, up to its expected scaling of <span class="math inline">\(1/N\)</span>.</li>
</ul>
<section id="effect-of-high-leverage" class="level2">
<h2 class="anchored" data-anchor-id="effect-of-high-leverage">Effect of high leverage</h2>
<p>What happens to a regression when you have very high leverage? Suppose that <span class="math inline">\(h_n = 1\)</span>. That means that the vector <span class="math inline">\(\ev_n\)</span>, which has <span class="math inline">\(1\)</span> in entry <span class="math inline">\(n\)</span> and <span class="math inline">\(0\)</span> otherwise, is an eigenvector of <span class="math inline">\(\proj{\X}\)</span>, and consequently <span class="math inline">\(\ev_n\)</span> is in the column span of <span class="math inline">\(\X\)</span>. This means that the <span class="math inline">\(n\)</span>–th entry of <span class="math inline">\(\proj{\X} \Y\)</span> is given by</p>
<p><span class="math display">\[
\yhat_n = \left( \proj{\X} \Y \right)_n = \y_n.
\]</span></p>
<p>This is the same as <span class="math inline">\(\y_n \approx \yhat_n = \betavhat^\trans \xv_n\)</span>, which forces the fit to pass through the point <span class="math inline">\((\xv_n, \y_n)\)</span>.</p>
<p>If there are <span class="math inline">\(P\)</span> such high–leverage points, then <span class="math inline">\(\betavhat\)</span> is completely determined by these points.</p>
<p>Note that, since <span class="math inline">\(\sumn h_n = P\)</span> and <span class="math inline">\(0 \le h_n \le 1\)</span>, there can only be <span class="math inline">\(P\)</span> leverage points that are approximately equal to one.</p>
<p>Although this intuition is for leverage scores that are exactly one, it applies to leverage scores that are large but not exactly one — outlier <span class="math inline">\(\xv_n\)</span>, with large leverage scores, force the regression line to fit the point, unlike outlier <span class="math inline">\(y_n\)</span> with large residuals. To see this, note that</p>
<p><span class="math display">\[
\begin{aligned}
\yhat_n ={}&amp; \left(\proj{\X} \Y \right)_{n} \\
={}&amp; \sum_{m=1}^N \left(\proj{\X}\right)_{nm} \y_m \\
={}&amp; \sum_{m \ne n} \left(\proj{\X}\right)_{nm} \y_m + h_n \y_n.
\end{aligned}
\]</span></p>
<p>How big can the off–diagonal entries <span class="math inline">\(\left(\proj{\X}\right)_{nm}\)</span> be when <span class="math inline">\(h_n\)</span> is large? Again using the <span class="math inline">\(n\)</span>–th standard basis vector <span class="math inline">\(\ev_n\)</span>, <span class="math display">\[
\begin{aligned}
1 ={}&amp; \norm{\ev_n}^2
\\\ge{}&amp; \norm{\proj{\X} \ev_n}^2
\\={}&amp; \ev_n^\trans \proj{\X} \proj{\X} \ev_n
\\={}&amp; \sum_{m=1}^N \left(\proj{\X}\right)_{nm}^2
\\={}&amp; \sum_{m \ne n} \left(\proj{\X}\right)_{nm}^2 + h_n^2,
\end{aligned}
\]</span> so that, when <span class="math inline">\(h_n \approx 1\)</span>, <span class="math display">\[
\sum_{m \ne n} \left(\proj{\X}\right)_{nm}^2 \le 1 - h_n^2 \approx 0,
\]</span> and so <span class="math display">\[
\yhat_n \approx h_n \y_n.
\]</span></p>
</section>
<section id="kleibers-whale-revisited" class="level2">
<h2 class="anchored" data-anchor-id="kleibers-whale-revisited">Kleiber’s whale revisited</h2>
<p>Recall Kleiber’s dataset of animal sizes and metabolisms. Let’s look at the original linear regression with and without the whale:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Look at the data, find outliers</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>lm_kl <span class="ot">&lt;-</span> <span class="fu">lm</span>(Metabol_kcal_per_day <span class="sc">~</span> Weight_kg <span class="sc">+</span> <span class="dv">1</span>, kleiber_df)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>lm_drop_kl <span class="ot">&lt;-</span> <span class="fu">lm</span>(Metabol_kcal_per_day <span class="sc">~</span> Weight_kg <span class="sc">+</span> <span class="dv">1</span>, kleiber_df <span class="sc">%&gt;%</span> <span class="fu">filter</span>(Animal <span class="sc">!=</span> <span class="st">"Whale"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Removed 1 row containing missing values (`geom_line()`).</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="7_InfluenceAndOutliers_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>In this case, the fact that the whale has high leverage was obvious. But we can also see this from the leverage scores:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="7_InfluenceAndOutliers_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="data-dropping" class="level1">
<h1>Data dropping</h1>
<p>Above, we’ve shown how extreme values of <span class="math inline">\(\xv_n\)</span> and <span class="math inline">\(\y_n\)</span> can affect a linear regression. These two concepts can be combined by considering the effect of “data dropping.” Specifically, we might ask how a regression would be different had we excluded a particular datapoint. This is a nice concept because it measures how “important” an entire datapoint is, rather than separately considering the response and regressor.</p>
<p>Let <span class="math inline">\(\betavhat_{-n}\)</span> denote the estimate of <span class="math inline">\(\betahat\)</span> with the datapoint <span class="math inline">\(n\)</span> left out. Recall that we wrote <span class="math display">\[
\betavhat =\sumn \omegav_n \y_n
\quad\quad\textrm{where }\omegav_n := (\X^\trans \X)^{-1} \xv_n.
\]</span> In your homework, you’ll show that, in linear regression, you can write down the exact formula:</p>
<p><span class="math display">\[
\betavhat_{-n} - \betavhat=   -\frac{\reshat_n}{1 - h_n} \omegav_n
\approx -\omegav_n \reshat_n \quad\textrm{(when $h_n$ is small)}
\]</span> The term <span class="math inline">\(\omegav_n\)</span> is very close to a leverage score — in fact, <span class="math inline">\(h_n = \xv_n^\trans \omegav_n\)</span>. This formula says that a regression coefficient will have a large change when both <span class="math inline">\(\reshat_n\)</span> and <span class="math inline">\(\omega_n\)</span> are large.</p>
<p>The negative of the approximation for small <span class="math inline">\(h_n\)</span> is known as the “empirical influence function” of the <span class="math inline">\(n\)</span>–th datapoint (sometimes just “influence function”):</p>
<p><span class="math display">\[
\textrm{Influence function of datapoint }n := \omegav_n \reshat_n.
\]</span></p>
<p>For linear models, it can be computed using the <code>influence</code> <code>R</code> function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>reg_all <span class="ot">&lt;-</span> <span class="fu">lm</span>(lm_form, births_df)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>infl <span class="ot">&lt;-</span> <span class="fu">influence</span>(reg_all)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>births_df <span class="ot">&lt;-</span> </span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>  births_df <span class="sc">%&gt;%</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">fage_infl=</span>infl<span class="sc">$</span>coefficients[, <span class="st">"fage"</span>]) <span class="sc">%&gt;%</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">yhat=</span>reg_all<span class="sc">$</span>fitted)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can look at the graph and color the points by their influence function. Clearly we can see that the large <code>fage</code> points have high influence.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="7_InfluenceAndOutliers_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<section id="bonus-content-data-dropping-generalization-to-nonlinear-estimators" class="level2">
<h2 class="anchored" data-anchor-id="bonus-content-data-dropping-generalization-to-nonlinear-estimators">Bonus content: Data dropping (generalization to nonlinear estimators)</h2>
<p>The above results rely heavily on the special structure of linear regression: e.g.&nbsp;the fact that <span class="math inline">\(\betahat\)</span> is a linear combination of <span class="math inline">\(\Y\)</span>, and that <span class="math inline">\(\Yhat\)</span> is a projection of <span class="math inline">\(\Y\)</span>. In more general settings such results are not available. In order to motivate the use of such diagnostics in more general settings (not covered in this class), let me introduce a slightly different approach based on derivatives.</p>
<p>Suppose we assign each datapoint a weight, <span class="math inline">\(\w_n\)</span>, and write</p>
<p><span class="math display">\[
\betavhat(\w) = \argmin{\beta} \sumn \w_n (\y_n - \xv_n^\trans \betav)^2.
\]</span></p>
<p>I have written <span class="math inline">\(\betahat(\wv)\)</span> because the optimal solution depends on the vector of weights, <span class="math inline">\(\wv = (\w_1, \ldots, \w_N)^\trans\)</span>. When <span class="math inline">\(\wv = \onev\)</span>, we recover the original problem. When we set one of the entries to zero, we remove that datapoint from the problem. Using this, we can approximate the effect of removing a datapoint using the first-order Taylor series expansion:</p>
<p><span class="math display">\[
\betahat_{-n} \approx
\betahat_{-n}^{linear} = \betahat + \frac{\partial \betahat(\wv)}{\partial \w_n}\vert_{\w_n=1} (0 - 1).
\]</span></p>
<p>One can show that</p>
<p><span class="math display">\[
\frac{\partial \betahat(\wv)}{\partial \w_n}\vert_{\w_n=1} = (\X^\trans \X)^{-1} \xv_n \reshat_n.
\]</span></p>
<p>Note that the Taylor series is a good approximation to the exact formula (given in the homework) when <span class="math inline">\(h_n \ll 1\)</span>, which is expected when <span class="math inline">\(h_n\)</span> goes to zero at rate <span class="math inline">\(1/N\)</span>:</p>
<p><span class="math display">\[
\betahat_{-n}  = \betahat - (\X^\trans \X)^{-1} \xv_n \frac{\reshat_n}{1 - h_n} \approx
\betahat - (\X^\trans \X)^{-1} \xv_n \reshat = \betahat_{-n}^{linear}.
\]</span></p>
<p>In more complicated nonlinear problems, the exact formula is unavailable, but the linear approximation is typically computed.</p>
<p>From this (or the exact formula), we can see that the effect of extreme values on <span class="math inline">\(\betahat\)</span> is actually a product of both <span class="math inline">\(\reshat_n\)</span> and <span class="math inline">\(\xv_n\)</span>. Large residuals will not have an effect when <span class="math inline">\(\xv_n = \zerov\)</span>, and outlier <span class="math inline">\(\xv_n\)</span> will not have an effect when <span class="math inline">\(\reshat_n = 0\)</span>.</p>
</section>
<section id="bonus-content-rank-one-updates-for-linear-regression-woodbury-formula" class="level2">
<h2 class="anchored" data-anchor-id="bonus-content-rank-one-updates-for-linear-regression-woodbury-formula">Bonus content: Rank-one updates for linear regression (Woodbury formula)</h2>
<p>The proof of the data–dropping formula for linear regression uses the Woodbury formula, for which I now give a proof for completness. Let <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> be <span class="math inline">\(N \times K\)</span> matrices, and <span class="math inline">\(\id_N\)</span> and <span class="math inline">\(\id_K\)</span> the <span class="math inline">\(N\times N\)</span> and <span class="math inline">\(K \times K\)</span> identity matrices, respectively. Using the fact that</p>
<p><span class="math display">\[
\begin{aligned}
(\id_N + U V^\trans)^{-1} (\id_N + U V^\trans) = \id_N
\quad\Rightarrow\quad&amp;
(\id_N + U V^\trans)^{-1} = \id_N - (\id_N + U V^\trans)^{-1} U V^\trans &amp; \textrm{(i)}\\
(\id_N + U V^\trans) U = U (\id_K +  V^\trans U)  
\quad\Rightarrow\quad&amp;
U (\id_N + V^\trans U )^{-1} = (\id_K + U^\trans V)^{-1} U  &amp; \textrm{(ii)}
\end{aligned}
\]</span></p>
<p>we have that</p>
<p><span class="math display">\[
\begin{aligned}
(\id_N + U V^\trans)^{-1} ={}&amp; \id_N - (\id_N + U V^\trans)^{-1} U V^\trans  &amp; \textrm{by (i)} \\
={}&amp; \id_N - U  (\id_K + V^\trans U )^{-1} V^\trans  &amp; \textrm{by (ii)}.
\end{aligned}
\]</span></p>
<p>We can use this to prove the Woodbury formula as a special case when <span class="math inline">\(K=1\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
(A + \uv \vv^\trans)^{-1} ={}&amp;
(A (\id_N  + A^{-1} \uv \vv^\trans))^{-1}
\\={}&amp;
(\id_N + (A^{-1} \uv) \vv^\trans )^{-1} A^{-1}
\\={}&amp;
(\id_N - A^{-1} \uv (1 + \vv^\trans A^{-1} \uv)^{-1} \vv^\trans ) A^{-1}
\\={}&amp;
A^{-1} - \frac{A^{-1} \uv \vv A^{-1}}{1 + \vv^\trans A^{-1} \uv}.
\end{aligned}
\]</span></p>
<p>This formula has importance for linear models far beyond the leave–one–out formula. For instance, the “kernel trick” in machine learning can be understood as an application of the Woodbury formula.</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb24" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Influence and outliers"</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co">    include-before-body:</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co">     - file: ../macros.md</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="fu"># Goals</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Discuss some was that extreme data can influence regression</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>There is no clear definition of an outlier</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The (unbounded) influence of outlying $y_n$</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Look at residuals using <span class="in">`fit$residuals`</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The influence of outyling $x_n$ and the leverage score</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Look at leverage scores using <span class="in">`hatvalues`</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The influence of removing a point (both leverage and residual)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Look at the influence function using <span class="in">`influence`</span></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>births_df <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"../datasets/births/births14.csv"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(fage))</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>kleiber_df <span class="ot">&lt;-</span> </span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">read.csv</span>(<span class="st">"../datasets/kleiber/kleiber.csv"</span>) </span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>mc_env <span class="ot">&lt;-</span> <span class="fu">new.env</span>()</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"../datasets/microcredit/microcredit_mx_data.Rdata"</span>, <span class="at">envir=</span>mc_env)</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a><span class="fu">ls</span>(mc_env)</span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>mx_df <span class="ot">&lt;-</span> mc_env<span class="sc">$</span>mx_df</span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a><span class="fu"># Births data</span></span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>Let's look at the <span class="in">`births14`</span> dataset, a random selection of 1000 observations</span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>from the US government</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(births_df)</span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>Although this is not a randomized controlled trial, we might look in the</span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a>data for suggestive patterns to guide or support future research.  This is </span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a>an **inference** problem.</span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>In particular, let's ask how father's age might affect birth weight.</span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>lm_form <span class="ot">&lt;-</span> <span class="fu">formula</span>(weight <span class="sc">~</span> fage )</span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a>reg_all <span class="ot">&lt;-</span> <span class="fu">lm</span>(lm_form, births_df)</span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_all)<span class="sc">$</span>coefficients</span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a>**How can we interpret this?**</span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-74"><a href="#cb24-74" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-75"><a href="#cb24-75" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(births_df) <span class="sc">+</span></span>
<span id="cb24-78"><a href="#cb24-78" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>fage, <span class="at">y=</span>weight), <span class="at">alpha=</span><span class="fl">0.2</span>, <span class="at">size=</span><span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb24-79"><a href="#cb24-79" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>fage, <span class="at">y=</span><span class="fu">predict</span>(reg_all, births_df))) </span>
<span id="cb24-80"><a href="#cb24-80" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb24-81"><a href="#cb24-81" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-82"><a href="#cb24-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-83"><a href="#cb24-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-84"><a href="#cb24-84" aria-hidden="true" tabindex="-1"></a><span class="fu">## Outliers in the births data</span></span>
<span id="cb24-85"><a href="#cb24-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-86"><a href="#cb24-86" aria-hidden="true" tabindex="-1"></a>We get pretty different slopes with and without those three very old fathers!</span>
<span id="cb24-87"><a href="#cb24-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-90"><a href="#cb24-90" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-91"><a href="#cb24-91" aria-hidden="true" tabindex="-1"></a>age_threshold <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb24-92"><a href="#cb24-92" aria-hidden="true" tabindex="-1"></a>reg_drop <span class="ot">&lt;-</span> <span class="fu">lm</span>(lm_form, births_df <span class="sc">%&gt;%</span> <span class="fu">filter</span>(fage <span class="sc">&lt;</span> age_threshold))</span>
<span id="cb24-93"><a href="#cb24-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-94"><a href="#cb24-94" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_all)<span class="sc">$</span>coefficients</span>
<span id="cb24-95"><a href="#cb24-95" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_drop)<span class="sc">$</span>coefficients</span>
<span id="cb24-96"><a href="#cb24-96" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-97"><a href="#cb24-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-100"><a href="#cb24-100" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-101"><a href="#cb24-101" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-102"><a href="#cb24-102" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(births_df) <span class="sc">+</span></span>
<span id="cb24-103"><a href="#cb24-103" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>fage, <span class="at">y=</span>weight, <span class="at">shape=</span>fage <span class="sc">&gt;=</span> age_threshold), <span class="at">alpha=</span><span class="fl">0.2</span>, <span class="at">size=</span><span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb24-104"><a href="#cb24-104" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>fage, <span class="at">y=</span><span class="fu">predict</span>(reg_all, births_df), <span class="at">color=</span><span class="st">"all"</span>)) <span class="sc">+</span></span>
<span id="cb24-105"><a href="#cb24-105" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>fage, <span class="at">y=</span><span class="fu">predict</span>(reg_drop, births_df), <span class="at">color=</span><span class="st">"without outliers"</span>))</span>
<span id="cb24-106"><a href="#cb24-106" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-107"><a href="#cb24-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-108"><a href="#cb24-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-109"><a href="#cb24-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-110"><a href="#cb24-110" aria-hidden="true" tabindex="-1"></a><span class="fu"># Outliers</span></span>
<span id="cb24-111"><a href="#cb24-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-112"><a href="#cb24-112" aria-hidden="true" tabindex="-1"></a>What should we do?</span>
<span id="cb24-113"><a href="#cb24-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-114"><a href="#cb24-114" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Remove the points with old fathers?</span>
<span id="cb24-115"><a href="#cb24-115" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Regress on them separately?</span>
<span id="cb24-116"><a href="#cb24-116" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Regress on $\log \textrm{fage}$?</span>
<span id="cb24-117"><a href="#cb24-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-118"><a href="#cb24-118" aria-hidden="true" tabindex="-1"></a>It depends on what we're trying to do,</span>
<span id="cb24-119"><a href="#cb24-119" aria-hidden="true" tabindex="-1"></a>and why we think those observations are so extreme.  </span>
<span id="cb24-120"><a href="#cb24-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-121"><a href="#cb24-121" aria-hidden="true" tabindex="-1"></a>Data can be an "outlier" because:</span>
<span id="cb24-122"><a href="#cb24-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-123"><a href="#cb24-123" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It's an extreme (but important) value the data can actually take</span>
<span id="cb24-124"><a href="#cb24-124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It's an extreme (and unimportant) value the data can actually take</span>
<span id="cb24-125"><a href="#cb24-125" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The data was entered incorrectly or corrupted</span>
<span id="cb24-126"><a href="#cb24-126" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Data from a different source mixed in with more typical data</span>
<span id="cb24-127"><a href="#cb24-127" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Adversaries trying to mess with your data to produce some desired conclusion</span>
<span id="cb24-128"><a href="#cb24-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-129"><a href="#cb24-129" aria-hidden="true" tabindex="-1"></a>**There are no good general answers or definitions of outliers.**  </span>
<span id="cb24-130"><a href="#cb24-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-131"><a href="#cb24-131" aria-hidden="true" tabindex="-1"></a>Today we will be studying only how and why extreme values can change a regression.</span>
<span id="cb24-132"><a href="#cb24-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-133"><a href="#cb24-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-134"><a href="#cb24-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-135"><a href="#cb24-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-136"><a href="#cb24-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-137"><a href="#cb24-137" aria-hidden="true" tabindex="-1"></a><span class="fu"># Unusual responses</span></span>
<span id="cb24-138"><a href="#cb24-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-139"><a href="#cb24-139" aria-hidden="true" tabindex="-1"></a>Recall that $\betavhat = (\X^\trans \X)^{-1} \X^ \trans \Y$.  This can be written</span>
<span id="cb24-140"><a href="#cb24-140" aria-hidden="true" tabindex="-1"></a>as a weighted sum of $\y_n$:</span>
<span id="cb24-141"><a href="#cb24-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-142"><a href="#cb24-142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-143"><a href="#cb24-143" aria-hidden="true" tabindex="-1"></a>\betavhat = (\X^\trans \X)^{-1} \X^\trans \Y = </span>
<span id="cb24-144"><a href="#cb24-144" aria-hidden="true" tabindex="-1"></a>  \sumn (\X^\trans \X)^{-1} \xv_n \y_n =: \sumn \omegav_n \y_n.</span>
<span id="cb24-145"><a href="#cb24-145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-146"><a href="#cb24-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-147"><a href="#cb24-147" aria-hidden="true" tabindex="-1"></a>It is clear that we can produce **arbitrarily large changes in $\betavhat$** by producing </span>
<span id="cb24-148"><a href="#cb24-148" aria-hidden="true" tabindex="-1"></a>arbitrarily large changes **in only a single $\y_n$**.</span>
<span id="cb24-149"><a href="#cb24-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-150"><a href="#cb24-150" aria-hidden="true" tabindex="-1"></a>We can make the births fit arbitrarily crazy by changing a single observation:</span>
<span id="cb24-153"><a href="#cb24-153" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-154"><a href="#cb24-154" aria-hidden="true" tabindex="-1"></a>fit_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>()</span>
<span id="cb24-155"><a href="#cb24-155" aria-hidden="true" tabindex="-1"></a>modify_row <span class="ot">&lt;-</span> <span class="fu">which</span>(births_df<span class="sc">$</span>fage <span class="sc">==</span> <span class="dv">40</span>)[<span class="dv">1</span>]</span>
<span id="cb24-156"><a href="#cb24-156" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (res <span class="cf">in</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">100</span>, <span class="dv">1000</span>)) {</span>
<span id="cb24-157"><a href="#cb24-157" aria-hidden="true" tabindex="-1"></a>  births_modified_df <span class="ot">&lt;-</span> births_df</span>
<span id="cb24-158"><a href="#cb24-158" aria-hidden="true" tabindex="-1"></a>  births_modified_df[modify_row, <span class="st">"weight"</span>] <span class="ot">&lt;-</span></span>
<span id="cb24-159"><a href="#cb24-159" aria-hidden="true" tabindex="-1"></a>    births_modified_df[modify_row, <span class="st">"weight"</span>] <span class="sc">+</span> res</span>
<span id="cb24-160"><a href="#cb24-160" aria-hidden="true" tabindex="-1"></a>  reg_mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(lm_form, births_modified_df)</span>
<span id="cb24-161"><a href="#cb24-161" aria-hidden="true" tabindex="-1"></a>  fit_df <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(</span>
<span id="cb24-162"><a href="#cb24-162" aria-hidden="true" tabindex="-1"></a>    fit_df,</span>
<span id="cb24-163"><a href="#cb24-163" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(births_modified_df, fage, weight) <span class="sc">%&gt;%</span></span>
<span id="cb24-164"><a href="#cb24-164" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">yhat=</span><span class="fu">fitted</span>(reg_mod), <span class="at">res=</span>res)</span>
<span id="cb24-165"><a href="#cb24-165" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb24-166"><a href="#cb24-166" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-167"><a href="#cb24-167" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-168"><a href="#cb24-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-171"><a href="#cb24-171" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-172"><a href="#cb24-172" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-173"><a href="#cb24-173" aria-hidden="true" tabindex="-1"></a>res_points <span class="ot">&lt;-</span> </span>
<span id="cb24-174"><a href="#cb24-174" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_rows</span>(</span>
<span id="cb24-175"><a href="#cb24-175" aria-hidden="true" tabindex="-1"></a>      births_df[modify_row, ] <span class="sc">%&gt;%</span> <span class="fu">select</span>(fage, weight),</span>
<span id="cb24-176"><a href="#cb24-176" aria-hidden="true" tabindex="-1"></a>      births_df[modify_row, ] <span class="sc">%&gt;%</span> <span class="fu">select</span>(fage, weight) <span class="sc">%&gt;%</span></span>
<span id="cb24-177"><a href="#cb24-177" aria-hidden="true" tabindex="-1"></a>        <span class="fu">mutate</span>(<span class="at">weight=</span>weight <span class="sc">+</span> <span class="dv">10</span>),</span>
<span id="cb24-178"><a href="#cb24-178" aria-hidden="true" tabindex="-1"></a>      births_df[modify_row, ] <span class="sc">%&gt;%</span> <span class="fu">select</span>(fage, weight) <span class="sc">%&gt;%</span></span>
<span id="cb24-179"><a href="#cb24-179" aria-hidden="true" tabindex="-1"></a>        <span class="fu">mutate</span>(<span class="at">weight=</span>weight <span class="sc">+</span> <span class="dv">20</span>),</span>
<span id="cb24-180"><a href="#cb24-180" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb24-181"><a href="#cb24-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-182"><a href="#cb24-182" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(fit_df) <span class="sc">+</span></span>
<span id="cb24-183"><a href="#cb24-183" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>fage, <span class="at">y=</span>weight), <span class="at">alpha=</span><span class="fl">0.2</span>, <span class="at">data=</span>births_df) <span class="sc">+</span></span>
<span id="cb24-184"><a href="#cb24-184" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>fage, <span class="at">y=</span>yhat, <span class="at">group=</span>res, <span class="at">color=</span><span class="fu">factor</span>(res))) <span class="sc">+</span></span>
<span id="cb24-185"><a href="#cb24-185" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>fage, <span class="at">y=</span>weight), <span class="at">color=</span><span class="st">"red"</span>, <span class="at">data=</span>res_points) <span class="sc">+</span></span>
<span id="cb24-186"><a href="#cb24-186" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Effect on the regression fit of adding `res` to the red point"</span>)</span>
<span id="cb24-187"><a href="#cb24-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-188"><a href="#cb24-188" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-189"><a href="#cb24-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-190"><a href="#cb24-190" aria-hidden="true" tabindex="-1"></a><span class="fu">## Unusual responses (look at residuals)</span></span>
<span id="cb24-191"><a href="#cb24-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-192"><a href="#cb24-192" aria-hidden="true" tabindex="-1"></a>Outlier $\y_n$ will also tend to have outlier residuals.  </span>
<span id="cb24-193"><a href="#cb24-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-194"><a href="#cb24-194" aria-hidden="true" tabindex="-1"></a>To see this, let's suppose there is one abberant value, $\y_*$, which is very large,</span>
<span id="cb24-195"><a href="#cb24-195" aria-hidden="true" tabindex="-1"></a>and which we enumerate separately from the well-behaved $\y_n$.</span>
<span id="cb24-196"><a href="#cb24-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-197"><a href="#cb24-197" aria-hidden="true" tabindex="-1"></a>Assume that $\xv_*$ is not an outlier (i.e. the response is an</span>
<span id="cb24-198"><a href="#cb24-198" aria-hidden="true" tabindex="-1"></a>outlier but the regressor is not).  That means $\frac{1}{N} (\X^\trans \X + \xv_* \xv_*^\trans) \approx \frac{1}{N} \X^\trans \X$.  Let $\betavhat_*= (\X^\trans \X)^{-1} \X^\trans \Y$ denote</span>
<span id="cb24-199"><a href="#cb24-199" aria-hidden="true" tabindex="-1"></a>the estimated coefficient without the outlier.</span>
<span id="cb24-200"><a href="#cb24-200" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb24-201"><a href="#cb24-201" aria-hidden="true" tabindex="-1"></a>We can write the data together with the outlier as</span>
<span id="cb24-202"><a href="#cb24-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-203"><a href="#cb24-203" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-204"><a href="#cb24-204" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb24-205"><a href="#cb24-205" aria-hidden="true" tabindex="-1"></a>\Y <span class="sc">\\</span></span>
<span id="cb24-206"><a href="#cb24-206" aria-hidden="true" tabindex="-1"></a>\y_*</span>
<span id="cb24-207"><a href="#cb24-207" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb24-208"><a href="#cb24-208" aria-hidden="true" tabindex="-1"></a>\quad\quad</span>
<span id="cb24-209"><a href="#cb24-209" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb24-210"><a href="#cb24-210" aria-hidden="true" tabindex="-1"></a>\X <span class="sc">\\</span></span>
<span id="cb24-211"><a href="#cb24-211" aria-hidden="true" tabindex="-1"></a>\xv_*^\trans</span>
<span id="cb24-212"><a href="#cb24-212" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb24-213"><a href="#cb24-213" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-214"><a href="#cb24-214" aria-hidden="true" tabindex="-1"></a>We have</span>
<span id="cb24-215"><a href="#cb24-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-216"><a href="#cb24-216" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-217"><a href="#cb24-217" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb24-218"><a href="#cb24-218" aria-hidden="true" tabindex="-1"></a>\reshat_* ={}&amp; \y_* - \yhat_* </span>
<span id="cb24-219"><a href="#cb24-219" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>={}&amp; \y^* - \xv_*^\trans \betahat</span>
<span id="cb24-220"><a href="#cb24-220" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>={}&amp; \y^* - \xv_*^\trans (\X^\trans \X + \xv_* \xv_*^\trans)^{-1} (\X^\trans \Y + \xv_* \y_*)</span>
<span id="cb24-221"><a href="#cb24-221" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>={}&amp; \y^* - \xv_*^\trans \left(</span>
<span id="cb24-222"><a href="#cb24-222" aria-hidden="true" tabindex="-1"></a>    \frac{1}{N} \left(</span>
<span id="cb24-223"><a href="#cb24-223" aria-hidden="true" tabindex="-1"></a>    \X^\trans \X + \xv_* \xv_*^\trans </span>
<span id="cb24-224"><a href="#cb24-224" aria-hidden="true" tabindex="-1"></a>    \right)</span>
<span id="cb24-225"><a href="#cb24-225" aria-hidden="true" tabindex="-1"></a>  \right)^{-1}</span>
<span id="cb24-226"><a href="#cb24-226" aria-hidden="true" tabindex="-1"></a>  \frac{1}{N} (\X^\trans \Y + \xv_* \y_*)</span>
<span id="cb24-227"><a href="#cb24-227" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>\approx{}&amp; \y^* - \xv_*^\trans \left(</span>
<span id="cb24-228"><a href="#cb24-228" aria-hidden="true" tabindex="-1"></a>    \frac{1}{N} \left(</span>
<span id="cb24-229"><a href="#cb24-229" aria-hidden="true" tabindex="-1"></a>    \X^\trans \X </span>
<span id="cb24-230"><a href="#cb24-230" aria-hidden="true" tabindex="-1"></a>    \right)</span>
<span id="cb24-231"><a href="#cb24-231" aria-hidden="true" tabindex="-1"></a>  \right)^{-1}</span>
<span id="cb24-232"><a href="#cb24-232" aria-hidden="true" tabindex="-1"></a>  \frac{1}{N} (\X^\trans \Y + \xv_* \y_*)</span>
<span id="cb24-233"><a href="#cb24-233" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>=&amp; \y^* - \xv_*^\trans \betavhat_* + </span>
<span id="cb24-234"><a href="#cb24-234" aria-hidden="true" tabindex="-1"></a>  \frac{1}{N} \xv_*^\trans </span>
<span id="cb24-235"><a href="#cb24-235" aria-hidden="true" tabindex="-1"></a>  \left(\frac{1}{N} \X^\trans \X \right)^{-1} \xv_* \y_*</span>
<span id="cb24-236"><a href="#cb24-236" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>\approx&amp; \y^* - \xv_*^\trans \betavhat_*</span>
<span id="cb24-237"><a href="#cb24-237" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb24-238"><a href="#cb24-238" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-239"><a href="#cb24-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-240"><a href="#cb24-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-241"><a href="#cb24-241" aria-hidden="true" tabindex="-1"></a>This means that although</span>
<span id="cb24-242"><a href="#cb24-242" aria-hidden="true" tabindex="-1"></a>the $\y_*$ causes the $\betahat$ to grow very large in an attempt</span>
<span id="cb24-243"><a href="#cb24-243" aria-hidden="true" tabindex="-1"></a>to fit it, its residual remains large, and with the same sign as $\y_*$.</span>
<span id="cb24-244"><a href="#cb24-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-245"><a href="#cb24-245" aria-hidden="true" tabindex="-1"></a>This means you may be able to identify outlier responses by looking</span>
<span id="cb24-246"><a href="#cb24-246" aria-hidden="true" tabindex="-1"></a>at a residual plot, e.g., a histogram of residuals, and seeing if any </span>
<span id="cb24-247"><a href="#cb24-247" aria-hidden="true" tabindex="-1"></a>fitted residuals are atypical.</span>
<span id="cb24-248"><a href="#cb24-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-249"><a href="#cb24-249" aria-hidden="true" tabindex="-1"></a><span class="fu">## Microcredit data</span></span>
<span id="cb24-250"><a href="#cb24-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-251"><a href="#cb24-251" aria-hidden="true" tabindex="-1"></a>Let's take a look at some data from a study of microcredit in Mexico.  The goal</span>
<span id="cb24-252"><a href="#cb24-252" aria-hidden="true" tabindex="-1"></a>of the study is to estimate the effect of microcredit, which was randomly</span>
<span id="cb24-253"><a href="#cb24-253" aria-hidden="true" tabindex="-1"></a>allocated.  </span>
<span id="cb24-254"><a href="#cb24-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-255"><a href="#cb24-255" aria-hidden="true" tabindex="-1"></a>For example, we might try to measure the effect of the treatment on "temptation goods."  We</span>
<span id="cb24-256"><a href="#cb24-256" aria-hidden="true" tabindex="-1"></a>run the model $\textrm{Temptation spend} ~ 1 + \textrm{treatment}$:</span>
<span id="cb24-257"><a href="#cb24-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-260"><a href="#cb24-260" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-261"><a href="#cb24-261" aria-hidden="true" tabindex="-1"></a>reg_mx <span class="ot">&lt;-</span> <span class="fu">lm</span>(temptation <span class="sc">~</span> treatment, mx_df)</span>
<span id="cb24-262"><a href="#cb24-262" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_mx)</span>
<span id="cb24-263"><a href="#cb24-263" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-264"><a href="#cb24-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-265"><a href="#cb24-265" aria-hidden="true" tabindex="-1"></a>However, we see that there are huge residuals:</span>
<span id="cb24-268"><a href="#cb24-268" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-269"><a href="#cb24-269" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-270"><a href="#cb24-270" aria-hidden="true" tabindex="-1"></a>mx_df <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">resid=</span>reg_mx<span class="sc">$</span>residuals) <span class="sc">%&gt;%</span></span>
<span id="cb24-271"><a href="#cb24-271" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb24-272"><a href="#cb24-272" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">x=</span>resid))</span>
<span id="cb24-273"><a href="#cb24-273" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-274"><a href="#cb24-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-275"><a href="#cb24-275" aria-hidden="true" tabindex="-1"></a>This is because spending has a very heavy tail!</span>
<span id="cb24-276"><a href="#cb24-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-279"><a href="#cb24-279" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-280"><a href="#cb24-280" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-281"><a href="#cb24-281" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mx_df) <span class="sc">+</span></span>
<span id="cb24-282"><a href="#cb24-282" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">x=</span>temptation, <span class="at">fill=</span><span class="fu">factor</span>(treatment)), <span class="at">bins=</span><span class="dv">200</span>) <span class="sc">+</span></span>
<span id="cb24-283"><a href="#cb24-283" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(treatment <span class="sc">~</span> .)</span>
<span id="cb24-284"><a href="#cb24-284" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-285"><a href="#cb24-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-288"><a href="#cb24-288" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-289"><a href="#cb24-289" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-290"><a href="#cb24-290" aria-hidden="true" tabindex="-1"></a><span class="co">#| display: false</span></span>
<span id="cb24-291"><a href="#cb24-291" aria-hidden="true" tabindex="-1"></a>mx_df<span class="sc">$</span>drop <span class="ot">&lt;-</span> reg_mx<span class="sc">$</span>residuals <span class="sc">&gt;</span> <span class="fu">quantile</span>(reg_mx<span class="sc">$</span>residuals, (<span class="dv">1</span> <span class="sc">-</span> <span class="dv">5</span> <span class="sc">/</span> <span class="fu">nrow</span>(mx_df)))</span>
<span id="cb24-292"><a href="#cb24-292" aria-hidden="true" tabindex="-1"></a>reg_drop_mx <span class="ot">&lt;-</span> </span>
<span id="cb24-293"><a href="#cb24-293" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm</span>(temptation <span class="sc">~</span> treatment, mx_df <span class="sc">%&gt;%</span> <span class="fu">filter</span>(<span class="sc">!</span>drop))</span>
<span id="cb24-294"><a href="#cb24-294" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-295"><a href="#cb24-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-296"><a href="#cb24-296" aria-hidden="true" tabindex="-1"></a>We can produce big changes in the regression by dropping the</span>
<span id="cb24-297"><a href="#cb24-297" aria-hidden="true" tabindex="-1"></a><span class="in">`{r} sum(mx_df$drop)`</span> largest residuals (which is only <span class="in">`{r} 100 * sum(mx_df$drop) / nrow(mx_df)`</span></span>
<span id="cb24-298"><a href="#cb24-298" aria-hidden="true" tabindex="-1"></a>percent of the data):</span>
<span id="cb24-299"><a href="#cb24-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-302"><a href="#cb24-302" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-303"><a href="#cb24-303" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"With all datapoints:"</span>)</span>
<span id="cb24-304"><a href="#cb24-304" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_mx)<span class="sc">$</span>coefficients</span>
<span id="cb24-305"><a href="#cb24-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-306"><a href="#cb24-306" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Without the largest residual datapoints:"</span>)</span>
<span id="cb24-307"><a href="#cb24-307" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_drop_mx)<span class="sc">$</span>coefficients</span>
<span id="cb24-308"><a href="#cb24-308" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-309"><a href="#cb24-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-310"><a href="#cb24-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-311"><a href="#cb24-311" aria-hidden="true" tabindex="-1"></a><span class="fu"># Unusual regressors</span></span>
<span id="cb24-312"><a href="#cb24-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-313"><a href="#cb24-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-314"><a href="#cb24-314" aria-hidden="true" tabindex="-1"></a>Unusually large regressor values are called "high leverage points," since</span>
<span id="cb24-315"><a href="#cb24-315" aria-hidden="true" tabindex="-1"></a>small changes in $\beta$ produce large changes in the fitted</span>
<span id="cb24-316"><a href="#cb24-316" aria-hidden="true" tabindex="-1"></a>values at the corresponding points.  Since $\xv_n$ is a vector,</span>
<span id="cb24-317"><a href="#cb24-317" aria-hidden="true" tabindex="-1"></a>measuring what it means for $\xv_n$ to be an outlier is a little more</span>
<span id="cb24-318"><a href="#cb24-318" aria-hidden="true" tabindex="-1"></a>subtle than measuring what it means for a scalar like $\y_n$ to </span>
<span id="cb24-319"><a href="#cb24-319" aria-hidden="true" tabindex="-1"></a>be an outlier.  But a sensible thing to do is to measure the size</span>
<span id="cb24-320"><a href="#cb24-320" aria-hidden="true" tabindex="-1"></a>of $\xv_n$ relative to $\X^\trans \X$, which estimates</span>
<span id="cb24-321"><a href="#cb24-321" aria-hidden="true" tabindex="-1"></a>the spread of the $\xv_n$ values (if they are centered, it </span>
<span id="cb24-322"><a href="#cb24-322" aria-hidden="true" tabindex="-1"></a>is an estimate of the covariance).  We define the "leverage"</span>
<span id="cb24-323"><a href="#cb24-323" aria-hidden="true" tabindex="-1"></a>score</span>
<span id="cb24-324"><a href="#cb24-324" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-325"><a href="#cb24-325" aria-hidden="true" tabindex="-1"></a>h_n := \xv_n^\trans (\X^\trans \X)^{-1} \xv_n,</span>
<span id="cb24-326"><a href="#cb24-326" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-327"><a href="#cb24-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-328"><a href="#cb24-328" aria-hidden="true" tabindex="-1"></a>and we can check for unusual $\xv_n$ by looking for high leverage</span>
<span id="cb24-329"><a href="#cb24-329" aria-hidden="true" tabindex="-1"></a>scores.</span>
<span id="cb24-330"><a href="#cb24-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-331"><a href="#cb24-331" aria-hidden="true" tabindex="-1"></a>Note that $h_n$ is the $n$--th diagonal entry of the "hat" matrix</span>
<span id="cb24-332"><a href="#cb24-332" aria-hidden="true" tabindex="-1"></a>$\proj{\X} = \X (\X^\trans \X)^{-1} \X^\trans$, so called because</span>
<span id="cb24-333"><a href="#cb24-333" aria-hidden="true" tabindex="-1"></a>it "puts the hat on $\Y$", since $\Yhat = \proj{\X} \Y$.  There</span>
<span id="cb24-334"><a href="#cb24-334" aria-hidden="true" tabindex="-1"></a>are a few useful consequences of this fact.</span>
<span id="cb24-335"><a href="#cb24-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-336"><a href="#cb24-336" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$0 \le h_n \le 1$</span>
<span id="cb24-337"><a href="#cb24-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-338"><a href="#cb24-338" aria-hidden="true" tabindex="-1"></a>*Proof:* Take the vector $\ev_n$ that is $1$ in entry $n$ and $0$</span>
<span id="cb24-339"><a href="#cb24-339" aria-hidden="true" tabindex="-1"></a>elsewhere.  </span>
<span id="cb24-340"><a href="#cb24-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-341"><a href="#cb24-341" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-342"><a href="#cb24-342" aria-hidden="true" tabindex="-1"></a>h_n = \ev_n^\trans \proj{\X} \ev_n = \ev_n^\trans \proj{\X} \ev_n = \norm{\proj{\X} \ev_n}^2</span>
<span id="cb24-343"><a href="#cb24-343" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-344"><a href="#cb24-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-345"><a href="#cb24-345" aria-hidden="true" tabindex="-1"></a>Since $\norm{h_n}^2 = 1$, and $\norm{\proj{\X} \ev_n}^2 \le \norm{\ev_n}$, it follows</span>
<span id="cb24-346"><a href="#cb24-346" aria-hidden="true" tabindex="-1"></a>that $h_n \le 1$.  And since $\norm{\proj{\X} \ev_n}^2 \ge 0$, it follows that $h_n \ge 0$.</span>
<span id="cb24-347"><a href="#cb24-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-348"><a href="#cb24-348" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\sumn h_n = P$</span>
<span id="cb24-349"><a href="#cb24-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-350"><a href="#cb24-350" aria-hidden="true" tabindex="-1"></a>There are $P$ linearly independent unit vectors in the column span of $\X$.  Call</span>
<span id="cb24-351"><a href="#cb24-351" aria-hidden="true" tabindex="-1"></a>them $\uv_1, \ldots, \uv_P$.  Since $\proj{\X} \uv_p = \uv_p$, the $\uv_p$ are all</span>
<span id="cb24-352"><a href="#cb24-352" aria-hidden="true" tabindex="-1"></a>eigenvectors with eigenvalues one.  Similarly, the $N - P$ unit vectors spanning</span>
<span id="cb24-353"><a href="#cb24-353" aria-hidden="true" tabindex="-1"></a>the space orthogonal to $\X$ are eigenvectors with eigenvalue $0$.  </span>
<span id="cb24-354"><a href="#cb24-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-355"><a href="#cb24-355" aria-hidden="true" tabindex="-1"></a>Finally, the $\trace{\proj{\X}} = \sum_n h_n = P$ since the trace is the sum of the eigenvalues.  </span>
<span id="cb24-356"><a href="#cb24-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-357"><a href="#cb24-357" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>At least some $h_n &gt; 0$.</span>
<span id="cb24-358"><a href="#cb24-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-359"><a href="#cb24-359" aria-hidden="true" tabindex="-1"></a>This follows directly from $h_n \ge 0$ and $\sum_n h_n = P$.</span>
<span id="cb24-360"><a href="#cb24-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-361"><a href="#cb24-361" aria-hidden="true" tabindex="-1"></a>Additionally, we can see that $\frac{d\yhat_n}{d \y_n}  = h_n$. (This </span>
<span id="cb24-362"><a href="#cb24-362" aria-hidden="true" tabindex="-1"></a>fact that one can use to define "leverage scores" in settings</span>
<span id="cb24-363"><a href="#cb24-363" aria-hidden="true" tabindex="-1"></a>beyond linear regression.)</span>
<span id="cb24-364"><a href="#cb24-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-365"><a href="#cb24-365" aria-hidden="true" tabindex="-1"></a>Putting this together, we can see that</span>
<span id="cb24-366"><a href="#cb24-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-367"><a href="#cb24-367" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Since $\sumn h_n = P$ and $0 \le h_n \le 1$, not too many leverage scores</span>
<span id="cb24-368"><a href="#cb24-368" aria-hidden="true" tabindex="-1"></a>  can be large.  </span>
<span id="cb24-369"><a href="#cb24-369" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>On average, a typical $h_n \approx P / N$ if the data is well-behaved.</span>
<span id="cb24-370"><a href="#cb24-370" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If a leverage score is large, it means that the value of $\y_n$ has a high</span>
<span id="cb24-371"><a href="#cb24-371" aria-hidden="true" tabindex="-1"></a>  influence on its own fit, $\yhat_n$.</span>
<span id="cb24-372"><a href="#cb24-372" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If a leverage score is large, $\xv_n$ is large relative to the estimated</span>
<span id="cb24-373"><a href="#cb24-373" aria-hidden="true" tabindex="-1"></a>  "covariance" $\meann \X^\trans \X$, up to its expected scaling of $1/N$.</span>
<span id="cb24-374"><a href="#cb24-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-375"><a href="#cb24-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-376"><a href="#cb24-376" aria-hidden="true" tabindex="-1"></a><span class="fu">## Effect of high leverage</span></span>
<span id="cb24-377"><a href="#cb24-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-378"><a href="#cb24-378" aria-hidden="true" tabindex="-1"></a>What happens to a regression when you have very high leverage?  Suppose</span>
<span id="cb24-379"><a href="#cb24-379" aria-hidden="true" tabindex="-1"></a>that $h_n = 1$.  That means that the vector $\ev_n$, which has $1$ in </span>
<span id="cb24-380"><a href="#cb24-380" aria-hidden="true" tabindex="-1"></a>entry $n$ and $0$ otherwise, is an eigenvector of $\proj{\X}$, and</span>
<span id="cb24-381"><a href="#cb24-381" aria-hidden="true" tabindex="-1"></a>consequently $\ev_n$ is in the column span of $\X$.  This means that</span>
<span id="cb24-382"><a href="#cb24-382" aria-hidden="true" tabindex="-1"></a>the $n$--th entry of $\proj{\X} \Y$ is given by</span>
<span id="cb24-383"><a href="#cb24-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-384"><a href="#cb24-384" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-385"><a href="#cb24-385" aria-hidden="true" tabindex="-1"></a>\yhat_n = \left( \proj{\X} \Y \right)_n = \y_n.</span>
<span id="cb24-386"><a href="#cb24-386" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-387"><a href="#cb24-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-388"><a href="#cb24-388" aria-hidden="true" tabindex="-1"></a>This is the same as $\y_n \approx \yhat_n = \betavhat^\trans \xv_n$,</span>
<span id="cb24-389"><a href="#cb24-389" aria-hidden="true" tabindex="-1"></a>which forces the fit to pass through the point $(\xv_n, \y_n)$.</span>
<span id="cb24-390"><a href="#cb24-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-391"><a href="#cb24-391" aria-hidden="true" tabindex="-1"></a>If there are $P$ such high--leverage points, then $\betavhat$ is completely</span>
<span id="cb24-392"><a href="#cb24-392" aria-hidden="true" tabindex="-1"></a>determined by these points.</span>
<span id="cb24-393"><a href="#cb24-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-394"><a href="#cb24-394" aria-hidden="true" tabindex="-1"></a>Note that, since $\sumn h_n = P$ and $0 \le h_n \le 1$, there can only</span>
<span id="cb24-395"><a href="#cb24-395" aria-hidden="true" tabindex="-1"></a>be $P$ leverage points that are approximately equal to one.</span>
<span id="cb24-396"><a href="#cb24-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-397"><a href="#cb24-397" aria-hidden="true" tabindex="-1"></a>Although this intuition is for leverage scores that are exactly one, it</span>
<span id="cb24-398"><a href="#cb24-398" aria-hidden="true" tabindex="-1"></a>applies to leverage scores that are large but not exactly one --- outlier</span>
<span id="cb24-399"><a href="#cb24-399" aria-hidden="true" tabindex="-1"></a>$\xv_n$, with large leverage scores, force the regression line to fit the</span>
<span id="cb24-400"><a href="#cb24-400" aria-hidden="true" tabindex="-1"></a>point, unlike outlier $y_n$ with large residuals.  To see this, note</span>
<span id="cb24-401"><a href="#cb24-401" aria-hidden="true" tabindex="-1"></a>that </span>
<span id="cb24-402"><a href="#cb24-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-403"><a href="#cb24-403" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-404"><a href="#cb24-404" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb24-405"><a href="#cb24-405" aria-hidden="true" tabindex="-1"></a>\yhat_n ={}&amp; \left(\proj{\X} \Y \right)_{n} <span class="sc">\\</span></span>
<span id="cb24-406"><a href="#cb24-406" aria-hidden="true" tabindex="-1"></a>={}&amp; \sum_{m=1}^N \left(\proj{\X}\right)_{nm} \y_m <span class="sc">\\</span></span>
<span id="cb24-407"><a href="#cb24-407" aria-hidden="true" tabindex="-1"></a>={}&amp; \sum_{m \ne n} \left(\proj{\X}\right)_{nm} \y_m + h_n \y_n.</span>
<span id="cb24-408"><a href="#cb24-408" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb24-409"><a href="#cb24-409" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-410"><a href="#cb24-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-411"><a href="#cb24-411" aria-hidden="true" tabindex="-1"></a>How big can the off--diagonal entries $\left(\proj{\X}\right)_{nm}$ be</span>
<span id="cb24-412"><a href="#cb24-412" aria-hidden="true" tabindex="-1"></a>when $h_n$ is large?  Again using the $n$--th standard basis vector $\ev_n$,</span>
<span id="cb24-413"><a href="#cb24-413" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-414"><a href="#cb24-414" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb24-415"><a href="#cb24-415" aria-hidden="true" tabindex="-1"></a>1 ={}&amp; \norm{\ev_n}^2 </span>
<span id="cb24-416"><a href="#cb24-416" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>\ge{}&amp; \norm{\proj{\X} \ev_n}^2</span>
<span id="cb24-417"><a href="#cb24-417" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>={}&amp; \ev_n^\trans \proj{\X} \proj{\X} \ev_n</span>
<span id="cb24-418"><a href="#cb24-418" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>={}&amp; \sum_{m=1}^N \left(\proj{\X}\right)_{nm}^2</span>
<span id="cb24-419"><a href="#cb24-419" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>={}&amp; \sum_{m \ne n} \left(\proj{\X}\right)_{nm}^2 + h_n^2,</span>
<span id="cb24-420"><a href="#cb24-420" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb24-421"><a href="#cb24-421" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-422"><a href="#cb24-422" aria-hidden="true" tabindex="-1"></a>so that, when $h_n \approx 1$,</span>
<span id="cb24-423"><a href="#cb24-423" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-424"><a href="#cb24-424" aria-hidden="true" tabindex="-1"></a>\sum_{m \ne n} \left(\proj{\X}\right)_{nm}^2 \le 1 - h_n^2 \approx 0,</span>
<span id="cb24-425"><a href="#cb24-425" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-426"><a href="#cb24-426" aria-hidden="true" tabindex="-1"></a>and so</span>
<span id="cb24-427"><a href="#cb24-427" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-428"><a href="#cb24-428" aria-hidden="true" tabindex="-1"></a>\yhat_n \approx h_n \y_n.</span>
<span id="cb24-429"><a href="#cb24-429" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-430"><a href="#cb24-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-431"><a href="#cb24-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-432"><a href="#cb24-432" aria-hidden="true" tabindex="-1"></a><span class="fu">## Kleiber's whale revisited</span></span>
<span id="cb24-433"><a href="#cb24-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-434"><a href="#cb24-434" aria-hidden="true" tabindex="-1"></a>Recall Kleiber's dataset of animal sizes and metabolisms.  Let's look at the</span>
<span id="cb24-435"><a href="#cb24-435" aria-hidden="true" tabindex="-1"></a>original linear regression with and without the whale:</span>
<span id="cb24-436"><a href="#cb24-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-439"><a href="#cb24-439" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-440"><a href="#cb24-440" aria-hidden="true" tabindex="-1"></a><span class="co"># Look at the data, find outliers</span></span>
<span id="cb24-441"><a href="#cb24-441" aria-hidden="true" tabindex="-1"></a>lm_kl <span class="ot">&lt;-</span> <span class="fu">lm</span>(Metabol_kcal_per_day <span class="sc">~</span> Weight_kg <span class="sc">+</span> <span class="dv">1</span>, kleiber_df)</span>
<span id="cb24-442"><a href="#cb24-442" aria-hidden="true" tabindex="-1"></a>lm_drop_kl <span class="ot">&lt;-</span> <span class="fu">lm</span>(Metabol_kcal_per_day <span class="sc">~</span> Weight_kg <span class="sc">+</span> <span class="dv">1</span>, kleiber_df <span class="sc">%&gt;%</span> <span class="fu">filter</span>(Animal <span class="sc">!=</span> <span class="st">"Whale"</span>))</span>
<span id="cb24-443"><a href="#cb24-443" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-444"><a href="#cb24-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-447"><a href="#cb24-447" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-448"><a href="#cb24-448" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-449"><a href="#cb24-449" aria-hidden="true" tabindex="-1"></a>kleiber_df <span class="ot">&lt;-</span> kleiber_df <span class="sc">%&gt;%</span></span>
<span id="cb24-450"><a href="#cb24-450" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">res=</span>lm_kl<span class="sc">$</span>residuals) <span class="sc">%&gt;%</span></span>
<span id="cb24-451"><a href="#cb24-451" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">yhat=</span>lm_kl<span class="sc">$</span>fitted.values) <span class="sc">%&gt;%</span></span>
<span id="cb24-452"><a href="#cb24-452" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">yhat_drop=</span><span class="fu">predict</span>(lm_drop_kl, kleiber_df)) <span class="sc">%&gt;%</span></span>
<span id="cb24-453"><a href="#cb24-453" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">lev=</span><span class="fu">hatvalues</span>(lm_kl))</span>
<span id="cb24-454"><a href="#cb24-454" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb24-455"><a href="#cb24-455" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(kleiber_df) <span class="sc">+</span></span>
<span id="cb24-456"><a href="#cb24-456" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>Weight_kg, <span class="at">y=</span>Metabol_kcal_per_day, <span class="at">color=</span>Animal, <span class="at">size=</span><span class="dv">2</span>)) <span class="sc">+</span></span>
<span id="cb24-457"><a href="#cb24-457" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>Weight_kg, <span class="at">y=</span>yhat, <span class="at">linetype=</span><span class="st">"With whale"</span>)) <span class="sc">+</span></span>
<span id="cb24-458"><a href="#cb24-458" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>Weight_kg, <span class="at">y=</span>yhat_drop, <span class="at">linetype=</span><span class="st">"Without whale"</span>)) <span class="sc">+</span></span>
<span id="cb24-459"><a href="#cb24-459" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="fu">max</span>(kleiber_df<span class="sc">$</span>Metabol_kcal_per_day) <span class="sc">*</span> <span class="fl">1.05</span>)</span>
<span id="cb24-460"><a href="#cb24-460" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-461"><a href="#cb24-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-462"><a href="#cb24-462" aria-hidden="true" tabindex="-1"></a>In this case, the fact that the whale has high leverage was obvious.  But</span>
<span id="cb24-463"><a href="#cb24-463" aria-hidden="true" tabindex="-1"></a>we can also see this from the leverage scores:</span>
<span id="cb24-464"><a href="#cb24-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-467"><a href="#cb24-467" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-468"><a href="#cb24-468" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-469"><a href="#cb24-469" aria-hidden="true" tabindex="-1"></a>kleiber_df <span class="sc">%&gt;%</span></span>
<span id="cb24-470"><a href="#cb24-470" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(lev) <span class="sc">%&gt;%</span></span>
<span id="cb24-471"><a href="#cb24-471" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb24-472"><a href="#cb24-472" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>Animal, <span class="at">y=</span>lev)) <span class="sc">+</span></span>
<span id="cb24-473"><a href="#cb24-473" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"Leverage score"</span>) <span class="sc">+</span> </span>
<span id="cb24-474"><a href="#cb24-474" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"Animal (sorted in leverage score order)"</span>) <span class="sc">+</span></span>
<span id="cb24-475"><a href="#cb24-475" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">90</span>, <span class="at">vjust =</span> <span class="fl">0.5</span>, <span class="at">hjust=</span><span class="dv">1</span>))</span>
<span id="cb24-476"><a href="#cb24-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-477"><a href="#cb24-477" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-478"><a href="#cb24-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-479"><a href="#cb24-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-480"><a href="#cb24-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-481"><a href="#cb24-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-482"><a href="#cb24-482" aria-hidden="true" tabindex="-1"></a><span class="fu"># Data dropping</span></span>
<span id="cb24-483"><a href="#cb24-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-484"><a href="#cb24-484" aria-hidden="true" tabindex="-1"></a>Above, we've shown how extreme values of $\xv_n$ and $\y_n$ can affect a</span>
<span id="cb24-485"><a href="#cb24-485" aria-hidden="true" tabindex="-1"></a>linear regression.  These two concepts can be combined by considering the</span>
<span id="cb24-486"><a href="#cb24-486" aria-hidden="true" tabindex="-1"></a>effect of "data dropping."  Specifically, we might ask how a regression</span>
<span id="cb24-487"><a href="#cb24-487" aria-hidden="true" tabindex="-1"></a>would be different had we excluded a particular datapoint.  This is a</span>
<span id="cb24-488"><a href="#cb24-488" aria-hidden="true" tabindex="-1"></a>nice concept because it measures how "important" an entire datapoint</span>
<span id="cb24-489"><a href="#cb24-489" aria-hidden="true" tabindex="-1"></a>is, rather than separately considering the response and regressor. </span>
<span id="cb24-490"><a href="#cb24-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-491"><a href="#cb24-491" aria-hidden="true" tabindex="-1"></a>Let $\betavhat_{-n}$ denote the estimate of $\betahat$ with the datapoint $n$</span>
<span id="cb24-492"><a href="#cb24-492" aria-hidden="true" tabindex="-1"></a>left out.  Recall that we wrote </span>
<span id="cb24-493"><a href="#cb24-493" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-494"><a href="#cb24-494" aria-hidden="true" tabindex="-1"></a>\betavhat =\sumn \omegav_n \y_n</span>
<span id="cb24-495"><a href="#cb24-495" aria-hidden="true" tabindex="-1"></a>\quad\quad\textrm{where }\omegav_n := (\X^\trans \X)^{-1} \xv_n.</span>
<span id="cb24-496"><a href="#cb24-496" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-497"><a href="#cb24-497" aria-hidden="true" tabindex="-1"></a>In your homework, you'll show that, in linear regression, you can </span>
<span id="cb24-498"><a href="#cb24-498" aria-hidden="true" tabindex="-1"></a>write down the exact formula:</span>
<span id="cb24-499"><a href="#cb24-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-500"><a href="#cb24-500" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-501"><a href="#cb24-501" aria-hidden="true" tabindex="-1"></a>\betavhat_{-n} - \betavhat=   -\frac{\reshat_n}{1 - h_n} \omegav_n</span>
<span id="cb24-502"><a href="#cb24-502" aria-hidden="true" tabindex="-1"></a>\approx -\omegav_n \reshat_n \quad\textrm{(when $h_n$ is small)}</span>
<span id="cb24-503"><a href="#cb24-503" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-504"><a href="#cb24-504" aria-hidden="true" tabindex="-1"></a>The term $\omegav_n$ is very close to a leverage score ---</span>
<span id="cb24-505"><a href="#cb24-505" aria-hidden="true" tabindex="-1"></a>in fact, $h_n = \xv_n^\trans \omegav_n$. </span>
<span id="cb24-506"><a href="#cb24-506" aria-hidden="true" tabindex="-1"></a>This formula says that a regression coefficient will have a large change</span>
<span id="cb24-507"><a href="#cb24-507" aria-hidden="true" tabindex="-1"></a>when both $\reshat_n$ and $\omega_n$ are large.</span>
<span id="cb24-508"><a href="#cb24-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-509"><a href="#cb24-509" aria-hidden="true" tabindex="-1"></a>The negative of the approximation for small $h_n$ is known as the "empirical influence function"</span>
<span id="cb24-510"><a href="#cb24-510" aria-hidden="true" tabindex="-1"></a>of the $n$--th datapoint (sometimes just "influence function"):</span>
<span id="cb24-511"><a href="#cb24-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-512"><a href="#cb24-512" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-513"><a href="#cb24-513" aria-hidden="true" tabindex="-1"></a>\textrm{Influence function of datapoint }n := \omegav_n \reshat_n.</span>
<span id="cb24-514"><a href="#cb24-514" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-515"><a href="#cb24-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-516"><a href="#cb24-516" aria-hidden="true" tabindex="-1"></a>For linear models, it can be computed using the <span class="in">`influence`</span> <span class="in">`R`</span> function:</span>
<span id="cb24-517"><a href="#cb24-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-518"><a href="#cb24-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-521"><a href="#cb24-521" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-522"><a href="#cb24-522" aria-hidden="true" tabindex="-1"></a>reg_all <span class="ot">&lt;-</span> <span class="fu">lm</span>(lm_form, births_df)</span>
<span id="cb24-523"><a href="#cb24-523" aria-hidden="true" tabindex="-1"></a>infl <span class="ot">&lt;-</span> <span class="fu">influence</span>(reg_all)</span>
<span id="cb24-524"><a href="#cb24-524" aria-hidden="true" tabindex="-1"></a>births_df <span class="ot">&lt;-</span> </span>
<span id="cb24-525"><a href="#cb24-525" aria-hidden="true" tabindex="-1"></a>  births_df <span class="sc">%&gt;%</span></span>
<span id="cb24-526"><a href="#cb24-526" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">fage_infl=</span>infl<span class="sc">$</span>coefficients[, <span class="st">"fage"</span>]) <span class="sc">%&gt;%</span></span>
<span id="cb24-527"><a href="#cb24-527" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">yhat=</span>reg_all<span class="sc">$</span>fitted)</span>
<span id="cb24-528"><a href="#cb24-528" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-529"><a href="#cb24-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-530"><a href="#cb24-530" aria-hidden="true" tabindex="-1"></a>We can look at the graph and color the points by their influence function. Clearly</span>
<span id="cb24-531"><a href="#cb24-531" aria-hidden="true" tabindex="-1"></a>we can see that the large <span class="in">`fage`</span> points have high influence.</span>
<span id="cb24-534"><a href="#cb24-534" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb24-535"><a href="#cb24-535" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb24-536"><a href="#cb24-536" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(births_df) <span class="sc">+</span></span>
<span id="cb24-537"><a href="#cb24-537" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>fage, <span class="at">y=</span>weight, <span class="at">color=</span>fage_infl), <span class="at">size=</span><span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb24-538"><a href="#cb24-538" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>fage, <span class="at">y=</span>yhat)) <span class="sc">+</span></span>
<span id="cb24-539"><a href="#cb24-539" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_gradient2</span>(<span class="at">low=</span><span class="st">"red"</span>, <span class="at">high=</span><span class="st">"blue"</span>)</span>
<span id="cb24-540"><a href="#cb24-540" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-541"><a href="#cb24-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-542"><a href="#cb24-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-543"><a href="#cb24-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-544"><a href="#cb24-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-545"><a href="#cb24-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-546"><a href="#cb24-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-547"><a href="#cb24-547" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bonus content: Data dropping (generalization to nonlinear estimators)</span></span>
<span id="cb24-548"><a href="#cb24-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-549"><a href="#cb24-549" aria-hidden="true" tabindex="-1"></a>The above results rely heavily on the special structure of linear regression:</span>
<span id="cb24-550"><a href="#cb24-550" aria-hidden="true" tabindex="-1"></a>e.g. the fact that $\betahat$ is a linear combination of $\Y$, and</span>
<span id="cb24-551"><a href="#cb24-551" aria-hidden="true" tabindex="-1"></a>that $\Yhat$ is a projection of $\Y$.  In more general settings such</span>
<span id="cb24-552"><a href="#cb24-552" aria-hidden="true" tabindex="-1"></a>results are not available.  In order to motivate the use of such </span>
<span id="cb24-553"><a href="#cb24-553" aria-hidden="true" tabindex="-1"></a>diagnostics in more general settings (not covered in this class),</span>
<span id="cb24-554"><a href="#cb24-554" aria-hidden="true" tabindex="-1"></a>let me introduce a slightly different approach based on derivatives.</span>
<span id="cb24-555"><a href="#cb24-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-556"><a href="#cb24-556" aria-hidden="true" tabindex="-1"></a>Suppose we assign each datapoint a weight, $\w_n$, and write</span>
<span id="cb24-557"><a href="#cb24-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-558"><a href="#cb24-558" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-559"><a href="#cb24-559" aria-hidden="true" tabindex="-1"></a>\betavhat(\w) = \argmin{\beta} \sumn \w_n (\y_n - \xv_n^\trans \betav)^2.</span>
<span id="cb24-560"><a href="#cb24-560" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-561"><a href="#cb24-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-562"><a href="#cb24-562" aria-hidden="true" tabindex="-1"></a>I have written $\betahat(\wv)$ because the optimal solution depends on the</span>
<span id="cb24-563"><a href="#cb24-563" aria-hidden="true" tabindex="-1"></a>vector of weights, $\wv = (\w_1, \ldots, \w_N)^\trans$.  When $\wv = \onev$,</span>
<span id="cb24-564"><a href="#cb24-564" aria-hidden="true" tabindex="-1"></a>we recover the original problem.  When we set one of the entries to</span>
<span id="cb24-565"><a href="#cb24-565" aria-hidden="true" tabindex="-1"></a>zero, we remove that datapoint from the problem.  Using this, we can</span>
<span id="cb24-566"><a href="#cb24-566" aria-hidden="true" tabindex="-1"></a>approximate the effect of removing a datapoint using the first-order</span>
<span id="cb24-567"><a href="#cb24-567" aria-hidden="true" tabindex="-1"></a>Taylor series expansion:</span>
<span id="cb24-568"><a href="#cb24-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-569"><a href="#cb24-569" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-570"><a href="#cb24-570" aria-hidden="true" tabindex="-1"></a>\betahat_{-n} \approx </span>
<span id="cb24-571"><a href="#cb24-571" aria-hidden="true" tabindex="-1"></a>\betahat_{-n}^{linear} = \betahat + \frac{\partial \betahat(\wv)}{\partial \w_n}\vert_{\w_n=1} (0 - 1).</span>
<span id="cb24-572"><a href="#cb24-572" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-573"><a href="#cb24-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-574"><a href="#cb24-574" aria-hidden="true" tabindex="-1"></a>One can show that</span>
<span id="cb24-575"><a href="#cb24-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-576"><a href="#cb24-576" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-577"><a href="#cb24-577" aria-hidden="true" tabindex="-1"></a>\frac{\partial \betahat(\wv)}{\partial \w_n}\vert_{\w_n=1} = (\X^\trans \X)^{-1} \xv_n \reshat_n.</span>
<span id="cb24-578"><a href="#cb24-578" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-579"><a href="#cb24-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-580"><a href="#cb24-580" aria-hidden="true" tabindex="-1"></a>Note that the Taylor series is a good approximation to the exact formula (given </span>
<span id="cb24-581"><a href="#cb24-581" aria-hidden="true" tabindex="-1"></a>in the homework) when $h_n \ll 1$, which is expected when $h_n$ goes to zero</span>
<span id="cb24-582"><a href="#cb24-582" aria-hidden="true" tabindex="-1"></a>at rate $1/N$:</span>
<span id="cb24-583"><a href="#cb24-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-584"><a href="#cb24-584" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-585"><a href="#cb24-585" aria-hidden="true" tabindex="-1"></a>\betahat_{-n}  = \betahat - (\X^\trans \X)^{-1} \xv_n \frac{\reshat_n}{1 - h_n} \approx</span>
<span id="cb24-586"><a href="#cb24-586" aria-hidden="true" tabindex="-1"></a>\betahat - (\X^\trans \X)^{-1} \xv_n \reshat = \betahat_{-n}^{linear}.</span>
<span id="cb24-587"><a href="#cb24-587" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-588"><a href="#cb24-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-589"><a href="#cb24-589" aria-hidden="true" tabindex="-1"></a>In more complicated nonlinear problems, the exact formula is unavailable, but</span>
<span id="cb24-590"><a href="#cb24-590" aria-hidden="true" tabindex="-1"></a>the linear approximation is typically computed.</span>
<span id="cb24-591"><a href="#cb24-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-592"><a href="#cb24-592" aria-hidden="true" tabindex="-1"></a>From this (or the exact formula), we can see that the effect of</span>
<span id="cb24-593"><a href="#cb24-593" aria-hidden="true" tabindex="-1"></a>extreme values on $\betahat$ is actually a product of both</span>
<span id="cb24-594"><a href="#cb24-594" aria-hidden="true" tabindex="-1"></a>$\reshat_n$ and $\xv_n$.  Large residuals will not have an</span>
<span id="cb24-595"><a href="#cb24-595" aria-hidden="true" tabindex="-1"></a>effect when $\xv_n = \zerov$, and outlier $\xv_n$ will not</span>
<span id="cb24-596"><a href="#cb24-596" aria-hidden="true" tabindex="-1"></a>have an effect when $\reshat_n = 0$. </span>
<span id="cb24-597"><a href="#cb24-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-598"><a href="#cb24-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-599"><a href="#cb24-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-600"><a href="#cb24-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-601"><a href="#cb24-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-602"><a href="#cb24-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-603"><a href="#cb24-603" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bonus content: Rank-one updates for linear regression (Woodbury formula)</span></span>
<span id="cb24-604"><a href="#cb24-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-605"><a href="#cb24-605" aria-hidden="true" tabindex="-1"></a>The proof of the data--dropping formula for linear regression uses the Woodbury </span>
<span id="cb24-606"><a href="#cb24-606" aria-hidden="true" tabindex="-1"></a>formula, for which I now give a proof for completness.  Let $U$ and $V$ be $N \times K$ matrices,</span>
<span id="cb24-607"><a href="#cb24-607" aria-hidden="true" tabindex="-1"></a>and $\id_N$ and $\id_K$ the $N\times N$ and $K \times K$ identity matrices, respectively.</span>
<span id="cb24-608"><a href="#cb24-608" aria-hidden="true" tabindex="-1"></a>Using the fact that</span>
<span id="cb24-609"><a href="#cb24-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-610"><a href="#cb24-610" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-611"><a href="#cb24-611" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb24-612"><a href="#cb24-612" aria-hidden="true" tabindex="-1"></a>(\id_N + U V^\trans)^{-1} (\id_N + U V^\trans) = \id_N</span>
<span id="cb24-613"><a href="#cb24-613" aria-hidden="true" tabindex="-1"></a>\quad\Rightarrow\quad&amp;</span>
<span id="cb24-614"><a href="#cb24-614" aria-hidden="true" tabindex="-1"></a>(\id_N + U V^\trans)^{-1} = \id_N - (\id_N + U V^\trans)^{-1} U V^\trans &amp; \textrm{(i)}<span class="sc">\\</span></span>
<span id="cb24-615"><a href="#cb24-615" aria-hidden="true" tabindex="-1"></a>(\id_N + U V^\trans) U = U (\id_K +  V^\trans U)  </span>
<span id="cb24-616"><a href="#cb24-616" aria-hidden="true" tabindex="-1"></a>\quad\Rightarrow\quad&amp;</span>
<span id="cb24-617"><a href="#cb24-617" aria-hidden="true" tabindex="-1"></a>U (\id_N + V^\trans U )^{-1} = (\id_K + U^\trans V)^{-1} U  &amp; \textrm{(ii)} </span>
<span id="cb24-618"><a href="#cb24-618" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb24-619"><a href="#cb24-619" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-620"><a href="#cb24-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-621"><a href="#cb24-621" aria-hidden="true" tabindex="-1"></a>we have that</span>
<span id="cb24-622"><a href="#cb24-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-623"><a href="#cb24-623" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-624"><a href="#cb24-624" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb24-625"><a href="#cb24-625" aria-hidden="true" tabindex="-1"></a>(\id_N + U V^\trans)^{-1} ={}&amp; \id_N - (\id_N + U V^\trans)^{-1} U V^\trans  &amp; \textrm{by (i)} <span class="sc">\\</span></span>
<span id="cb24-626"><a href="#cb24-626" aria-hidden="true" tabindex="-1"></a> ={}&amp; \id_N - U  (\id_K + V^\trans U )^{-1} V^\trans  &amp; \textrm{by (ii)}.</span>
<span id="cb24-627"><a href="#cb24-627" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb24-628"><a href="#cb24-628" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-629"><a href="#cb24-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-630"><a href="#cb24-630" aria-hidden="true" tabindex="-1"></a>We can use this to prove the Woodbury formula as a special case when $K=1$.</span>
<span id="cb24-631"><a href="#cb24-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-632"><a href="#cb24-632" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-633"><a href="#cb24-633" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb24-634"><a href="#cb24-634" aria-hidden="true" tabindex="-1"></a>(A + \uv \vv^\trans)^{-1} ={}&amp;</span>
<span id="cb24-635"><a href="#cb24-635" aria-hidden="true" tabindex="-1"></a>(A (\id_N  + A^{-1} \uv \vv^\trans))^{-1} </span>
<span id="cb24-636"><a href="#cb24-636" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>={}&amp; </span>
<span id="cb24-637"><a href="#cb24-637" aria-hidden="true" tabindex="-1"></a>(\id_N + (A^{-1} \uv) \vv^\trans )^{-1} A^{-1}</span>
<span id="cb24-638"><a href="#cb24-638" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>={}&amp; </span>
<span id="cb24-639"><a href="#cb24-639" aria-hidden="true" tabindex="-1"></a>(\id_N - A^{-1} \uv (1 + \vv^\trans A^{-1} \uv)^{-1} \vv^\trans ) A^{-1}</span>
<span id="cb24-640"><a href="#cb24-640" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>={}&amp; </span>
<span id="cb24-641"><a href="#cb24-641" aria-hidden="true" tabindex="-1"></a>A^{-1} - \frac{A^{-1} \uv \vv A^{-1}}{1 + \vv^\trans A^{-1} \uv}.</span>
<span id="cb24-642"><a href="#cb24-642" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb24-643"><a href="#cb24-643" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb24-644"><a href="#cb24-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-645"><a href="#cb24-645" aria-hidden="true" tabindex="-1"></a>This formula has importance for linear models far beyond the leave--one--out formula.  For</span>
<span id="cb24-646"><a href="#cb24-646" aria-hidden="true" tabindex="-1"></a>instance, the "kernel trick" in machine learning can be understood as an application</span>
<span id="cb24-647"><a href="#cb24-647" aria-hidden="true" tabindex="-1"></a>of the Woodbury formula.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>