[
  {
    "objectID": "lectures/grades_graph.html",
    "href": "lectures/grades_graph.html",
    "title": "CLT and prediction examples",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gridExtra)\nlibrary(plotly)\noptions(repr.plot.width=15, repr.plot.height=8)\n\n\nscores &lt;- read.csv(\"final_exams_spring24.csv\")$Total\nn_obs &lt;- length(scores)\nybar &lt;- mean(scores)\n\nhist(scores) \n\n\n\n\n\n\n\n\n\nbandwidth &lt;- 3\neval_score_dens &lt;- function(x) {\n  return(mean(dnorm(x- scores, sd=bandwidth)))\n}\n\nscore_grid &lt;- seq(0, 40, length.out=1000)\nscore_dens &lt;- sapply(score_grid, eval_score_dens)\nscore_dens &lt;- score_dens / sum(score_dens)\nscore_df &lt;- cumsum(score_dens) / sum(score_dens)\n\n# Look at my (made-up) density\nggplot() +\n    geom_line(aes(x=score_grid, y=score_dens)) + \n    xlab(\"Test score\") + ylab(\"(made up) Density\")\n\n\n\n\n\n\n\n\n\nscore_dist_df &lt;- data.frame(score=score_grid, score_dens=score_dens * 5 / max(score_dens))\n\neval_score_df_inv &lt;- approxfun(x=c(0, score_df), y=c(0, score_grid))\n\ndraw_scores &lt;- function(n_obs) {\n  u &lt;- runif(n_obs)\n  draws &lt;- round(eval_score_df_inv(u))\n  return(draws)\n}\n\n\n# The original data with my inferred density\nggplot() +\n  geom_histogram(aes(x=scores), bins=30) +\n  geom_line(aes(x=score, y=score_dens), color=\"blue\", data=score_dist_df, lwd=2) +\n  geom_vline(aes(xintercept=mean(scores)), color=\"red\") +\n    xlab(\"Test score\") + ylab(\"(made up) Density\")\n\n\n\n\n\n\n\n\n\n# Draw score distributions\n#scores_df &lt;- data.frame(draw_ind=0, scores=scores)\nscores_df &lt;- data.frame()\nfor (num_draws in c(10, 40, 100, 500)) {\n  for (draw_ind in 1:100) {\n    scores_df &lt;- bind_rows(\n      scores_df,\n      data.frame(\n        draw_ind=draw_ind, \n        num_draws=num_draws, \n        scores=draw_scores(num_draws)))\n  }\n}\n\nscores_df &lt;-\n  scores_df %&gt;%\n  group_by(num_draws, draw_ind) %&gt;%\n  mutate(ybar=mean(scores))\n\nscore_means &lt;- \n  scores_df %&gt;%\n  group_by(draw_ind, num_draws) %&gt;%\n  summarize(ybar=mean(scores))\n\n# Showing the fact that different draws lead to different histrograms and different means\ninner_join(scores_df, score_means, by=c(\"draw_ind\", \"num_draws\")) %&gt;%\n  filter(draw_ind &lt; 5, num_draws == 40) %&gt;%\n  ggplot() +\n    geom_histogram(aes(x=scores)) +\n    geom_vline(aes(xintercept=ybar), color=\"red\") +\n    facet_grid(~ draw_ind)\n\n\n`summarise()` has grouped output by 'draw_ind'. You can override using the `.groups` argument.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nThe CLT and LLN\n\n# The distribution of the score means for different numbers of draws\nscore_means %&gt;%\n  ggplot() +\n    geom_vline(aes(xintercept=ybar), color=\"red\", alpha=0.2) +\n    geom_histogram(aes(x=scores), alpha=0.2, data=data.frame(scores=scores)) +\n    geom_density(aes(x=ybar), color=\"red\", lwd=2) +\n    facet_grid(num_draws ~ .) +\n    xlim(0, 40)\n  \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nWarning message:\n“Removed 8 rows containing missing values (`geom_bar()`).”\n\n\n\n\n\n\n\n\n\n\n# The same plot with everything on the same panel\nggplot() + \n  geom_histogram(aes(x=scores), alpha=0.4) +\n  geom_line(aes(x=score_grid, y=5 * score_dens / max(score_dens), color=\"1\"), lwd=2) +\n  geom_density(aes(x=ybar, y=..density.. * 5 / max(..density..), color=ordered(num_draws), group=num_draws), \n               data=score_means,\n               lwd=2) +\n  labs(color=\"Number of draws\") +\n  xlim(25, 30)\n\n#ggplotly(p)\n\nWarning message:\n“The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.”\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nWarning message:\n“Removed 44 rows containing non-finite values (`stat_bin()`).”\nWarning message:\n“Removed 50 rows containing non-finite values (`stat_density()`).”\nWarning message:\n“Removed 2 rows containing missing values (`geom_bar()`).”\nWarning message:\n“Removed 875 rows containing missing values (`geom_line()`).”\n\n\n\n\n\n\n\n\n\n\n\nPrediction\n\nmean_true &lt;- sum(score_dens * score_grid) / sum(score_dens)\nmean_true\n\neval_true_prediction_error &lt;- function(score_guess) {\n    return(sum(score_dens * (score_grid - score_guess)^2) / sum(score_dens))\n}\n\ntrue_prediction_error &lt;- sapply(score_grid, eval_true_prediction_error)\n\nplt &lt;-     \n    ggplot() +\n        geom_line(aes(x=score_grid, y=true_prediction_error)) +\n        geom_vline(aes(xintercept=mean_true, color=\"True mean\")) +\n        geom_vline(aes(xintercept=ybar, color=\"Sample mean\")) +\n        xlab(\"Score\") + ylab(\"True error\") + expand_limits(y=0)\ngrid.arrange(plt + ggtitle(\"True loss\"), \n             plt + xlim(25,35) + ylim(0, 250) + ggtitle(\"(Zoomed in)\"), ncol=2)\n\n28.0258687403645\n\n\nWarning message:\n“Removed 750 rows containing missing values (`geom_line()`).”\n\n\n\n\n\n\n\n\n\n\n\neval_sample_prediction_error &lt;- function(score_guess, obs_scores) {\n    return(mean((obs_scores - score_guess)^2))\n}\nsample_prediction_error &lt;- sapply(score_grid, \\(s) eval_sample_prediction_error(s, scores))\n\nplt &lt;-     \n    ggplot() +\n        geom_line(aes(x=score_grid, y=sample_prediction_error)) +\n        geom_vline(aes(xintercept=mean_true, color=\"True mean\")) +\n        geom_vline(aes(xintercept=ybar, color=\"Sample mean\")) +\n        xlab(\"Score\") + ylab(\"True error\") + expand_limits(y=0)\ngrid.arrange(plt + ggtitle(\"Empirical loss\"), \n             plt + xlim(25,35) + ylim(0, 250) + ggtitle(\"(zoomed in)\"), ncol=2)\n\nWarning message:\n“Removed 750 rows containing missing values (`geom_line()`).”\n\n\n\n\n\n\n\n\n\n\npred_df &lt;- data.frame()\nfor (draw_ind in 1:10) {\n    scores_draw &lt;- draw_scores(length(scores))\n    sample_prediction_error &lt;- sapply(\n        score_grid, \\(s) eval_sample_prediction_error(s, scores_draw))\n\n    pred_df &lt;- bind_rows(\n        pred_df,\n        data.frame(\n            score=score_grid,\n            err=sample_prediction_error,\n            sample_mean=mean(scores_draw),\n            draw_ind=draw_ind))\n}\n\n\nggplot() +\n        geom_line(aes(x=score, y=err, group=draw_ind, color=\"Sample\"), data=pred_df) +\n        xlab(\"Score\") + ylab(\"True error\") + expand_limits(y=0) +\n        geom_vline(aes(xintercept=mean_true, color=\"True\"), lwd=3) +\n        geom_vline(aes(xintercept=sample_mean, color=\"Sample\"), data=pred_df) +\n        geom_line(aes(x=score_grid, y=true_prediction_error, color=\"True\"), lwd=3) +\n    ggtitle(\"Draws of empirical loss (compared to the true loss)\")"
  },
  {
    "objectID": "lectures/2_AmesHousingInference.html",
    "href": "lectures/2_AmesHousingInference.html",
    "title": "Multilinear regression with the Ames Housing data",
    "section": "",
    "text": "We will take a look at the Ames housing data from Veridical Data Science by Yu and Barter. It is also available on the course website in the datasets page. (The original data is on github.)\nThese are a sample of homes sold in Ames Iowas between 2006 and 2010 as recorded by the city assessor’s office. See the discussion in Chapter 8.4 of VDS..\nWe will be looking at this data to ask the question: how much might I increase my home’s value by renovating the kitchen?\nThe dataset lists sale price, as well as different “kitchen quality” measures:\nKitchenQual (Ordinal): Kitchen quality\n       Ex       Excellent\n       Gd       Good\n       TA       Typical/Average\n       Fa       Fair\n       Po       Poor\n\ngit_repo_dir &lt;- \"/home/rgiordan/Documents/git_repos/stat151a\"\nhousing_dir &lt;- file.path(git_repo_dir, \"datasets/ames_house/data\")\names_orig &lt;- read.table(file.path(housing_dir, \"AmesHousing.txt\"), sep=\"\\t\", header=T)\n\nWe will follow some of the cleaning suggestions in the VDS book.\nWe’ll also look only at “good” and “excellent” kitchens, on the assumption that we are upgrading our kitchen from its present “good” condtion to “excellent.”\n\names &lt;- ames_orig %&gt;%\n  filter(Sale.Condition == \"Normal\",\n         # remove agricultural, commercial and industrial\n         !(MS.Zoning %in% c(\"A (agr)\", \"C (all)\", \"I (all)\"))) %&gt;%\n  filter(Kitchen.Qual %in% c(\"Gd\", \"Ex\")) %&gt;%\n  mutate(Overall.Qual=factor(Overall.Qual))\n\n\nggplot(ames) + \ngeom_histogram(aes(x=SalePrice), bins=50)"
  },
  {
    "objectID": "lectures/2_AmesHousingInference.html#viewing-the-difference-of-means-as-a-regression",
    "href": "lectures/2_AmesHousingInference.html#viewing-the-difference-of-means-as-a-regression",
    "title": "Multilinear regression with the Ames Housing data",
    "section": "Viewing the difference of means as a regression",
    "text": "Viewing the difference of means as a regression\nNote that we can compute the same thing with a regression.\n\nreg &lt;- lm(SalePrice ~ Kitchen.Qual - 1, ames)\nprint(summary(reg))\n\n\nCall:\nlm(formula = SalePrice ~ Kitchen.Qual - 1, data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-213307  -44953   -7878   35297  427993 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \nKitchen.QualEx   327006       6305   51.87   &lt;2e-16 ***\nKitchen.QualGd   207828       2215   93.83   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 67910 on 1054 degrees of freedom\nMultiple R-squared:  0.916, Adjusted R-squared:  0.9158 \nF-statistic:  5747 on 2 and 1054 DF,  p-value: &lt; 2.2e-16\n\n\n\nWhat is going on? When we run the regression, R is converting Kitchen.Qual into a binary matrix.\n\nx &lt;- model.matrix(SalePrice ~ Kitchen.Qual - 1, ames)\n\nbind_cols(\n    select(ames, Kitchen.Qual),\n    x) %&gt;% \nhead(, 10)\n\n\nA data.frame: 6 × 3\n\n\n\nKitchen.Qual\nKitchen.QualEx\nKitchen.QualGd\n\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\nGd\n0\n1\n\n\n2\nEx\n1\n0\n\n\n3\nGd\n0\n1\n\n\n4\nGd\n0\n1\n\n\n5\nGd\n0\n1\n\n\n6\nGd\n0\n1\n\n\n\n\n\nWe then find the coefficients \\(\\beta_{ex}\\) and \\(\\beta_{gd}\\) that minimizes\n\\[\n\\sum_{n=1}^N (y_n - (\\beta_{ex} \\textrm{Kitchen.QualEx} + \\beta_{gd} \\textrm{Kitchen.QualGd}))^2\n\\]\nwhere \\(y_n =\\)SalePrice.\nExercise: prove that the optimal values, \\(\\hat\\beta_{ex}\\) and \\(\\hat\\beta_{gd}\\), are just the sample means within their respective categories, e.g.\n\\[\n\\hat\\beta_{ex} = \\frac{1}{N_{ex}} \\sum_{n: \\textrm{Kitchen.Qual} = \\textrm{Ex}} y_n\n\\]\nThere are a couple ways to get the difference out of R.\n\npredict(reg, data.frame(Kitchen.Qual=\"Ex\")) - predict(reg, data.frame(Kitchen.Qual=\"Gd\"))\n\n1: 119178.513206163\n\n\n\nbetahat &lt;- coefficients(reg)\ndiff &lt;- betahat[\"Kitchen.QualEx\"] - betahat[\"Kitchen.QualGd\"]\nnames(diff) &lt;- NULL\nprint(diff)\n\n[1] 119178.5\n\n\nHere’s one you might not have thought of:\n\nreg2 &lt;- lm(SalePrice ~ Kitchen.Qual + 1, ames)\nprint(summary(reg2))\n\n\nCall:\nlm(formula = SalePrice ~ Kitchen.Qual + 1, data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-213307  -44953   -7878   35297  427993 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      327006       6305   51.87   &lt;2e-16 ***\nKitchen.QualGd  -119178       6683  -17.83   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 67910 on 1054 degrees of freedom\nMultiple R-squared:  0.2318,    Adjusted R-squared:  0.2311 \nF-statistic:   318 on 1 and 1054 DF,  p-value: &lt; 2.2e-16\n\n\n\nHere, the model includes a constant, which is always 1. Then it includes an encoding vector only for Kitchen.QualGd.\nExercise: Why doesn’t it include one for Kitchen.QualEx?\n\nx &lt;- model.matrix(SalePrice ~ Kitchen.Qual + 1, ames)\n\nbind_cols(\n    select(ames, Kitchen.Qual),\n    x) %&gt;% \nhead(, 10)\n\n\nA data.frame: 6 × 3\n\n\n\nKitchen.Qual\n(Intercept)\nKitchen.QualGd\n\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\nGd\n1\n1\n\n\n2\nEx\n1\n0\n\n\n3\nGd\n1\n1\n\n\n4\nGd\n1\n1\n\n\n5\nGd\n1\n1\n\n\n6\nGd\n1\n1\n\n\n\n\n\n\npredict(reg2, data.frame(Kitchen.Qual=\"Ex\")) - predict(reg2, data.frame(Kitchen.Qual=\"Gd\"))\n\n1: 119178.513206164\n\n\nOne way to interpret this is “what is the effect on sale price of having a good kitchen when controlling for the average home price?”"
  },
  {
    "objectID": "lectures/2_AmesHousingInference.html#controlling-for-house-quality",
    "href": "lectures/2_AmesHousingInference.html#controlling-for-house-quality",
    "title": "Multilinear regression with the Ames Housing data",
    "section": "Controlling for house quality",
    "text": "Controlling for house quality\nWe also have a measure of overall house quality:\nOverall Qual (Ordinal): Rates the overall material and finish of the house\n\n       10       Very Excellent\n       9        Excellent\n       8        Very Good\n       7        Good\n       6        Above Average\n       5        Average\n       4        Below Average\n       3        Fair\n       2        Poor\n       1        Very Poor\nIf we have discovered the “true effect of remodeling your kitchen” then the difference in sales price should not depend on the overall house condition, right? (After all, I haven’t said what my house condition is when I asked the question.) Let’s see.\n\names %&gt;%\n  ggplot() +\n  geom_boxplot(aes(x=Kitchen.Qual, y=SalePrice, fill=Kitchen.Qual)) +\n  facet_grid( ~ Overall.Qual)\n\n\n\n\n\n\n\n\n\names %&gt;%\n    group_by(Overall.Qual, Kitchen.Qual) %&gt;%\n    summarize(mean_price=mean(SalePrice), .groups=\"drop\") %&gt;%\n    pivot_wider(id_cols=Overall.Qual, names_from=Kitchen.Qual, values_from=mean_price) %&gt;%\n    mutate(difference=Ex - Gd)\n\n\nA tibble: 9 × 4\n\n\nOverall.Qual\nGd\nEx\ndifference\n\n\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n2\n59000.0\nNA\nNA\n\n\n3\n81100.0\nNA\nNA\n\n\n4\n132468.4\nNA\nNA\n\n\n5\n146430.7\n169931.2\n23500.598\n\n\n6\n180235.6\n142425.0\n-37810.618\n\n\n7\n209187.3\n239900.0\n30712.678\n\n\n8\n263591.5\n311165.7\n47574.197\n\n\n9\n328735.8\n355043.4\n26307.578\n\n\n10\n475000.0\n478246.2\n3246.154\n\n\n\n\n\nThis not only gives a very different answer to our original difference in means, but it even gives surprising answers, such as a negative value for a particular value of overall quality.\nExercise: How can you explain the difference in these results?\nHow can we compute these separate means as regression?\n\nreg_qual &lt;- lm(SalePrice ~ Overall.Qual : Kitchen.Qual - 1, ames)\nprint(summary(reg_qual))\n\n\nCall:\nlm(formula = SalePrice ~ Overall.Qual:Kitchen.Qual - 1, data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-168246  -29701   -4211   22776  276754 \n\nCoefficients: (3 not defined because of singularities)\n                              Estimate Std. Error t value Pr(&gt;|t|)    \nOverall.Qual2:Kitchen.QualEx        NA         NA      NA       NA    \nOverall.Qual3:Kitchen.QualEx        NA         NA      NA       NA    \nOverall.Qual4:Kitchen.QualEx        NA         NA      NA       NA    \nOverall.Qual5:Kitchen.QualEx    169931      16713  10.168  &lt; 2e-16 ***\nOverall.Qual6:Kitchen.QualEx    142425      23636   6.026 2.33e-09 ***\nOverall.Qual7:Kitchen.QualEx    239900      13646  17.580  &lt; 2e-16 ***\nOverall.Qual8:Kitchen.QualEx    311166       9271  33.565  &lt; 2e-16 ***\nOverall.Qual9:Kitchen.QualEx    355043       6493  54.679  &lt; 2e-16 ***\nOverall.Qual10:Kitchen.QualEx   478246      13111  36.478  &lt; 2e-16 ***\nOverall.Qual2:Kitchen.QualGd     59000      47271   1.248  0.21227    \nOverall.Qual3:Kitchen.QualGd     81100      27292   2.972  0.00303 ** \nOverall.Qual4:Kitchen.QualGd    132468      10845  12.215  &lt; 2e-16 ***\nOverall.Qual5:Kitchen.QualGd    146431       4408  33.219  &lt; 2e-16 ***\nOverall.Qual6:Kitchen.QualGd    180236       3286  54.857  &lt; 2e-16 ***\nOverall.Qual7:Kitchen.QualGd    209187       2491  83.964  &lt; 2e-16 ***\nOverall.Qual8:Kitchen.QualGd    263592       3173  83.083  &lt; 2e-16 ***\nOverall.Qual9:Kitchen.QualGd    328736      14253  23.065  &lt; 2e-16 ***\nOverall.Qual10:Kitchen.QualGd   475000      33426  14.211  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 47270 on 1041 degrees of freedom\nMultiple R-squared:  0.9598,    Adjusted R-squared:  0.9592 \nF-statistic:  1657 on 15 and 1041 DF,  p-value: &lt; 2.2e-16\n\n\n\nWe can see that the estimates are the same as the means.\nLet’s look at the regressor matrix that we’re constructing:\n\nx &lt;- model.matrix(SalePrice ~ Overall.Qual : Kitchen.Qual - 1, ames)\n\nbind_cols(\n    select(ames, SalePrice, Overall.Qual, Kitchen.Qual),\n    x) %&gt;%\nhead(15) %&gt;% t()\n\n\nA matrix: 21 × 15 of type chr\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\nSalePrice\n172000\n244000\n195500\n213500\n191500\n236500\n189000\n171500\n212000\n538000\n216000\n184000\n149900\n306000\n275000\n\n\nOverall.Qual\n6\n7\n6\n8\n8\n8\n7\n7\n8\n8\n7\n7\n6\n8\n9\n\n\nKitchen.Qual\nGd\nEx\nGd\nGd\nGd\nGd\nGd\nGd\nGd\nEx\nGd\nGd\nGd\nGd\nEx\n\n\nOverall.Qual2:Kitchen.QualEx\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nOverall.Qual3:Kitchen.QualEx\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nOverall.Qual4:Kitchen.QualEx\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nOverall.Qual5:Kitchen.QualEx\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nOverall.Qual6:Kitchen.QualEx\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nOverall.Qual7:Kitchen.QualEx\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nOverall.Qual8:Kitchen.QualEx\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nOverall.Qual9:Kitchen.QualEx\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\nOverall.Qual10:Kitchen.QualEx\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nOverall.Qual2:Kitchen.QualGd\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nOverall.Qual3:Kitchen.QualGd\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nOverall.Qual4:Kitchen.QualGd\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nOverall.Qual5:Kitchen.QualGd\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nOverall.Qual6:Kitchen.QualGd\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\nOverall.Qual7:Kitchen.QualGd\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n0\n\n\nOverall.Qual8:Kitchen.QualGd\n0\n0\n0\n1\n1\n1\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n\nOverall.Qual9:Kitchen.QualGd\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nOverall.Qual10:Kitchen.QualGd\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "lectures/2_AmesHousingInference.html#controlling-for-living-area",
    "href": "lectures/2_AmesHousingInference.html#controlling-for-living-area",
    "title": "Multilinear regression with the Ames Housing data",
    "section": "Controlling for living area",
    "text": "Controlling for living area\nWe might ask the same question about living area. Unfortunately this is now a continuous variable.\n\names %&gt;%\n    ggplot() +\n      geom_histogram(aes(x=Gr.Liv.Area), bins=40)\n\n\n\n\n\n\n\n\nIt no longer makes sense to look at the difference in means “for a fixed living area,” as you can see by the scatter plot.\n\names %&gt;%\n  ggplot() +\n  geom_point(aes(x=Gr.Liv.Area, y=SalePrice, color=Kitchen.Qual)) +\n  expand_limits(y=0)\n\n\n\n\n\n\n\n\n\names %&gt;%\n    mutate(LivingAreaBin=cut(Gr.Liv.Area, 10)) %&gt;%\n    ggplot() +\n        geom_boxplot(aes(x=Kitchen.Qual, y=SalePrice, fill=Kitchen.Qual)) +\n        facet_grid( ~ LivingAreaBin)\n\n\n\n\n\n\n\n\n\nreg_la &lt;- lm(SalePrice ~ Gr.Liv.Area + Kitchen.Qual, ames)\nprint(summary(reg_la))\n\names %&gt;%\n    mutate(PredPrice=predict(reg_la, ames)) %&gt;%\n  ggplot() +\n      geom_point(aes(x=Gr.Liv.Area, y=SalePrice, color=Kitchen.Qual)) +\n      geom_line(aes(x=Gr.Liv.Area, y=PredPrice, color=Kitchen.Qual, group=Kitchen.Qual), lwd=2) +\n      expand_limits(y=0)\n\n\nCall:\nlm(formula = SalePrice ~ Gr.Liv.Area + Kitchen.Qual, data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-238658  -24841   -2552   23240  240508 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    115200.021   7234.776   15.92   &lt;2e-16 ***\nGr.Liv.Area       104.977      2.912   36.05   &lt;2e-16 ***\nKitchen.QualGd -79532.676   4606.565  -17.27   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 45460 on 1053 degrees of freedom\nMultiple R-squared:  0.6561,    Adjusted R-squared:  0.6555 \nF-statistic:  1005 on 2 and 1053 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\n\nTaking this seriously, we might estimate that the coefficient on Kitchen.QualGd tells us the answer to our question. But the modeling assumptions to not look reasonable. Note, for example, the particularly bad fit of the Ex line for low living area houses."
  },
  {
    "objectID": "lectures/2_AmesHousingInference.html#controlling-for-too-much",
    "href": "lectures/2_AmesHousingInference.html#controlling-for-too-much",
    "title": "Multilinear regression with the Ames Housing data",
    "section": "Controlling for too much",
    "text": "Controlling for too much\nFurthermore, we can even further subdivide and try to “control” for overall quality and living area simultaneously. This is probably closer to what we actually want, but we have real problems with data sparsity now.\n\noptions(repr.plot.width=20, repr.plot.height=6)\names %&gt;%\n  ggplot() +\n  geom_point(aes(x=Gr.Liv.Area, y=SalePrice, color=Kitchen.Qual)) +\n  facet_grid( ~ Overall.Qual) +\n  expand_limits(y=0) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\noptions(repr.plot.width=12, repr.plot.height=6)\n\n\n\n\n\n\n\n\nThis is getting difficult. What about a linear model?\n\nbig_reg &lt;-\n    lm(SalePrice ~ Gr.Liv.Area + Kitchen.Qual + Overall.Qual, ames)\n\nprint(summary(big_reg))\n\n\nCall:\nlm(formula = SalePrice ~ Gr.Liv.Area + Kitchen.Qual + Overall.Qual, \n    data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-143175  -19875   -1192   18301  150483 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     32630.906  34316.248   0.951  0.34188    \nGr.Liv.Area        77.251      2.451  31.524  &lt; 2e-16 ***\nKitchen.QualGd -19904.252   4506.865  -4.416 1.11e-05 ***\nOverall.Qual3   12932.882  39229.663   0.330  0.74171    \nOverall.Qual4   17359.803  34900.926   0.497  0.61901    \nOverall.Qual5   35439.120  34151.188   1.038  0.29964    \nOverall.Qual6   49218.494  34127.289   1.442  0.14954    \nOverall.Qual7   65443.802  34124.965   1.918  0.05541 .  \nOverall.Qual8  107153.925  34189.871   3.134  0.00177 ** \nOverall.Qual9  167573.989  34581.308   4.846 1.45e-06 ***\nOverall.Qual10 241076.614  35627.832   6.767 2.19e-11 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 33970 on 1045 degrees of freedom\nMultiple R-squared:  0.8094,    Adjusted R-squared:  0.8075 \nF-statistic: 443.7 on 10 and 1045 DF,  p-value: &lt; 2.2e-16\n\n\n\nOne way to interpret this is via making predictions for a given house, one with a renovated kitchen and one without.\n\nsample_house &lt;- \n    filter(ames, Kitchen.Qual == \"Gd\") %&gt;% \n    sample_n(1)\n\nsample_house_reno &lt;- sample_house %&gt;% mutate(Kitchen.Qual=\"Ex\")\n\npredict(big_reg, sample_house_reno) - predict(big_reg, sample_house)  \n\n1: 19904.2518250824\n\n\nOf course, the problem is that we may get different answers depending on what regression we run:\n\nlm(SalePrice ~ Gr.Liv.Area + Kitchen.Qual, ames) %&gt;% \n    summary()\n\n\nCall:\nlm(formula = SalePrice ~ Gr.Liv.Area + Kitchen.Qual, data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-238658  -24841   -2552   23240  240508 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    115200.021   7234.776   15.92   &lt;2e-16 ***\nGr.Liv.Area       104.977      2.912   36.05   &lt;2e-16 ***\nKitchen.QualGd -79532.676   4606.565  -17.27   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 45460 on 1053 degrees of freedom\nMultiple R-squared:  0.6561,    Adjusted R-squared:  0.6555 \nF-statistic:  1005 on 2 and 1053 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "quizzes/quizzes.html",
    "href": "quizzes/quizzes.html",
    "title": "Quizzes",
    "section": "",
    "text": "Forthcoming.",
    "crumbs": [
      "Quizzes"
    ]
  },
  {
    "objectID": "quizzes/c45bd141bc098d09ca957d5dd3885992.html",
    "href": "quizzes/c45bd141bc098d09ca957d5dd3885992.html",
    "title": "STAT151A Quiz 1",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\n\\newcommand{\\f}{f}\n\\newcommand{\\fhat}{\\hat{f}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{#1}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\\def\\Q{\\mybold{Q}}\n\n\n\\def\\eps{\\varepsilon}\n\n$$\n\n\n\n\nPlease write your full name and email address here:\n\\[\\\\[2in]\\]\nAlso, please put your intials on each page in case the pages get separated.\n\\[\\\\[1in]\\]\nYou have 30 minutes for this quiz.\nThere are three questions, (1), (2), and (3), each weighted equally..\nThere are extra pages at the end if you need more space for solutions.\n\n\nQuestion 1\nLet \\(\\beta_1\\) and \\(\\beta_2\\) denote two unknowns, and consider the following system of equations.\n\\[\n\\begin{aligned}\n\\beta_1 + 3 \\beta_2 ={}& 4 \\\\\n\\beta_1 + 3 \\beta_2 ={}& 5 \\\\\n\\beta_1 + 2 \\beta_2 ={}& 7 \\\\\n\\beta_1 + 1 \\beta_2 ={}& 1 \\\\\n\\beta_1 + 9 \\beta_2 ={}& 9 \\\\\n\\end{aligned}\n\\]\n(a) Write the system of equations in matrix form \\(\\X \\betav = \\Y\\), where \\(\\betav = \\begin{pmatrix}\\beta_1 \\\\ \\beta_2\\end{pmatrix}\\) is the \\(2\\)–vector of unknowns, \\(\\X\\) is a \\(5 \\times 2\\) matrix, and \\(\\Y\\) is a \\(5\\)–vector.\n(b) How many solutions does this sytem of equations have? Briefly justify your answer.\nSolutions\n(a)\n\\[\n\\X = \\begin{pmatrix}\n1 & 3 \\\\\n1 & 3 \\\\\n1 & 2 \\\\\n1 & 1 \\\\\n1 & 9 \\\\\n\\end{pmatrix}\n\\quad\n\\Y =\n\\begin{pmatrix}\n4 \\\\\n5 \\\\\n7 \\\\\n1 \\\\\n9 \\\\\n\\end{pmatrix}\n\\quad\n\\]\n(b)\nThere are no solutions. For example, the first two equations are incompatible with one another.\n\n\n\nQuestion 2\nFor this problem, I will use the following definitions.\n\n\\(\\X\\) denotes an \\(N \\times P\\) matrix\n\\(\\Y\\) denotes an \\(N\\)–vector (i.e. an \\(N \\times 1\\) matrix)\n\\(\\betav\\) denotes a \\(P\\)–vector\n\nTake \\(N &gt; P &gt; 1\\). You may assume that \\(\\X\\) is full column rank.\nFor each expression, write the dimension of the result, or write “badly formed” if the expression is not a valid matrix expression.\n\n\\(\\X^\\trans \\X\\)\n\\(\\X \\beta\\)\n\\(\\Y^\\trans \\Y\\)\n\\(\\Y \\Y^\\trans\\)\n\\(\\trace{\\Y \\Y^\\trans}\\)\n\\(\\Y - \\X \\beta\\)\n\\((\\X^\\trans \\X)^{-1}\\)\n\\(\\X^\\trans \\left( \\Y - \\X \\beta \\right)\\)\n\\(\\X^\\trans \\Y - \\X^\\trans \\X \\beta\\)\n\\((\\X^\\trans \\X)^{-1} \\X^\\trans \\Y - \\beta\\)\n\nSolutions\n\n\\(\\X^\\trans \\X: P \\times P\\)\n\\(\\X \\beta: N \\times 1\\)\n\\(\\Y^\\trans \\Y: 1 \\times 1\\)\n\\(\\Y \\Y^\\trans: N \\times N\\)\n\\(\\trace{\\Y \\Y^\\trans}: 1 \\times 1\\)\n\\(\\Y - \\X \\beta: N \\times 1\\)\n\\((\\X^\\trans \\X)^{-1}: P \\times P\\)\n\\(\\X^\\trans \\left( \\Y - \\X \\beta \\right): P \\times 1\\)\n\\(\\X^\\trans \\Y - \\X^\\trans \\X \\beta: P \\times 1\\)\n\\((\\X^\\trans \\X)^{-1} \\X^\\trans \\Y - \\beta: P \\times 1\\)\n\n\n\n\nQuestion 3\nSuppose I have a dataset in which each row is a student, and the columns contain the following variables for a given year:\n\nTheir final score in a linear models class\nThe self-reported total number of hours spent studying per week\nTheir grade in a prerequesite theoretical statistics course (like STAT135)\nWhether or not they are a foreign exchange student\n\nConsider two ways I might consider using this dataset:\n(Use 1) How much can a student expect to increase their linear models grade by spending more hours studying?\n(Use 2) How can I identify students who may need extra assistance at the beginning of the next semseter of linear models?\n(a) Which of these two uses is a prediction problem, and which is an inference problem?\n(b) Which question do you expect to be easier to answer with this dataset?\nBriefly justify your answers.\nSolutions\n(a) Use 1 is an inference problem, and use 2 is a prediction problem. Use 1 is a causal inference problem estimating the effect of changing study habits on grades; for example, if the students who study more are already good at math, then a positive association between studying and good grades may be spurious. In contrast, if we only want to identify students who might struggle (Use 2) then all we need to know is the association between a variable and the outcome rather than any causual effect.\n(b) Use 2 would be easier to answer, since we can answer it even in the presence of unobserved confounding variables.\n\nExtra space for answers (indicate clearly which problem you are working on)\n\nExtra space for answers (indicate clearly which problem you are working on)"
  },
  {
    "objectID": "lectures/lectures.html",
    "href": "lectures/lectures.html",
    "title": "Lectures and Labs",
    "section": "",
    "text": "Linear algebra review materials\n\n\n\nWeeks 1–2\n\nCourse philosophy\nSample means\n\nFinal exam grades\n\nInference in the Ames housing data\n\n\n\nWeeks 3–4\n\nMultilinear Regression as loss minimization\nFamiliar examples in matrix form\nMultilinear Regression as projection\n\n\n\nWeeks 5–6\n\nData transformations of the regressors\nData transformations of the response\nInfluence and outliers\n\n\n\nWeeks 7–8\n\nThe FWL theorem\nMultivariate random variables\nStochastic modeling of the residual (in progress)\nOmitted variable bias (in progress)\nRegression to the mean (in progress)\n\n\n\nWeeks 9–10\n\nConfidence intervals and hypothesis testing (in progress)",
    "crumbs": [
      "Lectures"
    ]
  },
  {
    "objectID": "lectures/CoursePhilosophy.html",
    "href": "lectures/CoursePhilosophy.html",
    "title": "Class organization and philosophy",
    "section": "",
    "text": "This is a course about linear models. You probably all know what linear models are already — in short, they are models which fit straight lines through data, possibly high-dimensional data. Every setting we consider in this class will have the following attributes:\n\nA bunch of data points. We’ll index with \\(n = 1, \\ldots, N\\).\nEach datapoint consists of:\n\nA scalar-valued “response” \\(y_n\\)\nA vector-valued “regressor” \\(\\xv_n = (\\x_{n1}, \\ldots, \\x_{nP})\\).\n\n\n\n\n\n\n\n\nNotation\n\n\n\nThroughout the course, I will (almost) always use the letter “x” for regressors and the letter “y” for responses. There will always be \\(N\\) datapoints, and the regressors will be \\(P\\) dimensional. Vectors and matrices will be boldface.\nOf course, I may deviate from this (and any) notation convention by saying so explicitly.\n\n\n\n\nWe will be interested in what \\(\\xv_n\\) can tell us about \\(\\y_n\\). This setup is called a “regression problem,” and can be attacked with lots of models, including non-linear models. But we will focus on approaches to this problem that operate via fitting straight lines to the dependence of \\(y_n\\) on \\(\\xv_n\\).\nRelative to a lot of other machine learning or statistical procedures, linear models are relatively easy to analyze and understand. Yet they are also complex enough to exhibit a lot of the strengths and pitfalls of all machine learning and statistics. So really, this is only partly a course about linear models per se. I hope to make it a course about concepts in statistics in machine learning more generally, but viewed within the relatively simple framework of linear models. Some examples that one might touch on at least briefly include:\n\nAsymptotics under misspecification\nRegularization\nSparsity\nThe bias / variance tradeoff\nThe influence function\nDomain transfer\nDistributed learning\nConformal inference\nPermutation testing\nBayesian methods\nBenign overfitting\n\nI will probably not get to all of these, though you should feel free to ask about them in office hours. Students in the past have also profitably studied more advanced methods as part of their final project.\n\n\n\nIn Lecture XXI of Wittgenstein’s “Lectures on the Foundations of Mathematics” we find this diagram and quote:\n\n\n\nEntrails of a Goose\n\n\nIn the same passage, he goes on to say,\n\n“A use of language has normally what we might call a point. This is immensely important. Although it’s true this is a matter of degree, and we can’t just say where it ends.”\n\nWhen we run a regression, we must have a point. Otherwise we are just looking at goose entrails.\nOur results and conclusions will be expressed in formal mathematical statements and in software. For the purpose of this class, I view mathematics and coding as analogous to language, grammar, and style: you need to have a command of these things in order to say something. But the content of this course doesn’t stop and math and conding, just as learning language alone does not give you something to say. Linear models will be a mathematical and computational tool for communicating with and about the real world. Datasets can speak to us in the language of linear models, and we can communicate with other humans through the language of linear models. Learning to communicate effectively in this way is the most important content of this course, and is a skill that will remain relevent whether or not you ever interpret or fit another linear model in your life.\nStatistical analyses are mathematical, and so are often formulated as beginning from “assumptions,” from which we derive statistical conclusions, from which we derive real world conclusions. For example, we might look at housing price data and:\n\nAssume that the housing prices are a linear function of square footage, with IID normally distributed errors,\nUsing the assumption construct a 95% confidence interval for the slope of the line and,\nInfer something about how much more a particular house would sell for if you build it larger.\n\nA statistics class will typically spend a lot of time on getting from (1) to (2). (For better or worse, the present course will not be a complete exception.) The problem is that the assumption (1) is flatly absurd, and that the leap from (2) to (3) is far more problematic than the leap from (1) to (2). On the other hand, it is probably not really the case that steps (1) and (2) tell you nothing at all about the relationship between square footage and housing costs, it’s just that they don’t constitute any watertight or formal style of inference. Probably a better word for (1) would be “conceit” rather than “assumption,” since you know it to be false, but are willing to entertain it tenatively in order to learn something from the data. Here we can come back to Wittgenstein’s generosity: “They say, after all, that it gives them some guidance”.\nWhether or not a statistical analysis is “good” cannot be evaluated outside a particular context. It is not a mathematical property, even though mathematical techniques can illuminate egregious errors. And it appears to be too strict to say that the assumptions (or conceits) are false and so the whole thing is nonsense, and yet it is clear that statistical analysis does not (typically) live up to any strong claim of logical rigor. So we have to ask contextual questions. Why do we care about the conclusions of this analysis? What will they be used for? Who needs to understand our analysis? What are the consequences of certain kinds of errors, including the application of implausible conceits?\nOutside of a classroom, you will probably never encounter a linear model without a real question and context attached to it. I will make a real effort in this class to respect this fact, and always present data in context, to the extent possible within a classroom setting. I hope you will in turn get in the habit of always thinking about the context of a problem, even when going through formal homework and lab exercises.\nFor pedagogical reasons we may have to step into abstract territory at times, but I will make an effort to tie what we learn back to reality, and, in grading we’ll make sure to reward your efforts to do so as well. Just as there is not a “correct” essay in an English class, this will often mean that there are not “correct” analyses for a dataset, even though there are certainly better and worse approaches, as well as unambiguous errors."
  },
  {
    "objectID": "lectures/CoursePhilosophy.html#linear-models-as-a-gateway-into-statistics-and-machine-learning-writ-large",
    "href": "lectures/CoursePhilosophy.html#linear-models-as-a-gateway-into-statistics-and-machine-learning-writ-large",
    "title": "Class organization and philosophy",
    "section": "",
    "text": "We will be interested in what \\(\\xv_n\\) can tell us about \\(\\y_n\\). This setup is called a “regression problem,” and can be attacked with lots of models, including non-linear models. But we will focus on approaches to this problem that operate via fitting straight lines to the dependence of \\(y_n\\) on \\(\\xv_n\\).\nRelative to a lot of other machine learning or statistical procedures, linear models are relatively easy to analyze and understand. Yet they are also complex enough to exhibit a lot of the strengths and pitfalls of all machine learning and statistics. So really, this is only partly a course about linear models per se. I hope to make it a course about concepts in statistics in machine learning more generally, but viewed within the relatively simple framework of linear models. Some examples that one might touch on at least briefly include:\n\nAsymptotics under misspecification\nRegularization\nSparsity\nThe bias / variance tradeoff\nThe influence function\nDomain transfer\nDistributed learning\nConformal inference\nPermutation testing\nBayesian methods\nBenign overfitting\n\nI will probably not get to all of these, though you should feel free to ask about them in office hours. Students in the past have also profitably studied more advanced methods as part of their final project."
  },
  {
    "objectID": "lectures/CoursePhilosophy.html#linear-models-as-a-gateway-into-epistemology",
    "href": "lectures/CoursePhilosophy.html#linear-models-as-a-gateway-into-epistemology",
    "title": "Class organization and philosophy",
    "section": "",
    "text": "In Lecture XXI of Wittgenstein’s “Lectures on the Foundations of Mathematics” we find this diagram and quote:\n\n\n\nEntrails of a Goose\n\n\nIn the same passage, he goes on to say,\n\n“A use of language has normally what we might call a point. This is immensely important. Although it’s true this is a matter of degree, and we can’t just say where it ends.”\n\nWhen we run a regression, we must have a point. Otherwise we are just looking at goose entrails.\nOur results and conclusions will be expressed in formal mathematical statements and in software. For the purpose of this class, I view mathematics and coding as analogous to language, grammar, and style: you need to have a command of these things in order to say something. But the content of this course doesn’t stop and math and conding, just as learning language alone does not give you something to say. Linear models will be a mathematical and computational tool for communicating with and about the real world. Datasets can speak to us in the language of linear models, and we can communicate with other humans through the language of linear models. Learning to communicate effectively in this way is the most important content of this course, and is a skill that will remain relevent whether or not you ever interpret or fit another linear model in your life.\nStatistical analyses are mathematical, and so are often formulated as beginning from “assumptions,” from which we derive statistical conclusions, from which we derive real world conclusions. For example, we might look at housing price data and:\n\nAssume that the housing prices are a linear function of square footage, with IID normally distributed errors,\nUsing the assumption construct a 95% confidence interval for the slope of the line and,\nInfer something about how much more a particular house would sell for if you build it larger.\n\nA statistics class will typically spend a lot of time on getting from (1) to (2). (For better or worse, the present course will not be a complete exception.) The problem is that the assumption (1) is flatly absurd, and that the leap from (2) to (3) is far more problematic than the leap from (1) to (2). On the other hand, it is probably not really the case that steps (1) and (2) tell you nothing at all about the relationship between square footage and housing costs, it’s just that they don’t constitute any watertight or formal style of inference. Probably a better word for (1) would be “conceit” rather than “assumption,” since you know it to be false, but are willing to entertain it tenatively in order to learn something from the data. Here we can come back to Wittgenstein’s generosity: “They say, after all, that it gives them some guidance”.\nWhether or not a statistical analysis is “good” cannot be evaluated outside a particular context. It is not a mathematical property, even though mathematical techniques can illuminate egregious errors. And it appears to be too strict to say that the assumptions (or conceits) are false and so the whole thing is nonsense, and yet it is clear that statistical analysis does not (typically) live up to any strong claim of logical rigor. So we have to ask contextual questions. Why do we care about the conclusions of this analysis? What will they be used for? Who needs to understand our analysis? What are the consequences of certain kinds of errors, including the application of implausible conceits?\nOutside of a classroom, you will probably never encounter a linear model without a real question and context attached to it. I will make a real effort in this class to respect this fact, and always present data in context, to the extent possible within a classroom setting. I hope you will in turn get in the habit of always thinking about the context of a problem, even when going through formal homework and lab exercises.\nFor pedagogical reasons we may have to step into abstract territory at times, but I will make an effort to tie what we learn back to reality, and, in grading we’ll make sure to reward your efforts to do so as well. Just as there is not a “correct” essay in an English class, this will often mean that there are not “correct” analyses for a dataset, even though there are certainly better and worse approaches, as well as unambiguous errors."
  },
  {
    "objectID": "lectures/8_StochasticModelingOfOLS.html",
    "href": "lectures/8_StochasticModelingOfOLS.html",
    "title": "Stochastic assumptions on the residual",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\n\\newcommand{\\f}{f}\n\\newcommand{\\fhat}{\\hat{f}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{#1}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\\def\\Q{\\mybold{Q}}\n\n\n\\def\\eps{\\varepsilon}\n\n$$\n\n\n\n\n\nGoals\n\nIntroduce statistical assumptions to OLS\n\nHierachies of assumptions\n\nIndependent mean zero residuals\nIID residuals\nIID Normal residuals\n\nBias in OLS\nVariance in OLS\nFixed versus random regressors\nMeaning and criticism of assumptions\n\n\n\n\nBirths dataset\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nRecall our births dataset. Want to infer smoking effect. How to tell if chance?\nWe need some stochastic assumptions. Do the example for sample means, then controlling for mage. Note that there’s a natural zero residual assumption for the sample means that isn’t necessarily the case for the model with mage.\n\n\nAssumptions on the “true” model\nThere is often no sensible true model. But a sort of “what if” exercise can help understand the data and avoid being fooled by noise, as long as we maintain a critical distance.\nThe loosest assumption is that \\((\\xv_n, \\y_n)\\) are IID pairs. We’ll just set this aside until later — this is a “machine learning” style assumption.\nWe might say something like: Assume that \\(\\y_n = \\betav^\\trans \\xv_n + \\res_n\\) for some \\(\\res_n\\). Note that this is vacuous! (Even if we assume IID \\((\\xv_n, \\y_n)\\).)\nBut it’s not vacuous if we assume something about the distribution of \\(\\res_n\\). If we assume \\(\\expect{\\res_n \\vert \\xv_n} = 0\\), then \\(\\expect{\\y_n \\vert \\x_n} = \\betav^\\trans \\xv_n\\). This is a strong assumption!\nWrite out \\(\\expect{\\betavhat}\\), show unbiased.\nAssume \\(\\eps_n\\) independent with \\(\\var{\\eps_n} = \\sigma_n^2\\). (They need not be all the same.) Then we can write out \\(\\var{\\y_n}\\) and \\(\\cov{\\betavhat}\\).\nAssume that \\(\\eps_n\\) are IID with variance \\(\\sigma^2\\). Then we can even estimate the variance!\nFinally assume that \\(\\eps_n\\) are IID normal. Then \\(\\betavhat\\) is itself normal.\n\n\nFixed versus random regressors\nTalk about what the distinction means. We’ll keep the regressors “fixed” for simplicity until we talk about machine learning later in the course. We still might condition to be explicit.\n\n\nMeaning of assumptions\nThere is often an imagineable world where the error assumptions might be true, but that often is not the right question you want to ask.\nKleiber example\nAluminum example"
  },
  {
    "objectID": "lectures/7_5_MultivariateNormal.html",
    "href": "lectures/7_5_MultivariateNormal.html",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "",
    "text": "\\(\\LaTeX\\)"
  },
  {
    "objectID": "lectures/7_5_MultivariateNormal.html#the-bivariate-normal-distribution",
    "href": "lectures/7_5_MultivariateNormal.html#the-bivariate-normal-distribution",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "The bivariate normal distribution",
    "text": "The bivariate normal distribution\nSuppose that \\({\\u}_1\\) and \\({\\u}_2\\) are independent standard normal random variables. Suppose we define the new random variables\n\\[\n\\begin{aligned}\n{\\x_1} :={}& a_{11} {\\u}_1 + a_{12} {\\u}_2 \\\\\n{\\x_2} :={}& a_{21} {\\u}_1 + a_{22} {\\u}_2.\n\\end{aligned}\n\\]\nWe can see that \\({\\x_1}\\) and \\({x_2}\\) are each normal random variables, and we can compute their means and variances:\n\\[\n\\begin{aligned}\n\\expect{{\\x_1}} :={}& a_{11} \\expect{{\\u}_1} + a_{12} \\expect{{\\u}_2} = 0 \\\\\n\\expect{{\\x_2}} :={}& a_{21} \\expect{{\\u}_1} + a_{22} \\expect{{\\u}_2} = 0 \\\\\n\\var{{\\x_1}} :={}& a_{11}^2 \\var{{\\u}_1} + a_{12}^2 \\var{{\\u}_2} = a_{11}^2 + a_{12}^2 \\\\\n\\var{{\\x_2}} :={}& a_{21}^2 \\var{{\\u}_1} + a_{22}^2 \\var{{\\u}_2} = a_{21}^2 + a_{22}^2.\n\\end{aligned}\n\\]\nBut in general \\({\\x_1}\\) and \\({x_2}\\) are not independent. If they were, we would have \\(\\expect{{\\x_1} {\\x_2}} = \\expect{{\\x_1}} \\expect{{\\x_2}} = 0\\), but in fact we have\n\\[\n\\expect{{\\x_1} {\\x_2}} =\n\\expect{(a_{11} {\\u}_1 + a_{12} {\\u}_2)(a_{21} {\\u}_1 + a_{22} {\\u}_2)} =\na_{11} a_{21} \\expect{{\\u}_1^2} + a_{12} a_{22} \\expect{{\\u}_2^2} =\na_{11} a_{21} + a_{12} a_{22}.\n\\]\nThe variables \\({\\x_1}\\) and \\({x_2}\\) are instances of the bivariate normal distribution, which is the two-dimensional analogue of the normal distribution. One can define the bivariate normal distribution in many ways. Here, we have defined it by taking linear combinations of independent univariate normal distributions. It turns out this is always possible, and for the purposes of this class, we will see that it is enough to use such a definition.\nThe properties we derived above can in fact be represented more succinctly in vector notation. We can write\n\\[\n{\\xv} =\n\\begin{pmatrix}\n{\\x_1} \\\\\n{\\x_2} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n{\\u}_1 \\\\\n{\\u}_2 \\\\\n\\end{pmatrix} =:\n\\A {\\uv}.\n\\]\nNote that the matrix \\(\\A\\) is not random. Defining the expectation of a vector to be the vector of expectations of its entries, we get\n\\[\n\\expect{{\\xv}} = \\A \\expect{{\\uv}} = \\A \\zerov = \\zerov,\n\\]\njust as above.\nDefining the “variance” of \\({\\xv}\\) is more subtle. Recall that a normal distribution is fully characterized by its variance. We would like this to be the case for the bivariate normal as well. But for this it is not enough to know the marginal varianecs \\(\\var{{\\x_1}}\\) and \\(\\var{{\\x_2}}\\), since the “covariance” \\(\\cov{{\\x_1}, {\\x_2}}\\) is also very important for the behavior of the random variable.\n\n\n\n\n\n\nNotation\n\n\n\nWhen dealing with two scalar– or vector–valued random variables, I will write the covariance as a function of two arguments, e.g., \\(\\cov{{\\x_1}, {\\x_2}}\\). However, when speaking of only a single random variable I might write \\(\\cov{{\\xv}}\\) as a shorthand for the covariance of a vector with itself, e.g., \\(\\cov{{\\xv}} = \\cov{{\\xv}, {\\xv}}\\). I will try to reserve the variance \\(\\var{{\\x}}\\) for only the variance of a single scalar random variable.\n\n\nA convenient way to write all the covariances in a single expression is to define\n\\[\n\\begin{aligned}\n\\cov{{\\xv}} ={}&\n  \\expect{\\left({\\xv} - \\expect{{\\xv}} \\right)\n          \\left({\\xv} - \\expect{{\\xv}} \\right)^\\trans}\n          \\\\={}&\n  \\expect{\n    \\begin{pmatrix}\n      ({\\x_1} - \\expect{{\\x_1}})^2 &\n        ({\\x_1} - \\expect{{\\x_1}})({\\x_2} - \\expect{{\\x_2}}) \\\\\n      ({\\x_1} - \\expect{{\\x_1}})({\\x_2} - \\expect{{\\x_2}}) &\n        ({\\x_2} - \\expect{{\\x_2}})^2\n    \\end{pmatrix}\n  }\n          \\\\={}&\n  \\begin{pmatrix}\n  \\var{{\\x_1}} & \\cov{{\\x_1}, {\\x_2}} \\\\\n  \\cov{{\\x_1}, {\\x_2}} & \\var{{\\x_2}}\n  \\end{pmatrix}.\n\\end{aligned}\n\\]\nNote that the dimension of the matrix inside the expectation is \\(2 \\times 2\\), since we take the transpose on the left. By expanding, we see that each entry of this “covariance matrix” has the covariance between two entries of the vector, and the diagonal contains the “marginal” variances.\nThis expression in fact allows quite convenient calculation of all the covariances above, since\n\\[\n\\begin{aligned}\n\\cov{{\\xv}} ={}&\n\\expect{\\A {\\uv} {\\uv}^\\trans \\A^\\trans}\n\\\\={}&\n\\A  \\expect{{\\uv} {\\uv}^\\trans } \\A^\\trans\n\\\\={}&\n\\A  \\id \\A^\\trans\n\\\\={}&\n\\A \\A^\\trans.\n\\end{aligned}\n\\]\nYou can readily verify that the expression matches the ones derived manually above."
  },
  {
    "objectID": "lectures/7_5_MultivariateNormal.html#the-multivariate-normal-distribution",
    "href": "lectures/7_5_MultivariateNormal.html#the-multivariate-normal-distribution",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "The multivariate normal distribution",
    "text": "The multivariate normal distribution\nWe can readily generalize the previous section to \\(P\\)–dimensional vectors. Let \\({\\uv}\\) denote a vector of \\(P\\) standard normal random variables. Then define\n\\[\n{\\xv} := \\A {\\uv} + \\muv.\n\\]\nThen we say that \\({\\xv}\\) is a multivariate normal random variable with mean\n\\[\n\\expect{{\\xv}} = \\A \\expect{{\\uv}} + \\muv = \\muv\n\\]\nand\n\\[\n\\cov{{\\xv}} =\n\\expect{({\\xv} - \\muv)({\\xv} - \\muv)^\\trans} =\n\\expect{(\\A {\\uv})(\\A {\\uv})^\\trans} =\n\\A \\A^\\trans,\n\\]\nwriting\n\\[\n{\\xv} \\sim \\gauss{\\muv, \\A \\A^\\trans}.\n\\]\nSuppose we want to design a multivariate normal with a given covariance matrix \\(\\Sigmam\\). If we require that \\(\\Sigmam\\) be positive semi-definite (see exercise), the we can take \\(\\A = \\Sigmam^{1/2}\\), and use that to construct a multivariate normal with covariance \\(\\Sigmam\\).\n\n\n\n\n\n\nNotation\n\n\n\nI will write \\({\\xv} \\sim \\gauss{\\muv, \\Sigmam}\\) to mean that \\({\\xv}\\) is a multivariate normal random variable with mean \\(\\muv\\) and covariance matrix \\(\\Sigmam\\).\n\n\nNote that we will not typically go through the construction \\({\\xv} = \\Sigmam^{1/2} {\\uv}\\) — we’ll take for granted that we can do so. The construction in terms of univariate random variables is simply an easy way to define multivariate random variables without having to deal with multivariate densities.\n\n\n\n\n\n\nExercise\n\n\n\nShow that if you have a vector-valued random variable with a non positive semi-definite covariance matrix, you can construct a univariate random variable with negative variance, which is impossible. It follows that every vector covariance must be postive semi-definite.\n\n\nA few useful properties come out immediately from properties of the univariate normal distribution:\n\nThe entries of a multivariate normal random vector are independent if an only if the covariance matrix is diagonal.\nAny linear combination of a multivariate normal random variable is itself multivariate normal (possibly with different dimension).\n\\(\\prob{{\\xv} \\in S} = \\prob{{\\uv} \\in \\{\\uv: \\A \\uv \\in S\\}}\\)\n\n\nProperties of the multivariate normal\nSince we have defined a \\(\\xv \\sim \\gauss{\\muv, \\Sigmam}\\) random variable to be \\(\\xv = \\Sigmam^{1/2} \\uv + \\muv\\) where \\(\\uv\\) is a standard normal random variable, we can derive the distribution of linear transformations of \\(\\xv\\) directly. In particular, suppose that \\(\\xv\\) is a \\(P\\)–vector, and let \\(\\vv\\) be another \\(P\\)–vector and \\(\\A\\) a \\(P \\times P\\) matrix, then\n\\[\n\\begin{aligned}\n\\vv^\\trans \\xv \\sim \\gauss{\\vv^\\trans \\mu, \\vv^\\trans \\Sigmam \\vv}\n\\quad\\textrm{and}\\quad\n\\A \\xv \\sim \\gauss{\\A \\mu, \\A \\Sigmam \\A^\\trans}.\n\\end{aligned}\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nProve the preceding result.\n\n\nIn particular,\n\\[\n\\begin{aligned}\n\\expect{\\vv^\\trans \\xv} ={}& \\vv^\\trans \\mu \\\\\n\\expect{\\A \\xv} ={}& \\A \\mu \\\\\n\\cov{\\vv^\\trans \\xv} ={}& \\vv^\\trans \\Sigmam \\vv \\\\\n\\cov{\\A \\xv} ={}& \\A \\Sigmam \\A^\\trans.\n\\end{aligned}\n\\]\nAs a special case, we can take \\(\\vv = \\ev_n\\), the vector with \\(1\\) in entry \\(n\\) and \\(0\\) elsewhere, to get \\[\n\\ev_n^\\trans \\xv = \\xv_n \\sim \\gauss{\\mu_n, \\Sigmam_{nn}},\n\\] as expected."
  },
  {
    "objectID": "lectures/7_5_MultivariateNormal.html#generic-random-vectors",
    "href": "lectures/7_5_MultivariateNormal.html#generic-random-vectors",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "Generic random vectors",
    "text": "Generic random vectors\nIn general, we can consider a random vector — or a random matrix — as a collection of potentially non-independent random values.\nFor such a vector \\({\\xv} \\in \\rdom{P}\\), we can speak about its expectation and covariance just as we would for a multivariate normal distribution. Specifically,\n\\[\n\\expect{{\\xv}} =\n\\begin{pmatrix}\n\\expect{{\\x}_1} \\\\\n\\vdots \\\\\n\\expect{{\\x}_P} \\\\\n\\end{pmatrix}\n\\quad\\textrm{and}\\quad\n\\cov{{\\xv}} =\n\\expect{\\left({\\xv} - \\expect{{\\xv}} \\right)\n        \\left({\\xv} - \\expect{{\\xv}} \\right)^\\trans} =\n\\begin{pmatrix}\n\\var{{\\x_1}} & \\cov{{\\x_1}, {\\x_2}} & \\ldots & \\cov{{\\x_1}, {\\x_P}} \\\\\n\\cov{{\\x_2}, {\\x_1}} & \\ldots & \\ldots & \\cov{{\\x_2}, {\\x_P}} \\\\\n\\vdots & & & \\vdots \\\\\n\\cov{{\\x_P}, {\\x_1}} & \\ldots & \\ldots & \\var{{\\x_P}} \\\\\n\\end{pmatrix}.\n\\]\nFor these expressions to exist, it suffices for \\(\\expect{{\\x}_p^2} &lt; \\infty\\) for all \\(p \\in \\{1,\\ldots,P\\}\\).\nWe will at times talk about the expectation of a random matrix, \\({\\X}\\), which is simply the matrix of expectations. The notation for covariances of course doesn’t make sense for matrices, but it won’t be needed. (In fact, in cases where the covariance of a random matrix is needed in statistics, the matrix is typically stacked into a vector first.)"
  },
  {
    "objectID": "lectures/7_5_MultivariateNormal.html#the-law-of-large-numbers-for-vectors",
    "href": "lectures/7_5_MultivariateNormal.html#the-law-of-large-numbers-for-vectors",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "The law of large numbers for vectors",
    "text": "The law of large numbers for vectors\nThe law of large numbers is particularly simple for vectors — as long as the dimension stays fixed as \\(N \\rightarrow \\infty\\), you can simply apply the LLN to each component separately. Suppose you’re given a sequence of vector-valued random variables, \\({\\xv}_n \\in \\rdom{P}\\). Write \\(\\expect{{\\xv}_n} = \\muv_n\\). Then, as long as we can apply the LLN to each component, we get\n\\[\n\\overline{{\\xv}} :=\n\\meann {\\xv}_n \\rightarrow \\overline{\\muv}\n\\quad\\textrm{as }N\\rightarrow \\infty.\n\\]"
  },
  {
    "objectID": "lectures/7_5_MultivariateNormal.html#extensions",
    "href": "lectures/7_5_MultivariateNormal.html#extensions",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "Extensions",
    "text": "Extensions\nSince we see that the shape of the vector doesn’t matter, we can also apply the LLN to matrices. For example, if \\({\\X}_n\\) is a sequence of random matrices, with \\(\\expect{{\\X}_n} = \\A\\), then\n\\[\n\\meann {\\X}_n \\rightarrow \\A.\n\\]\nWe will also use (without proof) a theorem called the continuous mapping theorem, which says that, for a continous function \\(f(\\cdot)\\), then\n\\[\nf\\left(\\meann {\\xv}_n\\right) \\rightarrow f(\\overline{\\mu}).\n\\]\nNote that the preceding statment is different than saying\n\\[\n\\meann  f\\left( {\\xv}_n \\right) \\rightarrow \\expect{f\\left( {\\xv}_n \\right)},\n\\]\nwhich may also be true, but which applies the LLN to the random variables \\(f\\left( {\\xv}_n \\right)\\)."
  },
  {
    "objectID": "lectures/7_5_MultivariateNormal.html#the-central-limit-theorem-for-vectors",
    "href": "lectures/7_5_MultivariateNormal.html#the-central-limit-theorem-for-vectors",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "The central limit theorem for vectors",
    "text": "The central limit theorem for vectors\nWe might hope that we can apply the CLT componentwise just as we did for the LLN, but we are not so lucky. It is true that, assuming (as before) that \\(\\meann \\v_{np} \\rightarrow \\overline{v}_p\\) for each \\(p\\), that\n\\[\n\\sqrt{N} (\\overline{{\\xv}}_p - \\overline{\\mu}_p) \\rightarrow\n  \\gauss{0, \\overline{v}_p}.\n\\]\nHowever, this does not tell us about the joint behavior of the random variables. For the CLT, we have to consider the behavior of the whole vector. In particular, write\n\\[\n\\Sigmam_n := \\cov{{\\xv}_n}\n\\]\nand assume that\n\\[\n\\meann \\Sigmam_n \\rightarrow \\overline{\\Sigmam}\n\\]\nfor each entry of the matrix, where \\(\\overline{\\Sigmam}\\) is element-wise finite. (Note that this requires the average of each entry of the diagonal to to converge!) Then\n\\[\n\\frac{1}{\\sqrt{N}} \\left( \\sumn {\\xv}_n - \\expect{{\\xv}_n} \\right)\n\\rightarrow {\\zv} \\quad\\textrm{ where }\n{\\zv} \\sim \\gauss{\\zerov, \\overline{\\Sigmam}}.\n\\]\nFinally, we note (again without proof) that the continuous mapping theorem applies to the CLT as well. That is, if \\(f(\\cdot)\\) is continuous, then\n\\[\nf\\left( \\frac{1}{\\sqrt{N}} \\left( \\sumn {\\xv}_n - \\expect{{\\xv}_n} \\right) \\right)\n\\rightarrow f\\left( {\\zv} \\right).\n\\]"
  },
  {
    "objectID": "lectures/7_5_MultivariateNormal.html#examples-of-different-joint-behavior-with-the-same-marginal-behavior",
    "href": "lectures/7_5_MultivariateNormal.html#examples-of-different-joint-behavior-with-the-same-marginal-behavior",
    "title": "Vector and matrix-valued statistics and limit theorems.",
    "section": "Examples of different joint behavior with the same marginal behavior",
    "text": "Examples of different joint behavior with the same marginal behavior\nHere’s a simple example that illustrates the problem. Consider two settings, based on IID standard normal scalar variables \\({u}_n\\) and \\({v}_n\\).\n\nSetting one\nTake\n\\[{\\xv}_n =\n\\begin{pmatrix}\n{u}_n\\\\\n{v}_n\n\\end{pmatrix}.\n\\]\nWe know that \\(\\frac{1}{\\sqrt{N}} {\\xv}_n\\) is exactly multivariate normal with mean \\(\\zerov\\) and covariance matrix \\(\\id{}\\) for all \\(N\\). So, a fortiori,\n\\[\n\\frac{1}{\\sqrt{N}} \\sumn {\\xv}_n \\rightarrow \\gauss{\\zerov, \\id{}}.\n\\]\n\n\nSetting two\nTake\n\\[{\\yv}_n =\n\\begin{pmatrix}\n{u}_n\\\\\n{u}_n\n\\end{pmatrix}.\n\\]\nWe know that \\(\\frac{1}{\\sqrt{N}} {\\yv}_n\\) is exactly multivariate normal with mean \\(\\zerov\\) and covariance matrix \\(\\onev \\onev^\\trans\\) for all \\(N\\).\nSo, a fortiori,\n\\[\n\\frac{1}{\\sqrt{N}} \\sumn {\\yv}_n \\rightarrow \\gauss{\\zerov, \\onev \\onev^\\trans}.\n\\]\nFor both \\({\\xv}\\) and \\({\\yv}\\), each component converges to a standard normal random variable. But for \\({\\xv}\\) the two components were independent, and for \\({\\yv}\\) they were perfectly dependent. The behavior of the marginals cannot tell you about the joint behavior."
  },
  {
    "objectID": "lectures/5_Transformations_X.html",
    "href": "lectures/5_Transformations_X.html",
    "title": "Transforming regressors.",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\n\\newcommand{\\f}{f}\n\\newcommand{\\fhat}{\\hat{f}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{#1}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\\def\\Q{\\mybold{Q}}\n\n\n\\def\\eps{\\varepsilon}\n\n$$\n\n\n\n\n\\(\\,\\) \n\n\nGoals\n\nDiscuss transformations of regressors\n\nDefine R2, and talk about its relationship to the regressors\nPolynomial regressors\nInteraction regressors\n\n\n\n\nDefine R squared\nWe have defined \\(\\Yhat = \\X \\betavhat\\), and shown that \\(\\Yhat = \\proj{X} \\betavhat = \\X (\\X^\\trans\\X)^{-1}\\X^\\trans \\Y\\). We can call\n\\[\n\\begin{aligned}\n\\Yhat^\\trans \\Yhat :={}& ESS = \\textrm{\"Explained sum of squares\"} \\\\\n\\Y^\\trans \\Y :={}& TSS = \\textrm{\"Total sum of squares\"} \\\\\n\\resvhat^\\trans \\resvhat :={}& RSS = \\textrm{\"Residual sum of squares\"}.\n\\end{aligned}\n\\]\nIn your homework, you will show that \\(TSS = ESS + RSS\\). It follows immediately that \\(0 \\le ESS \\le TSS\\), and we can define \\[\nR^2 := \\frac{ESS}{TSS} \\in [0, 1].\n\\]\nIn a sense, high \\(R^2\\) means a “good fit” in the sense that the least squares fit has low error.\n\n\nHigh \\(R^2\\) is not necessarily a good fit\nBut high \\(R^2\\) does not necessarily mean that the fit is accurate or useful! In particular, by increasing the number of regressors, you can only make \\(R^2\\) increase, and there are clearly silly regressions with great \\(R^2\\).\nHere are two examples:\n\n\\(\\Y \\sim \\Y\\)\n\\(\\Y \\sim \\id\\)\n\nToday we will look at the effect of regressors on \\(R^2\\), and how to make new regressors from the ones you have. We will also talk about how to interpret such “interaction” regressors.\n\n\nRecall the Ames data\nLet’s try to predict SalePrice using the regressors in the Ames dataset. We’ll explore some different ways to include more regressors.\n\nggplot(ames_df) +\n  geom_point(aes(x=Gr.Liv.Area, y=SalePrice))\n\n\n\n\n\n\n\n\n\n\nLet’s run on a bunch of regressors\n\nregs &lt;- c(\"Neighborhood\", \"Bldg.Type\", \"Overall.Qual\", \"Overall.Cond\", \n          \"Exter.Qual\", \"Exter.Cond\", \n          \"X1st.Flr.SF\", \"X2nd.Flr.SF\", \"Gr.Liv.Area\", \"Low.Qual.Fin.SF\",\n          \"Kitchen.Qual\", \n          \"Garage.Area\", \"Garage.Qual\", \"Garage.Cond\",\n          \"Paved.Drive\", \"Open.Porch.SF\", \"Enclosed.Porch\",\n          \"Pool.Area\", \"Yr.Sold\")\n\nif (any(!(regs %in% names(ames_df)))) {\n  print(setdiff(regs, names(ames_df)))\n}\n\nI’ll run a bunch of regressions, including more and more regressors, and see what happens.\n\n\nHow does the “fit” change?\n\n\n\n\n\n\n\n\n\n\n\nHow does the “fit” change?\n\n\n\n\n\n\n\n\n\n\nYou can’t use R2 alone to determine which regressors to include, since R2 will always tell you to add more\nIt helps to be smart about what regressors you include\nHow to make more regressors from the regressors we have?\n\n\n\nPolynomial regressors\n\nreg &lt;- lm(SalePrice ~ Gr.Liv.Area, ames_df)\n\n\n\n\n\n\n\n\n\n\nIt looks like there may be non-linear dependence. How can we fit a nonlinear curve with “linear regression”?\n\n\nRegress on the square too\n\nreg_sq &lt;- lm(SalePrice ~ Gr.Liv.Area + I(Gr.Liv.Area^2), ames_df)\n\n\n\n\n\n\n\n\n\n\nIt looks like there may be non-linear dependence. How can we fit a nonlinear curve with “linear regression”?\n\n\nRegress on the higher-order polynomials\n\nreg_order1 &lt;- lm(SalePrice ~ Gr.Liv.Area, ames_df)\nreg_order2 &lt;- lm(SalePrice ~ Gr.Liv.Area + I(Gr.Liv.Area^2), ames_df)\nreg_order3 &lt;- lm(SalePrice ~ Gr.Liv.Area + I(Gr.Liv.Area^2) + I(Gr.Liv.Area^3), ames_df)\nreg_order4 &lt;- lm(SalePrice ~ Gr.Liv.Area + I(Gr.Liv.Area^2) + \n                 I(Gr.Liv.Area^3) + I(Gr.Liv.Area^4), ames_df)\n\n\n\n\n\n\n\n\n\n\nWhat will happen if you regress on higher and higher-order polynomials? How do you decide when to stop?\n\n\nRegress on the higher-order polynomials\nThe poly function does the same thing more compactly. It also uses “orthogonal polynomials,” which can help fitting.\n\nreg_order4 &lt;- lm(SalePrice ~ Gr.Liv.Area + I(Gr.Liv.Area^2) + \n                 I(Gr.Liv.Area^3) + I(Gr.Liv.Area^4), ames_df)\n\nreg_order4_v2 &lt;- lm(SalePrice ~ poly(Gr.Liv.Area, 4), ames_df)\nprint(max(abs(fitted(reg_order4) - fitted(reg_order4_v2))))\n\n[1] 6.984919e-10\n\n\n\n\nInteractions\n\n\n\n\n\n\n\n\n\nRecall that we found some grouping by kitchen quality. Let’s include the indicator in the regression:\n\nreg_additive &lt;- lm(SalePrice ~ Gr.Liv.Area + Kitchen.Qual, ames_df)\n\n\n\nInteractions\n\n\n\n\n\n\n\n\n\nThe fit is now different for each kitchen quality, and the overall slope changed too!\nHowever, the slope is still the same within each kitchen quality group. Why?\n\n\nXTX plot\nWe can see in the XTX matrix that there is some association between living area and kitchen quality.\nHow would we expect the slope to have changed if this matrix were diagonal?\n\nx &lt;- model.matrix(reg_additive) %&gt;% scale(center=FALSE)\nxtx &lt;- t(x) %*% x / nrow(ames_df)\nPlotMatrix(xtx)\n\n\n\n\n\n\n\n\n\n\nLevel dropping\nNote that one of the levels is automatically dropped? Why?\n\ntable(ames_df$Kitchen.Qual)\n\n\n  Ex   Fa   Gd   TA \n 108   42  844 1197 \n\nprint(coefficients(reg_additive))\n\n   (Intercept)    Gr.Liv.Area Kitchen.QualFa Kitchen.QualGd Kitchen.QualTA \n  162906.94088       82.67511  -144558.98868   -90449.59019  -128625.75269 \n\n\n\n\nInteractions\nHow would you fit a different slope for each kitchen quality? Use an interaction term. You can use the R notation : and it will construct the interaction automatically for you.\n\nreg_inter &lt;- lm(SalePrice ~ Gr.Liv.Area:Kitchen.Qual, ames_df)\nbind_cols(select(ames_df, Gr.Liv.Area, Kitchen.Qual), \n          as.data.frame(model.matrix(reg_inter)))[1,] %&gt;%\n          t()\n\n                           1     \nGr.Liv.Area                \"1656\"\nKitchen.Qual               \"TA\"  \n(Intercept)                \"1\"   \nGr.Liv.Area:Kitchen.QualEx \"0\"   \nGr.Liv.Area:Kitchen.QualFa \"0\"   \nGr.Liv.Area:Kitchen.QualGd \"0\"   \nGr.Liv.Area:Kitchen.QualTA \"1656\"\n\n\nHere’s a good cheat sheet for R formulas:\nhttps://www.econometrics.blog/post/the-r-formula-cheatsheet/\n\n\nInteraction XTX matrix\n\nx &lt;- model.matrix(reg_inter) %&gt;% scale(center=FALSE)\nxtx &lt;- t(x) %*% x / nrow(ames_df)\nPlotMatrix(xtx)\n\n\n\n\n\n\n\n\n\n\nInteraction plot\n\n\n\n\n\n\n\n\n\nIs there a point where all the slope interaction lines cross?\n\n\nBoth slopes and offsets\nUsing the R notation *, we can add both constant interactions and slope interactions:\n\nreg_add_inter &lt;- lm(SalePrice ~ Gr.Liv.Area*Kitchen.Qual, ames_df)\ncolnames(model.matrix(reg_add_inter))\n\n[1] \"(Intercept)\"                \"Gr.Liv.Area\"               \n[3] \"Kitchen.QualFa\"             \"Kitchen.QualGd\"            \n[5] \"Kitchen.QualTA\"             \"Gr.Liv.Area:Kitchen.QualFa\"\n[7] \"Gr.Liv.Area:Kitchen.QualGd\" \"Gr.Liv.Area:Kitchen.QualTA\"\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher–order interactions\nWe can create still higher–order interactions with the symbol ^:\n\ntable(ames_df$Garage.Qual)\n\n\n  Ex   Fa   Gd   Po   TA \n   3  101   19    3 2065 \n\ntable(ames_df$Exter.Qual)\n\n\n  Ex   Fa   Gd   TA \n  52   13  679 1447 \n\ntable(ames_df$Kitchen.Qual)\n\n\n  Ex   Fa   Gd   TA \n 108   42  844 1197 \n\n\n\nreg_higher &lt;- lm(\n  SalePrice ~ (Exter.Qual + Kitchen.Qual + Garage.Qual)^3,\n  ames_df)\n\n\n\nThree–way interactions have a lot of NA estimates.\n\ncoeff_names &lt;- names(coefficients(reg_higher))\ncbind(coeff_names[1:5], coeff_names[21:25], coeff_names[71:75])\n\n     [,1]             [,2]                        \n[1,] \"(Intercept)\"    \"Exter.QualFa:Garage.QualFa\"\n[2,] \"Exter.QualFa\"   \"Exter.QualGd:Garage.QualFa\"\n[3,] \"Exter.QualGd\"   \"Exter.QualTA:Garage.QualFa\"\n[4,] \"Exter.QualTA\"   \"Exter.QualFa:Garage.QualGd\"\n[5,] \"Kitchen.QualFa\" \"Exter.QualGd:Garage.QualGd\"\n     [,3]                                       \n[1,] \"Exter.QualTA:Kitchen.QualTA:Garage.QualPo\"\n[2,] \"Exter.QualFa:Kitchen.QualFa:Garage.QualTA\"\n[3,] \"Exter.QualGd:Kitchen.QualFa:Garage.QualTA\"\n[4,] \"Exter.QualTA:Kitchen.QualFa:Garage.QualTA\"\n[5,] \"Exter.QualFa:Kitchen.QualGd:Garage.QualTA\"\n\n\n\nprint(sum(is.na(coefficients(reg_higher))))\n\n[1] 48\n\n\nWhy are there so many NA coefficients?\n\n\nHigher–order Interaction Plot\nWhy do the higher–order interactions look like step functions?\n\n\nWarning in predict.lm(reg_higher, ames_df): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases"
  },
  {
    "objectID": "lectures/4_LSAsProjection.html",
    "href": "lectures/4_LSAsProjection.html",
    "title": "Least squares as a projection.",
    "section": "",
    "text": "Review / introduce some linear algebra\n\nOrthogonal bases\nEigendecompositions\nMatrix square roots\n\nDerive the OLS estimator from a geometric perspective\n\nDerive the form of projection operators without using vector calculus\nWrite the OLS estimator as a projection operator\nBriefly discuss some consequences of OLS as a projection"
  },
  {
    "objectID": "lectures/4_LSAsProjection.html#projection-basis-invariance",
    "href": "lectures/4_LSAsProjection.html#projection-basis-invariance",
    "title": "Least squares as a projection.",
    "section": "Projection basis invariance",
    "text": "Projection basis invariance\nSomething suspicious has happened, though — we started with a subspace, and got an answer that depends on the basis we chose. What if we had chosen a different orthonormal basis, \\(\\V\\), which spans \\(\\Sc\\)? Would we have gotten a different answer?\nIt turns out, no. Note that the fact that \\(\\V\\) spans \\(\\Sc\\) means that we can write\n\\[\n\\V = \\U \\U^\\trans \\V\n\\quad\\textrm{and}\\quad\n\\U = \\V \\V^\\trans \\U.\n\\]\nThat is, \\(\\V\\) is unchanged when projecting into the span of \\(\\U\\), and vice-versa. Using this, we can see that\n\\[\n\\begin{aligned}\n\\U \\U^\\trans \\vv ={}& \\U (\\V \\V^\\trans \\U)^\\trans \\vv \\\\\n={}& \\U \\U^\\trans \\V \\V^\\trans \\vv \\\\\n={}& (\\U \\U^\\trans \\V) \\V^\\trans \\vv \\\\\n={}& \\V \\V^\\trans \\vv.\n\\end{aligned}\n\\]\nSo the projection operator does not depend on what basis we choose."
  },
  {
    "objectID": "lectures/4_LSAsProjection.html#some-properties-of-projection-matrices",
    "href": "lectures/4_LSAsProjection.html#some-properties-of-projection-matrices",
    "title": "Least squares as a projection.",
    "section": "Some properties of projection matrices",
    "text": "Some properties of projection matrices\nThe projection matrix has some special properties that are worth mentioning.\n\nFirst, if \\(\\vv \\in \\Sc\\), then \\(\\proj{\\Sc}\\vv = \\vv\\) Similarly, \\(\\proj{\\Sc} \\vv = 0\\) is \\(\\vv \\in \\Sc^\\perp\\).\nSecond, \\(\\proj{\\Sc}\\) is symmetric, since \\(\\U \\U^\\trans = (\\U \\U^\\trans)^\\trans\\).\nThird, \\(\\proj{\\Sc}\\) is “idempotent”, meaning \\(\\proj{\\Sc} = \\proj{\\Sc} \\proj{\\Sc}\\).\n\n\n\n\n\n\n\nExercise\n\n\n\nProve these results.\n\n\nThese properties say that \\(\\proj{\\Sc}\\) has a special eigenstructure. In particular the fact that \\(\\proj{\\Sc} = \\proj{\\Sc} \\proj{\\Sc}\\) means the eigenvalues of \\(\\proj{\\Sc}\\) must be either zero or one, as we now show. Let \\(\\lambda\\) be an eigenvalue of \\(\\proj{\\Sc}\\) with eigenvector \\(\\vv\\). Then \\[\n\\lambda \\vv = \\proj{\\Sc} \\vv =  \\proj{\\Sc} \\ldots \\proj{\\Sc} \\vv = \\lambda^k \\vv.\n\\]\nTherefore we need \\(\\lambda^k = \\lambda\\) for any \\(k\\)! Only two values are possible: \\(\\lambda \\in \\{ 0, 1\\}\\). And we know that, for any basis vectors \\(\\av \\in \\Sc\\) and \\(\\av^\\perp \\in \\Sc^\\perp\\),\n\\[\n\\proj{\\Sc} \\av = \\av \\quad\\textrm{and}\\quad \\proj{\\Sc} \\av^\\perp = 0,\n\\]\nso the eigenvectors are precisely any vectors lying entirely in either \\(\\Sc\\) or \\(\\Sc^\\perp\\)."
  },
  {
    "objectID": "lectures/4_LSAsProjection.html#the-orthogonal-projection-matrix",
    "href": "lectures/4_LSAsProjection.html#the-orthogonal-projection-matrix",
    "title": "Least squares as a projection.",
    "section": "The orthogonal projection matrix",
    "text": "The orthogonal projection matrix\nOnce we have the projection matrix for \\(\\Sc\\), we can easily form the projection matrix onto the set’s orthogonal complement, \\(\\Sc^\\perp\\):\n\\[\n\\proj{\\Sc^\\perp} = \\id - \\proj{\\Sc}.\n\\]\nOne way to see this is via the identity between left and right inverses. By orthogonality,\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\n\\U &\n\\U^\\perp\n\\end{pmatrix}^\\trans\n\\begin{pmatrix}\n\\U &\n\\U^\\perp\n\\end{pmatrix} = \\id.\n\\end{aligned}\n\\]\nThen, by the equivalence between left and right inverses,\n\\[\n\\begin{aligned}\n\\id =\n\\begin{pmatrix}\n\\U &\n\\U^\\perp\n\\end{pmatrix}\n\\begin{pmatrix}\n\\U &\n\\U^\\perp\n\\end{pmatrix}^\\trans\n=\n\\begin{pmatrix}\n\\U &\n\\U^\\perp\n\\end{pmatrix}\n\\begin{pmatrix}\n\\U^\\trans \\\\\n(\\U^\\perp)^\\trans\n\\end{pmatrix}\n\\end{aligned}\n=\n\\U \\U^\\trans + \\U^\\perp (\\U^\\perp)^\\trans.\n\\]\nSolving gives\n\\[\n\\begin{aligned}\n\\proj{\\Sc^\\perp} = \\U^\\perp (\\U^\\perp)^\\trans = \\id - \\U \\U^\\trans = \\id - \\proj{\\Sc}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/4_LSAsProjection.html#square-roots-and-inverses-via-eigendecomposition",
    "href": "lectures/4_LSAsProjection.html#square-roots-and-inverses-via-eigendecomposition",
    "title": "Least squares as a projection.",
    "section": "Square roots and inverses via eigendecomposition",
    "text": "Square roots and inverses via eigendecomposition\nThe projection matrix was defined in terms of orthogonal bases. However, we would like it for spaces spanned by a generic, non-orthonormal set of vectors to solve our least squares problem, since in general \\(\\X\\) does not have orthonormal columns.\nTo bridge the gap, we can construct a linear operator that converts a set of vectors to an orthogonal basis.\nFirst, recall that a square, symmetric \\(P \\times P\\) matrix \\(\\A\\) can always be written in the form\n\\[\n\\begin{aligned}\n\\A &= \\U \\Lambda \\U^\\trans\n\\quad\\textrm{where}\\quad \\\\\n\\U^\\trans\\U &= \\id \\textrm{ ($\\U$ is orthogonal) and }\\\\\n\\Lammat &=\n\\begin{pmatrix}\n\\lambda_1 & 0  & \\ldots & 0 \\\\\n0 & \\lambda_2 & 0\\ldots & 0 \\\\\n\\vdots &  & \\ddots & \\vdots \\\\\n0 & 0 & \\ldots 0 & \\lambda_P  \\\\\n\\end{pmatrix}\n\\textrm{ is diagonal.}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nProve that the columns of \\(\\U\\) are the eigenvectors, and the \\(\\lambda_p\\) are the corresponding eigenvalues.\n\n\nA matrix is called “positive definite” if the eigenvalues are all strictly positive, and positive semi-definite if they are all non-zero.\n\n\n\n\n\n\nExercise\n\n\n\nProve that positive definiteness is equivalent to \\(\\vv^\\trans \\A \\vv &gt; 0\\), and semi-definiteness to \\(\\vv^\\trans \\A \\vv \\ge 0\\).\n\n\n\n\n\n\n\n\nExercise\n\n\n\nProve that if \\(\\A\\) is of the form \\(\\A = \\Q^\\trans \\Q\\) for some \\(\\Q\\), then \\(\\A\\) is positive semi-definite.\n\n\nNote that \\(\\A\\) is invertible if and only if its eigenvalues are non-zero. To see that nonzero eigenvalues are sufficient, we can write \\(\\A^{-1} = \\U \\Lambda^{-1} \\U^\\trans\\).\n\n\n\n\n\n\nExercise\n\n\n\nVerify that \\(\\A^{-1}\\) so defined is an inverse.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nProve that, if some \\(\\lambda_p = 0\\), then \\(\\A\\) is not invertible.\n\n\nFrom the eigendecomposition, one can define the square root of a symmetric positive semit-definite matrix \\(\\A\\): that is, a matrix \\(\\A^{1/2}\\) such that \\((\\A^{1/2})^\\trans \\A^{1/2} = \\A\\). First, define\n\\[\n\\Lammat^{1/2} :=\n\\begin{pmatrix}\n\\sqrt{\\lambda_1} & 0  & \\ldots & 0 \\\\\n0 & \\sqrt{\\lambda_2} & 0\\ldots & 0 \\\\\n\\vdots &  & \\ddots & \\vdots \\\\\n0 & 0 & \\ldots 0 & \\sqrt{\\lambda_P}  \\\\\n\\end{pmatrix}.\n\\]\nThen \\(\\U \\Lammat^{1/2} \\U^\\trans\\) is a square root of \\(\\A\\). Matrix square roots are not unique, but that will not matter — all that matters is that symmetric positive semi-definite matrices always have a square root. However, when I write \\(\\A^{1/2}\\), I will refer to the symmetric square root given in the formula above.\nFurther \\(\\A^{1/2}\\) itself is invertible if and only if \\(\\A\\) is invertible.\n\n\n\n\n\n\nExercise\n\n\n\nProve that \\(\\A^{1/2}\\) so defined is a square root.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFind a different matrix than the one given above that is also a square root.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nProve that any square root \\(\\A^{1/2}\\) is invertible."
  },
  {
    "objectID": "lectures/4_LSAsProjection.html#matrix-square-roots-for-orthonormalization",
    "href": "lectures/4_LSAsProjection.html#matrix-square-roots-for-orthonormalization",
    "title": "Least squares as a projection.",
    "section": "Matrix square roots for orthonormalization",
    "text": "Matrix square roots for orthonormalization\nIn our case, recall that we want to find an orthonormal basis of the column span of \\(\\X\\). Let us assume that \\(\\X\\) has full column rank. Observe that\n\n\\(\\X^\\trans \\X\\) is symmetric and positive definite.\n\\(\\Rightarrow\\) We can form the inverse square root \\((\\X^\\trans \\X)^{-1/2}\\).\n\\(\\Rightarrow\\) We can define \\(\\U := \\X (\\X^\\trans \\X)^{-1/2}\\).\n\nThen\n\n\\(\\U\\) is orthonormal and\n\\(\\U\\) has the same column span as \\(\\X\\).\n\nEffectively we have produced an orthonormal basis for \\(\\X\\).\n\n\n\n\n\n\nExercise\n\n\n\nProve that \\(\\U\\) is orthonormal and \\(\\U\\) has the same column span as \\(\\X\\).\n\n\nNote that right multiplying \\(\\X\\) effectively transforms each row of \\(\\X\\) — that is, the regressor \\(\\x_n\\) — into a new regressor \\(\\z_n = (\\X^\\trans \\X)^{-1/2} \\z_n\\) which has an identity covariance matrix.\nUsing our orthonormal basis, we can write the projection matrix as\n\\[\n\\proj{\\Sc_\\X} = \\U \\U^\\trans =\n\\X (\\X^\\trans \\X)^{-1/2} (\\X^\\trans \\X)^{-1/2} \\X^\\trans =\n\\X (\\X^\\trans \\X)^{-1} \\X^\\trans.\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nCheck directly that any vector that can be written in the form \\(\\vv = \\X \\beta\\) satisfies \\(\\vv = \\proj{\\Sc_\\X} \\vv\\), and that any \\(\\vv \\in \\Sc_\\X^\\perp\\) satisfies \\(\\proj{\\Sc_\\X} \\vv = \\zerov\\).\nOne might simply begin with the above formula and verify that it acts as a projection directly. However, doing so may seem mysterious or somehow lucky. Bulding the projection up as we have done shows where it comes from.\n\n\nFinally, we have derived the projection operator onto the span of \\(\\X\\):\n\\[\n\\begin{aligned}\n\\proj{\\Sc_\\X} \\Y ={}& \\X (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\n     ={} \\X \\betahat \\quad\\textrm{where}\\\\\n     \\betahat ={}& (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y.\n\\end{aligned}\n\\]\nWe give this fitted vector a special name:\n\\[\n\\Yhat := \\proj{\\Sc_\\X} \\Y = \\X \\betahat.\n\\]\nAdditionally, we have that the fitted residuals are given by\n\\[\n\\resvhat = \\Y - \\X \\betahat = \\id \\Y - \\proj{\\Sc_\\X} \\Y\n  = (\\id - \\proj{\\Sc_\\X})\\Y = \\proj{\\Sc_\\X^\\perp} \\Y.\n\\]\nNote that \\(\\Y = \\Yhat + \\resvhat = (\\proj{\\Sc_\\X}  + \\proj{\\Sc_\\X^\\perp}) \\Y\\). Also, our squared error loss is given by \\(\\norm{\\resvhat}_2^2 = \\norm{\\proj{\\Sc_\\X^\\perp} \\Y}_2^2\\), the squared magnitude of the component of \\(\\Y\\) not lying in \\(\\X\\)."
  },
  {
    "objectID": "lectures/3_LSAsLossMin.html",
    "href": "lectures/3_LSAsLossMin.html",
    "title": "Multilinear regression as loss minimization.",
    "section": "",
    "text": "\\(\\textcolor{white}{\\LaTeX}\\)"
  },
  {
    "objectID": "lectures/3_LSAsLossMin.html#the-sample-mean",
    "href": "lectures/3_LSAsLossMin.html#the-sample-mean",
    "title": "Multilinear regression as loss minimization.",
    "section": "The sample mean",
    "text": "The sample mean\n\n\n\n\n\n\nNotation\n\n\n\nI will use \\(\\onev\\) to denote a vector full of ones. Usually it will be an \\(N\\)–vector, but sometimes its dimension will just be implicit. Similarly, \\(\\zerov\\) is a vector of zeros.\n\n\nWe showed earlier that the sample mean is a special case of the regression \\(\\y_n \\sim 1 \\cdot \\beta\\). This can be expressed in matrix notation by taking \\(\\X = \\onev\\) as a \\(N\\times 1\\) vector. We then have\n\\[\n\\X^\\trans \\X = \\onev^\\trans \\onev = \\sumn 1 \\cdot 1 = N,\n\\]\nso \\(\\X^\\trans \\X\\) is invertible as long as \\(N &gt; 0\\) (i.e., if you have at least one datapoint), with \\((\\X^\\trans \\X)^{-1} = 1/N\\). We also have\n\\[\n\\X^\\trans \\Y = \\onev^\\trans \\Y = \\sumn 1 \\cdot \\y_n = N \\ybar,\n\\]\nand so\n\\[\n\\betahat = (\\X^\\trans \\X)^{-1}  \\X^\\trans \\Y = (\\onev^\\trans \\onev)^{-1} \\onev^\\trans \\Y = \\frac{N \\ybar}{N} = \\ybar,\n\\]\nas expected."
  },
  {
    "objectID": "lectures/3_LSAsLossMin.html#a-single-regressor",
    "href": "lectures/3_LSAsLossMin.html#a-single-regressor",
    "title": "Multilinear regression as loss minimization.",
    "section": "A single regressor",
    "text": "A single regressor\nSuppose that we regress \\(\\y_n \\sim \\x_n\\) where \\(\\x_n\\) is a scalar. Let’s suppose that \\(\\expect{\\x_n} = 0\\) and \\(\\var{\\x_n} = \\sigma^2 &gt; 0\\). We have\n\\[\n\\X = \\begin{pmatrix}\n\\x_1 \\\\\n\\x_2 \\\\\n\\vdots\\\\\n\\x_N\n\\end{pmatrix}\n\\]\nso\n\\[\n\\X^\\trans \\X = \\sumn \\x_n^2.\n\\]\nDepending on the distribution of \\(\\x_n\\), it may be possible for \\(\\X^\\trans \\X\\) to be non–invertible!\n\n\n\n\n\n\nExercise\n\n\n\nProduce a distribution for \\(\\x_n\\) where \\(\\X^\\trans \\X\\) is non–invertible with positive probability for any \\(N\\).\n\n\nHowever, as \\(N \\rightarrow \\infty\\), \\(\\frac{1}{N} \\X^\\trans \\X \\rightarrow \\sigma^2\\) by the LLN, and since \\(\\sigma^2 &gt; 0\\), \\(\\frac{1}{N} \\X^\\trans \\X\\) will be invertible with probability approaching one as \\(N\\) goes to infinity."
  },
  {
    "objectID": "lectures/3_LSAsLossMin.html#onehot-encodings",
    "href": "lectures/3_LSAsLossMin.html#onehot-encodings",
    "title": "Multilinear regression as loss minimization.",
    "section": "One–hot encodings",
    "text": "One–hot encodings\nWe discussed one-hot encodings in the context of the Ames housing data. Suppose we have a columns \\(k_n \\in \\{ g, e\\}\\) indicating whether a kitchen is “good” or “excellent”. A one–hot encoding of this categorical variable is given by\n\\[\n\\begin{aligned}\n\\x_{ng} =\n\\begin{cases}\n1 & \\textrm{ if }k_n = g \\\\\n0 & \\textrm{ if }k_n \\ne g \\\\\n\\end{cases}\n&&\n\\x_{ne} =\n\\begin{cases}\n1 & \\textrm{ if }k_n = e \\\\\n0 & \\textrm{ if }k_n \\ne e \\\\\n\\end{cases}\n\\end{aligned}.\n\\]\nWe can then regress \\(\\y_n \\sim \\beta_g \\x_{ng} + \\beta_e \\x_{ne} = \\x_n^\\trans \\betav\\). The corresponding \\(\\X\\) matrix might look like\n\\[\n\\begin{aligned}\n\\mybold{k} =\n\\begin{pmatrix}\ng \\\\\ne \\\\\ng \\\\\ng \\\\\n\\vdots\n\\end{pmatrix}\n&&\n\\X = (\\xv_g \\quad \\xv_e) =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 0 \\\\\n1 & 0 \\\\\n\\vdots\n\\end{pmatrix}\n\\end{aligned}\n\\]\nNote that \\(\\xv_g^\\trans \\xv_g\\) is just the number of entries with \\(k_n = g\\), and \\(\\xv_g^\\trans \\xv_e = 0\\) because a kitchen is either good or excellent but never both.\nWe then have\n\\[\n\\X^\\trans \\X =\n\\begin{pmatrix}\n\\xv_g^\\trans \\xv_g  & \\xv_g^\\trans \\xv_e \\\\\n\\xv_e^\\trans \\xv_g  & \\xv_e^\\trans \\xv_e \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\nN_g  & 0 \\\\\n0  & N_e \\\\\n\\end{pmatrix}.\n\\]\nThen \\(\\X^\\trans \\X\\) is invertible as long as \\(N_g &gt; 0\\) and \\(N_e &gt; 0\\), that is, as long as we have at least one observation of each kitchen type, and\n\\[\n\\left(\\X^\\trans \\X\\right)^{-1} =\n\\begin{pmatrix}\n\\frac{1}{N_g}  & 0 \\\\\n0  & \\frac{1}{N_e} \\\\\n\\end{pmatrix}.\n\\]\nSimilarly, \\(\\xv_g^\\trans \\Y\\) is just the sum of entries of \\(\\Y\\) where \\(k_n = g\\), with the analogous conclusion for \\(\\xv_e\\). From this we recover the result that\n\\[\n\\betavhat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\n=\n\\begin{pmatrix}\n\\frac{1}{N_g}  & 0 \\\\\n0  & \\frac{1}{N_e} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\sum_{n: k_n=g} \\y_n \\\\\n\\sum_{n: k_n=e} \\y_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{N_g} \\sum_{n: k_n=g} \\y_n \\\\\n\\frac{1}{N_e} \\sum_{n: k_n=e} \\y_n\n\\end{pmatrix}.\n\\]\nIf we let \\(\\ybar_e\\) and \\(\\ybar_g\\) denote the sample means within each group, we have shows that \\(\\betahat_g = \\ybar_g\\) and \\(\\betahat_e = \\ybar_e\\), as we proved before without using the matrix formulation."
  },
  {
    "objectID": "lectures/3_LSAsLossMin.html#onehot-encodings-and-constants",
    "href": "lectures/3_LSAsLossMin.html#onehot-encodings-and-constants",
    "title": "Multilinear regression as loss minimization.",
    "section": "One–hot encodings and constants",
    "text": "One–hot encodings and constants\nRecall in the Ames housing data, we ran the following two regressions:\n\\[\n\\begin{aligned}\n\\y_n \\sim{}& \\beta_e \\x_{ne} + \\beta_g \\x_{ng}  \\\\\n\\y_n \\sim{}& \\gamma_0  + \\gamma_g \\x_{ng} + \\res_n = \\z_n^\\trans \\gammav,\n\\end{aligned}\n\\] where I take \\(\\gammav = (\\gamma_0, \\gamma_g)^\\trans\\) and \\(\\z_n = (1, \\x_{ng})^\\trans\\).\nWe found using R that the best fits were given by\n\\[\n\\begin{aligned}\n\\betahat_e =& \\ybar_e  & \\betahat_g =& \\ybar_g \\\\\n\\gammahat_0 =& \\ybar_e  & \\gammahat_g =& \\ybar_g - \\ybar_e \\\\\n\\end{aligned}\n\\]\nWe can compute the latter by constructing the \\(\\Z\\) matrix whose rows are \\(\\z_n^\\trans\\). (We use \\(\\Z\\) to differentiate the \\(\\X\\) matrix from the previous example.) Using similar reasoning to the one–hot encoding, we see that\n\\[\n\\Z^\\trans \\Z =\n\\begin{pmatrix}\nN & N_g \\\\\nN_g & N_g\n\\end{pmatrix}.\n\\]\nThis is invertible as long as \\(N_g \\ne N\\), i.e., as long as there is at least one \\(k_n = e\\). We have\n\\[\n(\\Z^\\trans \\Z)^{-1} =\n\\frac{1}{N_g (N - N_g)}\n\\begin{pmatrix}\nN_g & -N_g \\\\\n-N_g & N\n\\end{pmatrix}\n\\quad\\textrm{and}\\quad\n\\Z^\\trans \\Y =\n\\begin{pmatrix}\n\\sumn \\y_n \\\\\n\\sum_{n: k_n=g} \\y_n \\\\\n\\end{pmatrix}\n\\]\nIt is possible (but a little tedious) to prove \\(\\gammahat_0 = \\ybar_e\\) and \\(\\gammahat_g = \\ybar_g - \\ybar_e\\) using these formulas. But an easier way to see it is as follows.\nNote that \\(\\x_{ne} + \\x_{ng} = 1\\). That means we can always re-write the regression with a constant as\n\\[\n\\y_n \\sim \\gamma_0 + \\gamma_g \\x_{ng} = \\gamma_0 (\\x_{ne} + \\x_{ng}) + \\gamma_g \\x_{ng} =\n\\gamma_0 \\x_{ne} + (\\gamma_0 + \\gamma_g) \\x_{ng}.\n\\]\nNow, we already know from the one–hot encoding case that the sum of squared residuals is minimized by setting \\(\\gammahat_0 = \\ybar_e\\) and \\(\\gammahat_0 + \\gammahat_g = \\ybar_g\\). We can then solve for \\(\\gammahat_g = \\ybar_g - \\ybar_e\\), as expected.\nThis is case where we have two regressions whose regressors are invertible linear combinations of one another:\n\\[\n\\zv_n =\n\\begin{pmatrix}\n1 \\\\\n\\x_{ng}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\x_{ne} + \\x_{ng} \\\\\n\\x_{ng}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\x_{ng} \\\\\n\\x_{ne}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0\\\\\n\\end{pmatrix}\n\\xv_n.\n\\]\nIt follows that if you can acheive a least squares fit with \\(\\xv_n^\\trans \\betavhat\\), you can achieve exactly the same fit with\n\\[\n\\betavhat^\\trans \\xv_n =  \n\\betavhat^\\trans\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0\\\\\n\\end{pmatrix}^{-1} \\zv_n,\n\\]\nwhich can be achieved by taking\n\\[\n\\gammavhat^\\trans =\n\\betavhat^\\trans\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0\\\\\n\\end{pmatrix}^{-1} \\Rightarrow\n\\gammavhat =\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0\\\\\n\\end{pmatrix}^{-T} \\betavhat\n=\n\\frac{1}{-1}\n\\begin{pmatrix}\n0 & -1 \\\\\n-1 & 1\\\\\n\\end{pmatrix} \\betavhat\n=\n\\begin{pmatrix}\n\\betahat_2 \\\\\n\\betahat_1 - \\betahat_2 \\\\\n\\end{pmatrix},\n\\]\nexactly as expected.\nWe will see this is an entirely general result: when regressions are related by invertible linear transformations of regressors, the fit does not change, but the optimal coefficients are linear transforms of one another."
  },
  {
    "objectID": "lectures/3_LSAsLossMin.html#redundant-regressors",
    "href": "lectures/3_LSAsLossMin.html#redundant-regressors",
    "title": "Multilinear regression as loss minimization.",
    "section": "Redundant regressors",
    "text": "Redundant regressors\nSuppose we run the (silly) regression \\(\\y \\sim \\alpha \\cdot 1 + \\gamma \\cdot 3 + \\res_n\\). That is, we regress on both the constant \\(1\\) and the constant \\(3\\). We have\n\\[\n\\X =\n\\begin{pmatrix}\n1 & 3 \\\\\n1 & 3 \\\\\n1 & 3 \\\\\n\\vdots\n\\end{pmatrix}\n=\n(\\onev \\quad 3 \\onev)\n\\]\nand so\n\\[\n\\X^\\trans \\X =\n\\begin{pmatrix}\n\\onev^\\trans \\onev & 3 \\onev^\\trans \\onev \\\\\n3 \\onev^\\trans \\onev & 9 \\onev^\\trans \\onev \\\\\n\\end{pmatrix}\n=\nN \\begin{pmatrix}\n1 & 3  \\\\\n3 & 9  \\\\\n\\end{pmatrix}\n\\]\nThis is not invertible (the second row is \\(3\\) times the first, and the determinant is \\(9 - 3 \\cdot 3 = 0\\)). So \\(\\betavhat\\) is not defined. What went wrong?\nOne way to see this is to define \\(\\beta = \\alpha + 3 \\gamma\\) and write\n\\[\n\\y_n = (\\alpha + 3 \\gamma) + \\res_n = \\beta + \\res_n.\n\\]\nThere is obviously only one \\(\\betahat\\) that minimizes \\(\\sumn \\res_n^2\\), \\(\\betahat = \\ybar\\). But there are an infinite set of choices for \\(\\alpha\\) and \\(\\gamma\\) satisfying\n\\[\n\\alpha + 3 \\gamma = \\betahat = \\ybar.\n\\]\nSpecifically, for any value of \\(\\gamma\\) we can take \\(\\alpha = \\ybar - 3 \\gamma\\), leaving \\(\\beta\\) unchanged. All of these choices for \\(\\alpha,\\gamma\\) acheive the same \\(\\sumn \\res_n^2\\)! So the least squares criterion cannot distinguish among them.\nIn general, this is what it means for \\(\\X^\\trans \\X\\) to be non–invertibile. It happens precisely when there are redundant regressors, and many regression coefficients that result in the same fit."
  },
  {
    "objectID": "lectures/3_LSAsLossMin.html#redundant-regressors-and-zero-eigenvalues",
    "href": "lectures/3_LSAsLossMin.html#redundant-regressors-and-zero-eigenvalues",
    "title": "Multilinear regression as loss minimization.",
    "section": "Redundant regressors and zero eigenvalues",
    "text": "Redundant regressors and zero eigenvalues\nIn fact, \\(\\X^\\trans \\X\\) is invertible precisely when \\(\\X^\\trans \\X\\) has a zero eigenvalue. In the preceding example, we can see that\n\\[\n\\X^\\trans \\X\n\\begin{pmatrix}\n3 \\\\ -1\n\\end{pmatrix}\n=\nN \\begin{pmatrix}\n1 & 3  \\\\\n3 & 9  \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n3 \\\\ -1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\ 0\n\\end{pmatrix},\n\\]\nso \\((3, -1)^\\trans\\) is a zero eigenvector. (In general you might find this by numerical eigenvalue decomposition, but in this case you can just guess the zero eigenvalue.)\nGoing back to Equation 6, we see that this means that\n\\[\n\\X^\\trans \\Y =\n(\\X^\\trans \\X)\n\\begin{pmatrix}\n\\alphahat \\\\ \\gammahat\n\\end{pmatrix}\n=\nN \\begin{pmatrix}\n1 & 3  \\\\\n3 & 9  \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\alphahat \\\\ \\gammahat\n\\end{pmatrix}\n=\nN \\begin{pmatrix}\n1 & 3  \\\\\n3 & 9  \\\\\n\\end{pmatrix}\n\\left(\n\\begin{pmatrix}\n\\alphahat \\\\ \\gammahat\n\\end{pmatrix}\n+ C\n\\begin{pmatrix}\n3 \\\\ -1\n\\end{pmatrix}\n\\right)\n\\]\nfor any value of \\(C\\). This means there are an infinite set of “optimal” values, all of which set the gradient of the loss to zero, and all of which have the same value of the loss function (i.e. acheive the same fit). And you can check that these family of values are exactly the ones that satisfy \\(\\alpha + 3 \\gamma = \\betahat = \\ybar\\), since\n\\[\n\\alpha + 3 \\gamma =\n\\begin{pmatrix}\n1 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\n\\alpha \\\\ \\gamma\n\\end{pmatrix}\n\\quad\\quad\\textrm{and}\\quad\\quad\n\\begin{pmatrix}\n1 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\n3 \\\\ -1\n\\end{pmatrix} = 0.\n\\]\nSoon, we will see that this is a general result: when \\(\\X^\\trans \\X\\) is not invertible, that means there are many equivalent least squares fits, all characterized precisely by the zero eigenvectors of \\(\\X^\\trans \\X\\)."
  },
  {
    "objectID": "lectures/3_LSAsLossMin.html#zero-variance-regressors",
    "href": "lectures/3_LSAsLossMin.html#zero-variance-regressors",
    "title": "Multilinear regression as loss minimization.",
    "section": "Zero variance regressors",
    "text": "Zero variance regressors\nAn example of redundant regressors occurs when the sample variance of \\(\\x_n\\) is zero and a constant is included in the regression. Specifically, suppose that \\(\\overline{xx} - \\overline{x}^2 = 0\\).\n\n\n\n\n\n\nExercise\n\n\n\nProve that \\(\\overline{xx} - \\overline{x}^2 = 0\\) means \\(\\x_n\\) is a constant with \\(\\x_n = \\xbar\\). Hint: look at the sample variance of \\(\\x_n\\).\n\n\nLet’s regress \\(\\y_n \\sim \\beta_1 + \\beta_2 \\x_n\\).\nFor simplicity, let’s take \\(\\x_n = 3\\). In that case we can rewrite our estimating equation as\n\\[\n\\y_n = \\beta_1 + \\beta_2 \\x_n + \\res_n\n     = (\\beta_1 + \\beta_2 \\xbar) + \\res_n.\n\\]\nWe’re thus in the previous setting with \\(\\xbar\\) in place of the number \\(3\\)."
  },
  {
    "objectID": "lectures/3_LSAsLossMin.html#orthogonal-regressors",
    "href": "lectures/3_LSAsLossMin.html#orthogonal-regressors",
    "title": "Multilinear regression as loss minimization.",
    "section": "Orthogonal regressors",
    "text": "Orthogonal regressors\nSuppose we have regressors such that the columns of \\(\\X\\) are orthonormal. This seems strange at first, since we usually specify the rows of the regressors, not the columns. But in fact we have seen a near–example with one–hot encodings, which are defined row–wise, but which produce orthogonal column vectors in \\(\\X\\). If we divide a one–hot encoding by the square root of the number of ones in the whole dataset, we produce an normal column vector.\nIf \\(\\X\\) has orthonormal columns, then \\(\\X^\\trans \\X = \\id\\), the identity matrix, and so\n\\[\n\\betavhat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y = \\X^\\trans \\Y.\n\\]\nThis is of course the same answer we would have gotten if we had tried to write \\(\\Y\\) in the basis of the column vectors of \\(\\X\\):\n\\[\n\\begin{aligned}\n\\Y ={}& \\betahat_1 \\X_{\\cdot 1} + \\ldots + \\betahat_P \\X_{\\cdot P} = \\X \\betavhat \\Rightarrow \\\\\n\\X^\\trans \\Y ={}& \\X^\\trans \\X \\betavhat = \\betavhat\n\\end{aligned}\n\\]\nThis regression is particularly simple — each component of \\(\\betavhat\\) depends only on its corresponding column of \\(\\X\\).\nNote that if each entry of \\(\\xv_n\\) is mean zero, unit variance, and uncorrelated with the other entries, then \\(\\frac{1}{N} \\X^\\trans \\X \\rightarrow \\id\\) by the LLN. Such a regressor matrix is not typically orthogonal for any particular \\(N\\), but it approaches orthogonality as \\(N\\) grows."
  },
  {
    "objectID": "lectures/1_SampleMeans.html",
    "href": "lectures/1_SampleMeans.html",
    "title": "Sample means",
    "section": "",
    "text": "\\(\\textcolor{white}{\\LaTeX}\\)"
  },
  {
    "objectID": "lectures/1_SampleMeans.html#the-law-of-large-numbers",
    "href": "lectures/1_SampleMeans.html#the-law-of-large-numbers",
    "title": "Sample means",
    "section": "The law of large numbers",
    "text": "The law of large numbers\nTaking the conceit for granted, we want to know \\(\\mu\\), which is just a number, but we observe \\(\\ybar\\), which we are imagining is a random variable (since it is a function of \\(\\y_1, \\ldots, \\y_N\\), which are random). In what sense does \\(\\ybar\\) tell us anything about \\(\\mu\\)? As long as \\(\\var{y} &lt; \\infty\\), the most important property \\(\\ybar\\) has is “consistency”:\n\\[\n\\ybar \\rightarrow \\mu \\quad\\textrm{as }N \\rightarrow \\infty.\n\\]\nThis follows by the “law of large numbers,” of LLN, which will be an important tool in discussing the properties of linear regression.\n\n\n\n\n\n\nNote\n\n\n\nNote that the left hand side is a random variable, but the right hand side is a (non–random) constant. We won’t deal with this carefully in this class, but formally we mean something like\n\\[\n\\prob{\\abs{\\ybar - \\mu} &gt; \\varepsilon} \\rightarrow 0\n\\quad\\textrm{as }N \\rightarrow \\infty, \\textrm{ for any }\\varepsilon &gt; 0.\n\\]\nThis is a one of a (class of) deterministic limits that can apply to random variables. Specifically, this is called “convergence in probability”. There are in fact many different modes of probabilistic convergence, and their study is a very interesting (but more advanced) topic. (In this case, it happens that the LLN also applies with “almost sure” convergence.)\n\n\n\n\n\n\n\n\nNote\n\n\n\nOf course, there are many estimators besides \\(\\ybar\\) which are also consistent. Here are a few:\n\\[\n\\begin{aligned}\n\\frac{1}{N - 1} \\sumn \\y_n\n\\quad\\quad\\quad\n\\ybar + \\frac{1}{N}\n\\quad\\quad\\quad\n\\exp(1/N) \\ybar\n\\quad\\quad\\quad\n\\frac{1}{\\lfloor N/2 \\rfloor} \\sum_{n=1}^{\\lfloor N / 2 \\rfloor} \\y_n,\n\\end{aligned}\n\\]\nand so on. Why you would choose one over another is a major topic in statistics which we will touch on only lightly in this course."
  },
  {
    "objectID": "lectures/1_SampleMeans.html#the-central-limit-theorem",
    "href": "lectures/1_SampleMeans.html#the-central-limit-theorem",
    "title": "Sample means",
    "section": "The central limit theorem",
    "text": "The central limit theorem\nHow close is \\(\\ybar\\) to \\(\\mu\\) for any particular \\(N\\)? It’s impossible to know precisely, since we don’t actually know the distribution of \\(\\y\\) — we don’t even know its mean. But for large \\(N\\), we can take advantage of another asymptotic result, the central limit theorem, or CLT. Suppose that we know \\(\\sigma := \\sqrt{\\var{\\y}}\\). Then\n\\[\n\\frac{1}{\\sqrt{N}} \\sumn \\frac{\\y_n - \\mu}{\\sigma} \\rightarrow \\gauss{0, 1}\n\\quad\\textrm{as }N \\rightarrow \\infty.\n\\]\nThe CLT will also be a key tool in studying the properties of linear regression.\n\n\n\n\n\n\nNote\n\n\n\nNote that the left hand side is a random variable and the right hand side is also a random variable. Here, we mean \\[\n\\abs{\n  \\prob{\\frac{1}{\\sqrt{N}} \\sumn \\frac{\\y_n - \\mu}{\\sigma} \\le z} -\n  \\prob{\\gauss{0, 1} \\le z}\n} \\rightarrow 0\n\\quad\\textrm{as }N \\rightarrow \\infty, \\textrm{ for any }z.\n\\]\nThis is called “convergence in distribution.” Note that it’s the same as saying the distribution function of the left hand side converges pointwise to the distribution function of the right hand side. Again, we won’t be too concerned with modes of probabilistic convergence in this class."
  },
  {
    "objectID": "lectures/1_SampleMeans.html#confidence-intervals",
    "href": "lectures/1_SampleMeans.html#confidence-intervals",
    "title": "Sample means",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nUsing the CLT, we can say things like the following. Suppose that we choose \\(\\z_\\alpha\\) so that \\(\\prob{-\\z_\\alpha \\le \\gauss{0, 1} \\le \\z_\\alpha} = 0.95\\). Then by the CLT,\n\\[\n\\begin{aligned}\n0.95 ={}& \\prob{-\\z_\\alpha \\le \\gauss{0, 1} \\le \\z_\\alpha}\\\\\n\\approx{}& \\prob{-\\z_\\alpha \\le  \\frac{1}{\\sqrt{N}} \\sumn \\frac{\\y_n - \\mu}{\\sigma} \\le \\z_\\alpha} & \\textrm{(by the CLT applied twice)}\\\\\n=& \\prob{-\\sigma \\z_\\alpha \\le   \\frac{1}{\\sqrt{N}} \\sumn (\\y_n - \\mu) \\le \\sigma \\z_\\alpha} & \\textrm{(algebra)}\\\\\n=& \\prob{\n  - \\frac{\\sigma}{\\sqrt{N}} \\z_\\alpha \\le\n  \\frac{1}{N} \\sumn (\\y_n - \\mu) \\le\n  \\frac{\\sigma}{\\sqrt{N}} \\z_\\alpha\n  } & \\textrm{(algebra)}\\\\\n=& \\prob{ -   \\frac{\\sigma}{\\sqrt{N}} \\z_\\alpha \\le  \\ybar - \\mu \\le   \\frac{\\sigma}{\\sqrt{N}} \\z_\\alpha} & \\textrm{(algebra)}\\\\\n=& \\prob{ \\ybar - \\frac{\\sigma}{\\sqrt{N}} \\z_\\alpha \\le  \\mu \\le  \\ybar + \\frac{\\sigma}{\\sqrt{N}} \\z_\\alpha}. & \\textrm{(algebra)}\\\\\n\\end{aligned}\n\\]\nThis means that, whatever \\(\\mu\\) is, then with 95% probability (under IID random sampling), it lies in the interval \\(\\ybar \\pm \\frac{\\sigma}{\\sqrt{N}} \\z_\\alpha\\). That is, we have constructed a 95% two–sided confidence interval for the unknown \\(\\mu\\), which in accounts for the random variability in \\(\\ybar\\) as a measure of \\(\\mu\\).\nIn this case, estimating \\(\\sigma\\) using the sample variance estimator \\(\\meann (\\y_n - \\ybar)^2\\), the confidence interval is \\([26.62, 32.1]\\).\nIt’s worth reflecting on what this does and does not mean. For example, does this mean that, if I don’t change the exam or syllabus this year, that we are extremely unlikely to see a mean exam score above \\(33\\) points? Certainly not; this is an underestimate of any reasonable notion of subjective uncertainty in this year’s exam scores. But one might think of it as a rough lower bound on the subjective uncertainty, since our model holds many things constant that cannot plausibly be held constant in reality."
  },
  {
    "objectID": "datasets/data.html",
    "href": "datasets/data.html",
    "title": "Datasets",
    "section": "",
    "text": "The following datasets are used in lectures, homework, and labs.",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "datasets/data.html#ames-housing-dataset",
    "href": "datasets/data.html#ames-housing-dataset",
    "title": "Datasets",
    "section": "Ames Housing dataset",
    "text": "Ames Housing dataset\nThis dataset can be downloaded from the github repo of Veridical Data Science by Yu and Barter.\n\nLink to csv\nSource link (downloaded Aug 2024)",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "datasets/data.html#aluminum-dataset",
    "href": "datasets/data.html#aluminum-dataset",
    "title": "Datasets",
    "section": "Aluminum dataset",
    "text": "Aluminum dataset\nDataset containing stress-strain curves for commercially available aluminum samples at varying tempertures. The data accompanies B.S. Aakash, JohnPatrick Connors, Michael D. Shields, Variability in the thermo-mechanical behavior of structural aluminum, Thin-Walled Structures, Volume 144, 2019, 106122, ISSN 0263-8231. (link)\nPaper abstract: The nominal performance of structural aluminum alloys at elevated temperature has been thoroughly investigated in the past. Although it is well known that the performance of a given material specimen will differ from the nominal behavior, the extent of this variability has not been quantitied to date. This limits the ability to perform reliability and performance-based design and analysis for aluminum structures subjected to high temperatures (e.g. in structural fire engineering). This work presents an experimental investigation of the variability in the stress-strain behavior of AA 6061-T651 (as a model ductile aluminum alloy). We performed steady-state tensile tests on nine different batches of nominally identical material sourced from different suppliers/manufacturers at six different temperatures (20 °C, 100 °C, 150 °C, 200 °C, 250 °C, and 300 °C) under two different geometries to induce uniaxial tension and plane strain stress states in the gauge section. The results are investigated statistically to illustrate variability in the salient features of the stress-strain behavior of the material ranging from nonlinear elastic behavior to strain localization and ductile fracture. Some observations on material performance and its variability are made along the way. Overall, it is illustrated that variations between batches of material can be quite large and – especially as it relates to strain localization, necking, and material failure – variations can be very large even within a fixed batch of material. To encourage data of this nature to be expanded and integrated into research and practice to improve structural design and investigations, the full searchable dataset are publicly available with experimental details published concurrently through Data in Brief.\n\nLink to csv\nSource link (downloaded Dec 2023)",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "datasets/data.html#bodyfat-dataset",
    "href": "datasets/data.html#bodyfat-dataset",
    "title": "Datasets",
    "section": "Bodyfat dataset",
    "text": "Bodyfat dataset\nBodyfat and other physical measurements on a number of individuals.\nMeasurement standards are apparently those listed in Benhke and Wilmore (1974), pp. 45-48 where, for instance, the abdomen 2 circumference is measured “laterally, at the level of the iliac crests, and anteriorly, at the umbilicus”.\nThese data are used to produce the predictive equations for lean body weight given in the abstract “Generalized body composition prediction equation for men using simple measurement techniques”, K.W. Penrose, A.G. Nelson, A.G. Fisher, FACSM, Human Performance Research Center, Brigham Young University, Provo, Utah 84602 as listed in Medicine and Science in Sports and Exercise, vol. 17, no. 2, April 1985, p. 189.\n\nLink to csv\nSource link (downloaded Dec 2023)",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "datasets/data.html#spotify-dataset",
    "href": "datasets/data.html#spotify-dataset",
    "title": "Datasets",
    "section": "Spotify dataset",
    "text": "Spotify dataset\nThis dataset consists of roughly 30,000 Songs from the Spotify API with black-box machine learning quantifications of musical features. No guarantees are made on how the tracks were sampled.\n\nLink to csv\nSource link (downloaded Dec 2023)",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Calendar",
    "section": "",
    "text": "We can just embed the iframe html:"
  },
  {
    "objectID": "assignments/hw3.html",
    "href": "assignments/hw3.html",
    "title": "STAT151A Homework 3.",
    "section": "",
    "text": "This homework is due on Gradescope on Friday October 11th at 9pm.\n\n1 Interpretation of transforms of the response\nSuppose I have data of the where \\(n = 1, \\ldots N\\) indexes households, \\(y_n\\) is the expenditure on food in a time period (so that \\(y_n &gt; 0\\) for all \\(n\\)). Suppose that households are randomly selected to either receive food stamps (for which \\(x_n = 1\\)) or to not receive food stamps (for which \\(x_n = 0\\)). Also, suppose we measure household income \\(z_n\\).\n(a)\nSuppose we regress \\(y_n \\sim \\beta_0 + \\beta_1 x_n + \\beta_2 z_n\\). Let \\(\\hat{f}(x_, z) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x+ \\hat{\\beta}_2 z\\) denote the fit.\nUsing this regression, we might estimate the effect of food stamps on food expenditure by \\(\\hat{f}(1, z) - \\hat{f}(0, z)\\). How does this estimate depend on \\(z\\)?\n(b)\nSuppose we regress \\(\\log y_n \\sim \\gamma_0 + \\gamma_1 x_n + \\gamma_2 z_n\\). Let \\(\\hat{g}(x_, z) = \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x+ \\hat{\\gamma}_2 z\\).\nUsing this regression, we might estimate the effect of food stamps on food expenditure by \\(\\exp(\\hat{g}(1, z)) - \\exp(\\hat{g}(0, z))\\). How does this estimate depend on \\(z\\)?\n(c)\nThe regressions in (a) and (b) make different implicit assumptions about how food stamps affect consumption for a particular household. State these assumptions in ordinary language.\nSolutions\n(a) It doesn’t depend on \\(z\\):\n\\[\n\\hat{f}(1, z) - \\hat{f}(0, z) =\n\\hat{\\beta}_0 + \\hat{\\beta}_1 + \\hat{\\beta}_2 z- (\\hat{\\beta}_0 + \\hat{\\beta}_2 z) = \\hat{\\beta}_1.\n\\]\n(b) It is proportional to \\(\\exp(\\hat{\\gamma}_2 z)\\):\n\\[\n\\exp(\\hat{g}(1, z)) - \\exp(\\hat{g}(0, z)) =\n\\exp(\\hat{\\gamma}_0 + \\hat{\\gamma}_1 + \\hat{\\gamma}_2 z) -\n\\exp(\\hat{\\gamma}_0 + \\hat{\\gamma}_2 z) =\n(\\exp(\\hat{\\gamma}_1) - 1) \\exp(\\hat{\\gamma}_0) \\exp(\\hat{\\gamma}_2 z)\n\\]\n(c)\nIn the first model, food stamps have an additive effect on expenditure. In the second model (with the log response), food stamps have a multiplicative effect on expenditure.\n\n\n2 Fit and regressors\nGiven a regression on \\(\\boldsymbol{X}\\) with \\(P\\) regressors and \\(N\\) data points, and the corresponding \\(\\boldsymbol{Y}\\), \\(\\hat{\\boldsymbol{Y}}\\), and \\(\\hat{\\varepsilon}\\), define the following quantities: \\[\n\\begin{aligned}\nRSS :={}& \\hat{\\varepsilon}^\\intercal\\hat{\\varepsilon}& \\textrm{(Residual sum of squares)}\\\\\nTSS :={}& \\boldsymbol{Y}^\\intercal\\boldsymbol{Y}& \\textrm{(Total sum of squares)}\\\\\nESS :={}& \\hat{\\boldsymbol{Y}}^\\intercal\\hat{\\boldsymbol{Y}}& \\textrm{(Explained sum of squares)}\\\\\nR^2 :={}& \\frac{ESS}{TSS}.\n\\end{aligned}\n\\]\n\nProve that \\(RSS + ESS = TSS\\).\nExpress \\(R^2\\) in terms of \\(TSS\\) and \\(RSS\\).\nWhat is \\(R^2\\) when we include no regressors? (\\(P = 0\\))\nWhat is \\(R^2\\) when we include \\(N\\) linearly independent regressors? (\\(P=N\\))\nCan \\(R^2\\) ever decrease when we add a regressor? If so, how?\nCan \\(R^2\\) ever stay the same when we add a regressor? If so, how?\nCan \\(R^2\\) ever increase when we add a regressor? If so, how?\nDoes a high \\(R^2\\) mean the regression is useful? (You may argue by example.)\nDoes a low \\(R^2\\) mean the regression is not useful? (You may argue by example.)\n\nSolutions:\n\nThis follows from \\(\\hat{\\boldsymbol{Y}}^\\intercal\\hat{\\varepsilon}= \\boldsymbol{0}\\).\n\\(R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\\)\n\\(R^2 = 0\\) when we include no regressors\n\\(R^2 = 1\\) when we include \\(N\\) linearly independent regressors?\nNo, it cannot, since you project onto the same or larger subspace.\nYes, if you add a regressor column that is colinear with the existing columns.\nYes, if you add a linearly independent regressor column.\nNo, you might overfit.\nNo, you might have low signal to noise ratio.\n\n\n\n3 Prediction in the bodyfat example\nThis exercise will use the bodyfat example from the datasets. Suppose we’re interested in predicting bodyfat, which is difficult to measure precisely, with other variables which are easier to measure: Height, Weight, and Abdomen circumference.\nIf we do so, we get the following sum of squared errors:\n\nreg &lt;- lm(bodyfat ~ Abdomen + Height + Weight, bodyfat_df)\nprint(reg$coefficients)\n\n(Intercept)     Abdomen      Height      Weight \n-36.6147193   0.9515631  -0.1270307  -0.1307606 \n\nprint(sprintf(\"Error: %f\", mean(reg$residuals^2)))\n\n[1] \"Error: 19.456161\"\n\n\n(a)\nNoting that Heigh, Weight, and Abdomen are on different scales, your colleague suggests that you might get a better fit by normalizing them. But when you do, here’s what happened:\n\nbodyfat_df &lt;- bodyfat_df %&gt;%\n    mutate(height_norm=(Height - mean(Height)) / sd(Height),\n           weight_norm=(Weight - mean(Weight)) / sd(Weight),\n           abdomen_norm=(Abdomen - mean(Abdomen)) / sd(Abdomen))\nreg_norm &lt;- lm(bodyfat ~ abdomen_norm + height_norm + weight_norm, bodyfat_df)\nprint(reg_norm$coefficients)\n\n (Intercept) abdomen_norm  height_norm  weight_norm \n   19.150794    10.260778    -0.465295    -3.842945 \n\nprint(sprintf(\"Error: %f\", mean(reg_norm$residuals^2)))\n\n[1] \"Error: 19.456161\"\n\n\nOur coefficients changed, but our fitted error didn’t change at all.\n\nExplain why the fitted error did not change.\nExplain why the coefficients did change.\n\nThe new regressors are invertible linear transforms of the original regressors. This means \\(\\hat{\\boldsymbol{Y}}\\) does not change, but the coefficients change.\n(b)\nChastened, your colleague suggests that maybe it’s the difference between normalized height and weight that would helps us predict. After all, it makes sense that height should only matter relative to weight, and vice versa. So they run the regresson on the difference:\n\nbodyfat_df &lt;- bodyfat_df %&gt;%\n    mutate(hw_diff = height_norm - weight_norm)\nreg_diff &lt;- lm(bodyfat ~ abdomen_norm + height_norm + weight_norm + hw_diff, bodyfat_df)\nprint(reg_diff$coefficients)\n\n (Intercept) abdomen_norm  height_norm  weight_norm      hw_diff \n   19.150794    10.260778    -0.465295    -3.842945           NA \n\nprint(sprintf(\"Error: %f\", mean(reg_diff$residuals^2)))\n\n[1] \"Error: 19.456161\"\n\n\nNow, our fitted error didn’t change at all, but our difference coefficient wasn’t even estimated.\n\nExplain why the fitted error did not change.\nExplain why the difference coefficient was not estimated by R.\n\nThe new regressors are again invertible linear transforms of the original regressors. The difference is a linear combination of height_norm and weight_norm, so \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\) is not invertible, and \\betavhat is not uniquely defined. R reports NA for coefficients that it cannot estimate.\n(c)\nFinally, your colleague suggests regressing instead on the ratio of weight to height. Here are the results:\n\nbodyfat_df &lt;- bodyfat_df %&gt;%\n    mutate(hw_ratio = height_norm / weight_norm)\nreg_ratio &lt;- lm(bodyfat ~ abdomen_norm + height_norm + weight_norm + hw_ratio, bodyfat_df)\nprint(reg_ratio$coefficients)\n\n (Intercept) abdomen_norm  height_norm  weight_norm     hw_ratio \n19.149475485 10.271751693 -0.478683698 -3.848561820  0.009057317 \n\nprint(sprintf(\"Error: %f\", mean(reg_ratio$residuals^2)))\n\n[1] \"Error: 19.423560\"\n\n\nOur fitted error is different this time, and we could estimate this coefficient.\n\nExplain why we could we estimate the coefficient of the ratio of height to weight, but not the difference.\nExplain why the fitted error changed.\nIt happened that, by including the regressor hw_ratio, the fitted error decreased. Your colleague tells you that had it been a bad regressor, the error would have increased. Are they correct?\n\nYou can estimate the ratio because it is a nonlinear transformation of the regressors. With this new regressor, the fit can change. Including another regressor, no matter poorly associated with the response, can never make the error increase. Your colleague is incorrect.\n(d)\nLet \\(x_n = (\\textrm{Abdomen}_n, \\textrm{Height}_n, \\textrm{Weight}_n)^\\intercal\\) denote our set of regressors.\nYour colleague suggests a research project where you improve your fit by regressing \\(y_n \\sim z_n\\) for new regressors \\(z_n\\) of the form \\(z_n = \\boldsymbol{A}x_n\\), where the matrix \\(\\boldsymbol{A}\\) is chosen using a machine learning algorithm.\n\nWill this result produce a better fit to the data than simply regressing \\(y_n \\sim \\boldsymbol{x}_n\\)? Why or why not?\n\nSince \\(\\boldsymbol{A}x_n\\) is a linear combination of \\(\\boldsymbol{x}_n\\), it can never give a better fit than regressing on \\(x_n\\) alone. (It can give a worse fit if \\(\\boldsymbol{A}\\) is lower rank than \\(\\boldsymbol{x}_n\\).) Searching over all possible \\(\\boldsymbol{A}\\) will never give you a better fit than \\(y_n \\sim \\boldsymbol{x}_n\\). This is not a good idea for a research project.\n(e)\nFinally, your colleague suggests a research project where you again regression \\(y_n \\sim z_n\\), but now you let \\(z_n = f(x_n)\\) for any function \\(f\\), where you use a neural network to find the best fit to the data over all possible functions \\(f(x_n)\\).\n\nWill this result produce a better fit to the data than simply regressing \\(y_n \\sim \\boldsymbol{x}_n\\)? Why or why not?\nDo you think this result produce a useful prediction for new data? Why or why not?\n\nSince \\(f(\\boldsymbol{x}_n)\\) can be a nonlinear function, you can produce a better fit than regressing on \\(\\boldsymbol{x}_n\\) alone. In fact, the best possible fit among all possible functions is a highly expressive function which fits the data perfectly with zero error. You don’t need a neural network to identify this. Having zero error does not mean the fit is useful for anything — it likely overfits the training data and has poor predictive power on new data. This is not a good idea for a research project.\n\n\n4 Leaving a single datapoint out of regression\nThis homework problem derives a closed-form expression for the effect of leaving a datapoint out of the regression.\nWe will use the following result, known as the Woodbury formula (but also many other names, including the Sherman-Morrison-Woodbury formula). Let \\(A\\) denote an invertible matrix, and \\(\\boldsymbol{u}\\) and \\(\\boldsymbol{v}\\) vectors the same length as \\(A\\). Then\n\\[\n(A + \\boldsymbol{u}\\boldsymbol{v}^\\intercal)^{-1} ={}\nA^{-1} - \\frac{A^{-1} \\boldsymbol{u}\\boldsymbol{v}^\\intercal A^{-1}}{1 + \\boldsymbol{v}^\\intercal A^{-1} \\boldsymbol{u}}.\n\\]\nWe will also use the definition of a “leverage score” from lecture \\(h_n := \\boldsymbol{x}_n^\\intercal(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{x}_n\\). Note that \\(h_n = (\\boldsymbol{X}(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{X}^\\intercal)_{nn}\\) is the \\(n\\)–th diagonal entry of the projection matrix \\(\\underset{\\boldsymbol{X}}{\\boldsymbol{P}}\\).\nLet \\(\\hat{\\boldsymbol{\\beta}}_{-n}\\) denote the estimate of \\(\\hat{\\beta}\\) with the datapoint \\(n\\) left out. Similarly, let \\(\\boldsymbol{X}_{-n}\\) denote the \\(\\boldsymbol{X}\\) matrix with row \\(n\\) left out, and \\(\\boldsymbol{Y}_{-n}\\) denote the \\(\\boldsymbol{Y}\\) matrix with row \\(n\\) left out.\na Prove that\n\\[\n\\hat{\\boldsymbol{\\beta}}_{-n} = (\\boldsymbol{X}_{-n}^\\intercal\\boldsymbol{X}_{-n})^{-1} \\boldsymbol{X}_{-n}^\\intercal\\boldsymbol{Y}_{-n}\n= (\\boldsymbol{X}^\\intercal\\boldsymbol{X}- \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal)^{-1} (\\boldsymbol{X}^\\intercal\\boldsymbol{Y}- \\boldsymbol{x}_n y_n)\n\\]\nSolution\nThis follows from \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}= \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\) and \\(\\boldsymbol{X}^\\intercal\\boldsymbol{Y}= \\sum_{n=1}^N\\boldsymbol{x}_n y_n\\).\nb\nUsing the Woodbury formula, derive the following expression: \\[\n(\\boldsymbol{X}^\\intercal\\boldsymbol{X}- \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal)^{-1} =\n(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} + \\frac{(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} }{1 - h_n}\n\\]\nSolution\nDirect application of the formula with \\(\\boldsymbol{u}= \\boldsymbol{x}_n\\) and \\(\\boldsymbol{v}= -\\boldsymbol{x}_n\\) gives \\[\n(\\boldsymbol{X}^\\intercal\\boldsymbol{X}- \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal)^{-1} =\n(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} +\n  \\frac{(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} }{1 - \\boldsymbol{x}_n^\\intercal(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1}\\boldsymbol{x}_n}.\n\\]\nThen recognize the leverage score.\nc\nCombine (a) and (b) to derive the following explicit expression for \\(\\hat{\\boldsymbol{\\beta}}_{-n}\\):\n\\[\n\\hat{\\boldsymbol{\\beta}}_{-n} = \\hat{\\boldsymbol{\\beta}}- (\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{x}_n \\frac{1}{1 - h_n} \\hat{\\varepsilon}_n.\n\\]\nSolution\nWe have\n\\[\n(\\boldsymbol{X}^\\intercal\\boldsymbol{X}- \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal)^{-1} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}=\n\\hat{\\beta}+\n  \\frac{(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\hat{\\beta}}{1 - h_n}.\n\\]\nand\n\\[\n(\\boldsymbol{X}^\\intercal\\boldsymbol{X}- \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal)^{-1} \\boldsymbol{x}_n y_n =\n(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1}  \\boldsymbol{x}_n y_n + \\frac{(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{x}_n h_n }{1 - h_n} y_n.\n\\]\nCombining,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}}_{-n} ={}&\n\\hat{\\beta}+ (\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{x}_n  \\left(\n   \\frac{\\boldsymbol{x}_n^\\intercal\\hat{\\beta}}{1 - h_n} - y_n - \\frac{h_n }{1 - h_n} y_n\n\\right)\n\\\\={}&\n\\hat{\\beta}+ (\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{x}_n  \\left(\n   \\frac{1 }{1 - h_n} \\hat{y}_n - \\left(1 + \\frac{h_n }{1 - h_n} \\right)y_n  \n   \\right)\n\\\\={}&\n\\hat{\\beta}+ (\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{x}_n  \\left(\n   \\frac{1 }{1 - h_n} \\hat{y}_n - \\frac{1 }{1 - h_n} y_n  \n\\right)\n\\\\={}&\n\\hat{\\beta}- (\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{x}_n  \\frac{1 }{1 - h_n} \\hat{\\varepsilon}_n\n\\end{aligned}\n\\]\nd\nLetting \\(\\hat{y}_{n,-n} = \\boldsymbol{x}_n^\\intercal\\hat{\\boldsymbol{\\beta}}_{-n}\\) denote the estimate of \\(y_n\\) after deleting the \\(n\\)–th observation. Using (c), derive the following explicit expression the change in \\(\\hat{y}_n\\) upon deleting the \\(n\\)–th observation:\n\\[\n\\hat{y}_{n,-n} - \\hat{y}_n = \\boldsymbol{x}_n^\\intercal\\hat{\\boldsymbol{\\beta}}_{-n} - \\boldsymbol{x}_n^\\intercal\\hat{\\boldsymbol{\\beta}}= -\\frac{h_n}{1 - h_n} \\hat{\\varepsilon}_n.\n\\]\nThis shows that the effect of deleting observation \\(n\\) on \\(\\hat{y}_n\\) is large only if both the residual and leverage score is large.\nSolution Taking \\(\\boldsymbol{x}_n^\\intercal\\) times each side of the result from c gives\n\\[\n\\begin{aligned}\n\\hat{y}_{n,-n} ={}& \\boldsymbol{x}_n^\\intercal\\hat{\\boldsymbol{\\beta}}_{-n}\n\\\\={}&\n\\boldsymbol{x}_n^\\intercal\\hat{\\beta}- \\boldsymbol{x}_n^\\intercal(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{x}_n  \\frac{1 }{1 - h_n} \\hat{\\varepsilon}_n\n\\\\={}&\n\\hat{y}_n - \\frac{h_n}{1 - h_n} \\hat{\\varepsilon}_n.\n\\end{aligned}\n\\]\nThe result follows by subtracting \\(\\hat{y}_n\\) from both sides."
  },
  {
    "objectID": "assignments/hw1.html",
    "href": "assignments/hw1.html",
    "title": "STAT151A Homework 1 (prerequisites review).",
    "section": "",
    "text": "This homework is due on Gradescope on Friday September 13th at 9pm."
  },
  {
    "objectID": "assignments/hw1.html#better-code",
    "href": "assignments/hw1.html#better-code",
    "title": "STAT151A Homework 1 (prerequisites review).",
    "section": "Better code:",
    "text": "Better code:\nHere is an example of better code (but not necessarily better analysis). We will grade flexibly, there are many different ways you can do this.\n# What matters most for popularity?  Explore this question with linear regression.\n\n# Regress track_popularity ~ regressor + 1 for each regressor. \nregressors &lt;- c(\"danceability\", \"energy\", \"loudness\", \"speechiness\", \n                \"acousticness\", \"instrumentalness\", \"liveness\", \"tempo\")\n\n# Check that all regressors are present\nstopifnot(all(regressors %in% names(df)))\n# Run a univariate regression y ~ x + 1 and return the coefficient for x.\n#\n# Arguments:\n# - regressor: The name of a column in the data frame `df` to use as x\n# - response: The name of a column in the data frame `df` to use as y\n# - df: A dataframe containing the data to analyze.\n#\n# Returns:\n# - The estimate of the coefficient of x in the regression y ~ x + 1.\nRunRegression &lt;- function(regressor, response, dataframe) {\n  stopifnot(regressor %in% names(dataframe))\n  stopifnot(response %in% names(dataframe))\n\n  x &lt;- df[[regressor]] # Regressor\n  y &lt;- df[[response]] # Response\n  \n  # Subtract the means to account for regression on a constant\n  x_centered &lt;- x - mean(x)\n  y_centered &lt;- y - mean(y)\n  \n  # Regression estimate using the standard OLS forumula\n  betahat &lt;- mean(x_centered * y_centered) / mean(x_centered^2) \n\n  return(betahat)\n}\n# Run the regression for each regressor.\nresults &lt;- data.frame()\nfor (regressor in regressors) {\n  betahat &lt;- RunRegression(\n    regressor=regressor, response=\"track_popularity\", dataframe=df)\n  result &lt;- data.frame(regressor=regressor, betahat=betahat)\n  results &lt;- bind_rows(results, result)\n}\n# Sort by the regression coefficient and summarize the results.\nresults &lt;- arrange(results, desc(betahat))\nprint(results)\nprint(sprintf(\"%s is the regressor with the largest track popularity coefficient.\", \n      results[1, \"regressor\"]))"
  },
  {
    "objectID": "assignments/assignments.html",
    "href": "assignments/assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Homework 1 (with solutions)\nHomework 2 (with solutions)\nHomework 3 (with solutions)\nHomework 4 (pending GSI check) (due Friday October 25th at 9pm)",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/hw2.html",
    "href": "assignments/hw2.html",
    "title": "STAT151A Homework 2 (with solutions)",
    "section": "",
    "text": "This homework is due on Gradescope on Monday September 30th at 9pm.\n\n1 Ordinary least squares in matrix form\nConsider simple least squares regression \\(y_n = \\beta_1 + \\beta_2 x_n + \\varepsilon_n\\), where \\(x_n\\) is a scalar. Assume that we have \\(N\\) datapoints. We showed directly that the least–squares solution is given by\n\\[\n\\begin{aligned}\n\\hat{\\beta}_1 ={}& \\overline{y} - \\hat{\\beta}_2 \\overline{x}\n&and\\quad\\quad\n\\hat{\\beta}_2 ={}&\n    \\frac{\\overline{xy} - \\overline{x} \\, \\overline{y}}\n         {\\overline{xx}  - \\overline{x}^2}.\n\\end{aligned}\n\\]\nLet us re–derive this using matrix notation.\n\n(a)\nWrite simple linear regression in the form \\(\\boldsymbol{Y}= \\boldsymbol{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}\\). Be precise about what goes into each entry of \\(\\boldsymbol{Y}\\), \\(\\boldsymbol{X}\\), \\(\\boldsymbol{\\beta}\\), and \\(\\boldsymbol{\\varepsilon}\\). What are the dimensions of each?\n\n\n(b)\nWe proved that the optimal \\(\\hat{\\boldsymbol{\\beta}}\\) satisfies \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}= \\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\). Define the “barred quantities” \\[\n\\begin{aligned}\n\\overline{y} ={}& \\frac{1}{N} \\sum_{n=1}^Ny_n \\\\\n\\overline{x} ={}& \\frac{1}{N} \\sum_{n=1}^Nx_n \\\\\n\\overline{xy} ={}& \\frac{1}{N} \\sum_{n=1}^Nx_n y_n \\\\\n\\overline{xx} ={}& \\frac{1}{N} \\sum_{n=1}^Nx_n ^2,\n\\end{aligned}\n\\]\nIn terms of the barred quantities and the number of datpoints \\(N\\), write expressions for \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\) and \\(\\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\).\n\n\n(c)\nWhen is \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\) invertible? Write a formal expression in terms of the barred quantities. Interpret this condition intuitively in terms of the distribution of the regressors \\(x_n\\).\n\n\n(d)\nUsing the formula for the inverse of a \\(2\\times 2\\) matrix, find an expression for \\(\\hat{\\boldsymbol{\\beta}}\\), and confirm that we get the same answer that we got by solving directly.\n\n\n(e)\nIn the case where \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\) is not invertible, find three distinct values of \\(\\boldsymbol{\\beta}\\) that all achieve the same sum of squared residuals \\(\\boldsymbol{\\varepsilon}^\\intercal\\boldsymbol{\\varepsilon}\\).\nSolutions\n\n\n(a)\n\\[\n\\boldsymbol{X}= \\begin{pmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_N\n\\end{pmatrix}\n\\quad\\quad\n\\boldsymbol{Y}= \\begin{pmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_N\n\\end{pmatrix}\n\\quad\\quad\n\\boldsymbol{\\varepsilon}= \\begin{pmatrix}\n\\varepsilon_1 \\\\\n\\vdots \\\\\n\\varepsilon_N\n\\end{pmatrix}\n\\quad\\quad\n\\boldsymbol{\\beta}= \\begin{pmatrix}\n\\beta_1 \\\\\n\\beta_2\n\\end{pmatrix}\n\\]\nThese are \\(N \\times 2\\), \\(N \\times 1\\), \\(N \\times 1\\), and \\(2 \\times 1\\) respectively.\n\n\n(b)\n\\[\n\\boldsymbol{X}^\\intercal\\boldsymbol{X}=\nN\n\\begin{pmatrix}\n1 & \\bar{x}\\\\\n\\bar{x}& \\overline{xx}\n\\end{pmatrix}\n\\quad\\quad\n\\boldsymbol{X}^\\intercal\\boldsymbol{Y}=\nN\n\\begin{pmatrix}\n\\bar{y}\\\\\n\\overline{xy}\n\\end{pmatrix}\n\\]\n\n\n(c)\n\\(\\boldsymbol{X}\\intercal\\boldsymbol{X}\\) is invertible if the determinant, \\(N (\\overline{xx} - \\bar{x}\\, \\bar{x}) \\ne 0\\). This occurs when the sample variance of \\(x_n\\) is greater than zero.\n\n\n(d)\n\\[\n\\begin{aligned}\n(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}={}&\n\\frac{1}{N (\\overline{xx} - \\bar{x}\\, \\bar{x})}\n\\begin{pmatrix}\n  \\overline{xx} & -\\bar{x}\\\\\n  -\\bar{x}& 1\n\\end{pmatrix}\nN\n\\begin{pmatrix}\n  \\bar{y}\\\\\n  \\overline{xy}\n\\end{pmatrix}\n\\\\={}&\n\\frac{1}{\\overline{xx} - \\bar{x}\\, \\bar{x}}\n\\begin{pmatrix}\n  \\bar{y}\\, \\overline{xx} - \\bar{x}\\, \\overline{xy} \\\\\n  -\\bar{y}\\, \\bar{x}+ \\overline{xy}\n\\end{pmatrix}\n\\end{aligned}.\n\\]\nWe already have \\(\\hat{\\beta}_2 = (\\overline{xy} - \\bar{y}\\, \\bar{x}) / (\\overline{xx} - \\bar{x}\\, \\bar{x})\\) as expected. To see that \\(\\hat{\\beta}_1\\) is correct, write\n\\[\n\\begin{aligned}\n\\bar{y}\\, \\overline{xx} - \\bar{x}\\, \\overline{xy} ={}&\n\\bar{y}\\, \\overline{xx} -  \\bar{y}\\, \\bar{x}\\, \\bar{x}+  \\bar{y}\\, \\bar{x}\\, \\bar{x}- \\bar{x}\\, \\overline{xy}\n\\\\={}&\n\\bar{y}(\\overline{xx} - \\bar{x}\\, \\bar{x}) - \\bar{x}(\\overline{xy} - \\bar{y}\\, \\bar{x}).\n\\end{aligned}\n\\]\nPlugging this in gives\n\\[\n\\begin{aligned}\n\\frac{\\bar{y}\\, \\overline{xx} - \\bar{x}\\, \\overline{xy}}{\\overline{xx} - \\bar{x}\\, \\bar{x}}\n={}&\n\\frac{\\bar{y}(\\overline{xx} - \\bar{x}\\, \\bar{x}) - \\bar{x}(\\overline{xy} - \\bar{y}\\, \\bar{x})}\n     {\\overline{xx} - \\bar{x}\\, \\bar{x}}\n\\\\={}&\n\\bar{y}- \\bar{x}\\hat{\\beta}_2,\n\\end{aligned}\n\\]\nas expected.\n\n\n\n2 Probability and matrices\nFor this problem, assume that \\(y_n = \\beta_0 + x_n \\beta_1 + \\varepsilon_n\\) for scalar \\(x_n\\) and some fixed \\(\\beta_0\\) and \\(\\beta_1\\). Assume that\n\nThe residuals \\(\\varepsilon_n\\) are IID with \\(\\mathbb{E}\\left[\\varepsilon_n\\right] = 0\\) and \\(\\mathrm{Var}\\left(\\varepsilon_n\\right) = \\sigma^2\\).\nThe regressors \\(x_n\\) are IID with \\(\\mathbb{E}\\left[x_n\\right] = \\mu\\) and \\(\\mathrm{Var}\\left(x_n\\right) = \\nu^2\\).\nThe residuals are independent of the regressors.\n\n\n(a)\nEvaluate the following expressions. (You may need to remind yourself of the definition of conditional expectation and variance.)\n\n\\(\\mathbb{E}\\left[y_n\\right]\\)\n\\(\\mathrm{Var}\\left(y_n\\right)\\)\n\\(\\mathbb{E}\\left[y_n \\vert x_n\\right]\\)\n\\(\\mathrm{Var}\\left(y_n \\vert x_n\\right)\\)\n\n\n\n(b)\nCompute the following limits using the LLN, or say that the limit does not exist or is infinite.\n\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} \\sum_{n=1}^N\\varepsilon_n\\)\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} \\sum_{n=1}^N\\varepsilon_n^2\\)\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} \\sum_{n=1}^Nx_n\\)\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} \\sum_{n=1}^Nx_n^2\\)\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} \\sum_{n=1}^Nx_n \\varepsilon_n\\)\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} \\sum_{n=1}^Nx_n y_n\\)\n\n\n\n(c)\nCompute the following limits using the CLT, or say that the limit does not exist or is infinite.\n\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{\\sqrt{N}} \\sum_{n=1}^N\\varepsilon_n\\)\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{\\sqrt{N}} \\sum_{n=1}^Ny_n\\)\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{\\sqrt{N}} \\sum_{n=1}^N(y_n - (\\beta_0 + x_n \\beta_1))\\)\n\n\n\n(d)\nNoting that this is simple linear regression, let \\(\\boldsymbol{X}\\), \\(\\boldsymbol{Y}\\), and \\(\\boldsymbol{\\varepsilon}\\) be as in the solution to question one above. Evaluate the following limits, or say that the limit does not exist or is infinite.\nHere, \\((\\boldsymbol{A})_{ij}\\) denotes the \\(i,j\\)–th entry of the matrix \\(\\boldsymbol{A}\\). Let the regressor \\(x_n\\) be in the second column of \\(\\boldsymbol{X}\\).\n\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} \\boldsymbol{1}^\\intercal\\boldsymbol{\\varepsilon}\\)\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{\\sqrt{N}} \\boldsymbol{1}^\\intercal\\boldsymbol{\\varepsilon}\\)\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} (\\boldsymbol{X}^\\intercal\\boldsymbol{X})_{11}\\)\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} (\\boldsymbol{X}^\\intercal\\boldsymbol{X})_{12}\\)\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} (\\boldsymbol{X}^\\intercal\\boldsymbol{X})_{22}\\)\n\\(\\lim_{N \\rightarrow \\infty } (\\boldsymbol{X}^\\intercal\\boldsymbol{X})_{11}\\)\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} (\\boldsymbol{Y}- \\boldsymbol{X}\\boldsymbol{\\beta})^\\intercal(\\boldsymbol{Y}- \\boldsymbol{X}\\boldsymbol{\\beta})\\)\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{\\sqrt{N}} (\\boldsymbol{Y}- \\boldsymbol{X}\\boldsymbol{\\beta})^\\intercal(\\boldsymbol{Y}- \\boldsymbol{X}\\boldsymbol{\\beta})\\)\n\nHint: Write the matrix expressions as sums over \\(n=1\\) to \\(N\\).\nSolutions\n\n\n(a)\n\n\\(\\mathbb{E}\\left[y_n\\right] = \\beta_0 + \\mu \\beta_1\\)\n\\(\\mathrm{Var}\\left(y_n\\right) = \\beta_1^2 \\nu^2 + \\sigma^2\\)\n\\(\\mathbb{E}\\left[y_n \\vert x_n\\right] = \\beta_0 + x_n \\beta_1\\)\n\\(\\mathrm{Var}\\left(y_n \\vert x_n\\right) = \\sigma^2\\)\n\n\n\n(b)\n\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} \\sum_{n=1}^N\\varepsilon_n = 0\\) by the LLN\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} \\sum_{n=1}^N\\varepsilon_n^2 = \\sigma^2\\) by the LLN\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} \\sum_{n=1}^Nx_n = \\mu\\) by the LLN\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} \\sum_{n=1}^Nx_n^2 = \\mu^2 + \\nu^2\\) by the LLN\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} \\sum_{n=1}^Nx_n \\varepsilon_n = 0\\) by the LLN\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{N} \\sum_{n=1}^Nx_n y_n = \\beta_0 \\mu + \\beta_1 (\\nu^2 + \\mu^2)\\) by the LLN\n\n\n\n(c)\n\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{\\sqrt{N}} \\sum_{n=1}^N\\varepsilon_n = \\mathcal{N}\\left(0,\\sigma^2\\right)\\) by the CLT\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{\\sqrt{N}} \\sum_{n=1}^Ny_n\\) diverges if \\(\\beta_0 + \\beta_1 \\mu \\ne 0\\), and otherwise converges to \\(\\mathcal{N}\\left(0,  \\beta_1^2 \\nu^2 + \\sigma^2\\right)\\).\n\\(\\lim_{N \\rightarrow \\infty } \\frac{1}{\\sqrt{N}} \\sum_{n=1}^N(y_n - (\\beta_0 + x_n \\beta_1)) = \\mathcal{N}\\left(0, \\sigma^2\\right)\\) by the CLT because \\(y_n - (\\beta_0 + x_n \\beta_1) = \\varepsilon_n\\).\n\n\n\n\n3 One-hot encoding\nConsider a one–hot encoding of a variable \\(z_n\\) that takes three distinct values, “a”, “b”, and “c”. That is, let\n\\[\n\\boldsymbol{x}_n = \\begin{cases}\n\\begin{pmatrix}\n1 \\\\ 0 \\\\ 0\n\\end{pmatrix} & \\textrm{ when }z_n = a \\\\\n\\begin{pmatrix}\n0 \\\\ 1 \\\\ 0\n\\end{pmatrix} & \\textrm{ when }z_n = b \\\\\n\\begin{pmatrix}\n0 \\\\ 0 \\\\ 1\n\\end{pmatrix} & \\textrm{ when }z_n = c \\\\\n\\end{cases}\n\\]\nLet \\(\\boldsymbol{X}\\) be the regressor matrix with \\(\\boldsymbol{x}_n^\\intercal\\) in row \\(n\\).\n\n(a)\nLet \\(N_a\\) be the number of observations with \\(z_n\\) = a, and let \\(\\sum_{n:z_n = a}\\) denote a sum over rows with \\(z_n\\) = a, with analogous definitions for b and c. In terms of these quantities, write expressions for \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\) and \\(\\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\).\n\n\n(b)\nWhen is \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\) invertible? Explain intuitively why the regression problem cannot be solved when \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\) is not invertible. Write an explicit expression for \\((\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1}\\) when it is invertible.\n\n\n(c)\nUsing your previous answer, show that the least squares vector \\(\\hat{\\boldsymbol{\\beta}}\\) is the mean of \\(y_n\\) within distinct values of \\(z_n\\).\n\n\n(d)\nSuppose now you include a constant in the regression, so that\n\\[\ny_n = \\alpha + \\boldsymbol{\\beta}^\\intercal\\boldsymbol{x}_n + \\varepsilon_n,\n\\]\nand let \\(\\boldsymbol{X}'\\) denote the regressor matrix for this regression with coefficient vector \\((\\alpha, \\boldsymbol{\\beta}^\\intercal)^\\intercal\\). Write an expression for \\(\\boldsymbol{X}'^\\intercal\\boldsymbol{X}'\\) and show that it is not invertible.\n\n\n(e)\nFind three distinct values of \\((\\alpha, \\boldsymbol{\\beta}^\\intercal)\\) that all give the exact same fit \\(\\alpha + \\boldsymbol{\\beta}^\\intercal\\boldsymbol{x}_n\\).\nSolutions\n\n\n(a)\n\\[\n\\boldsymbol{X}^\\intercal\\boldsymbol{X}=\n\\begin{pmatrix}\nN_a & 0 & 0 \\\\\n0 & N_b & 0 \\\\\n0 & 0 & N_c \\\\\n\\end{pmatrix}\n\\quad\\quad\n\\boldsymbol{X}^\\intercal\\boldsymbol{Y}=\n\\begin{pmatrix}\n\\sum_{n:z_n = a} y_n \\\\\n\\sum_{n:z_n = b} y_n \\\\\n\\sum_{n:z_n = c} y_n \\\\\n\\end{pmatrix}.\n\\]\n\n\n(b)\nIt is invertbile as long as each of \\(N_a\\), \\(N_b\\), and \\(N_c\\) are nonzero. If there are no observations for a particular level, you of course cannot estimate its relationship with \\(y_n\\). When \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\) is invertible, then\n\\[\n\\boldsymbol{X}^\\intercal\\boldsymbol{X}^{-1} =\n\\begin{pmatrix}\n1/N_a & 0 & 0 \\\\\n0 & 1/N_b & 0 \\\\\n0 & 0 & 1/N_c \\\\\n\\end{pmatrix}\n\\]\n\n\n(c)\nBy direct multiplication,\n\\[\n\\hat{\\boldsymbol{\\beta}}=\n\\boldsymbol{X}^\\intercal\\boldsymbol{X}^{-1} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}=\n\\begin{pmatrix}\n\\frac{1}{N_a} \\sum_{n:z_n = a} y_n \\\\\n\\frac{1}{N_b} \\sum_{n:z_n = b} y_n \\\\\n\\frac{1}{N_c} \\sum_{n:z_n = c} y_n \\\\\n\\end{pmatrix}.\n\\]\n\n\n(d)\n\\[\n(\\boldsymbol{X}')^\\intercal\\boldsymbol{X}' =\n\\begin{pmatrix}\nN & N_a & N_b & N_c \\\\\nN_a & N_a & 0 & 0 \\\\\nN_b & 0 & N_b & 0 \\\\\nN_c & 0 & 0 & N_c \\\\\n\\end{pmatrix}.\n\\]\nThis is not invertible because the first column is the sum of the other three. Equivalently, \\((\\boldsymbol{X}')^\\intercal\\boldsymbol{X}' \\boldsymbol{v}= \\boldsymbol{0}\\) where \\(\\boldsymbol{v}= (1, -1, -1, -1)^\\intercal\\).\n\n\n(e)\nAny line of the form\n\\[\n(\\alpha + \\beta_1) z_{na} +\n(\\alpha + \\beta_2) z_{nb} +\n(\\alpha + \\beta_3) z_{nc}\n\\]\nwill give the same fit. Three equivalent sets that also happen to solve the least squares problem are\n\\[\n\\begin{aligned}\n(\\alpha, \\beta_1, \\beta_2, \\beta_3) ={} \\hat{\\boldsymbol{\\beta}}+ (0, 0, 0, 0) \\\\\n(\\alpha, \\beta_1, \\beta_2, \\beta_3) ={} \\hat{\\boldsymbol{\\beta}}+ (1, -1, -1, -1) \\\\\n(\\alpha, \\beta_1, \\beta_2, \\beta_3) ={} \\hat{\\boldsymbol{\\beta}}+ (2, -2, -2, -2).\n\\end{aligned}\n\\]\nThese are all of the form \\(\\hat{\\boldsymbol{\\beta}}+ C \\boldsymbol{v}\\) for \\(C = 0\\), \\(C = 1\\), and \\(C = 2\\), as they must be, where \\(\\boldsymbol{v}\\) is the null vector from (d).\n\n\n\n4 Correlated regressors\nSuppose that \\(y_n = \\boldsymbol{x}_n^\\intercal\\boldsymbol{\\beta}+ \\varepsilon_n\\) for some \\(\\boldsymbol{\\beta}\\). Suppose that \\(\\mathbb{E}\\left[\\varepsilon_n\\right] = 0\\) and \\(\\mathrm{Var}\\left(\\varepsilon_n\\right) = \\sigma^2\\), and \\(\\varepsilon_n\\) are independent of each other and the \\(\\boldsymbol{x}_n\\).\nLet \\(\\boldsymbol{x}_n \\in \\mathbb{R}^{2}\\), where\n\n\\(\\boldsymbol{x}_n\\) is independent of \\(\\boldsymbol{x}_m\\) for \\(n \\ne m\\),\n\\(\\mathbb{E}\\left[\\boldsymbol{x}_{n1}\\right] = \\mathbb{E}\\left[\\boldsymbol{x}_{n2}\\right] = 0\\),\n\\(\\mathrm{Var}\\left(\\boldsymbol{x}_{n1}\\right) = \\mathrm{Var}\\left(\\boldsymbol{x}_{n2}\\right) = 1\\), and\n\\(\\mathbb{E}\\left[\\boldsymbol{x}_{n1} \\boldsymbol{x}_{n2}\\right] = \\rho\\).\n\n\n(a)\nIf \\(\\left|\\rho\\right| &lt; 1\\), is \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\) always, sometimes, or never invertible?\n\n\n(b)\nIf \\(\\left|\\rho\\right| = 1\\), is \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\) always, sometimes, or never invertible?\n\n\n(c)\nWhat is \\(\\lim_{N \\rightarrow \\infty} \\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\)? When is the limit invertible?\n\n\n(d)\nState intuitively why there is no unique \\(\\hat{\\boldsymbol{\\beta}}\\) when \\(\\rho = 1\\). When \\(\\rho = 1\\), give two distinct values of \\(\\boldsymbol{\\beta}\\) that result in the same fit \\(\\boldsymbol{\\beta}^\\intercal\\boldsymbol{x}_n\\).\nSolutions\n\n\n(a)\nIt may be that \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\) is non–invertible by chance, depending on the distribution of \\(\\boldsymbol{x}_n\\). There are distributions for which \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\) is invertible with probability one, such as independent normals. But without extra information like this then the right answer is “sometimes.”\n\n\n(b)\nIf \\(\\rho = 1\\), then \\(\\boldsymbol{x}_{n1} = \\boldsymbol{x}_{n2}\\) almost surely because they are perfectly correlated. (The difference between them has zero variance.) In this case, \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\) is never invertible (non–invertible with probability one).\n\n\n(c)\nBy the LLN,\n\\[\n\\lim_{N \\rightarrow \\infty} \\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}=\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}.\n\\]\nThe limit is always invertible.\n\n\n(d)\nIf \\(\\rho = 1\\), then \\(\\boldsymbol{x}_{n1} = \\boldsymbol{x}_{n2}\\) with probability one, and we can write \\(\\boldsymbol{\\beta}^\\intercal\\boldsymbol{x}_n = (\\beta_1 + \\beta_2) \\boldsymbol{x}_{n1}\\). Intuitively, we cannot distinguish a relationship between \\(y_n\\) and the two components of \\(\\boldsymbol{x}_n\\) because the two components are numerically the same. Three values that give the same fit are \\(\\boldsymbol{\\beta}= (0, 0)^\\intercal\\), \\(\\boldsymbol{\\beta}= (1, -1)^\\intercal\\), and \\(\\boldsymbol{\\beta}= (-1, 1)^\\intercal\\).\n\n\n\n5 Matrix square roots\nIn the last homework, we proved that if \\(\\boldsymbol{A}\\) is a square symmetric matrix with eigenvalues \\(\\boldsymbol{u}_p\\) and eigenvectors \\(\\lambda_p\\), then we can write \\(\\boldsymbol{A}= \\boldsymbol{U}\\Lambda \\boldsymbol{U}^\\intercal\\), where \\(\\boldsymbol{U}= (\\boldsymbol{u}_1 \\ldots \\boldsymbol{u}_p)\\) has \\(\\boldsymbol{u}_p\\) in its \\(p\\)–th column, and \\(\\Lambda\\) is diagonal with \\(\\lambda_p\\) in the \\(p\\)–th diagonal entry. We also have that the eigenvectors can be taken to be orthonormal without loss of generality.\nWe will additionally assume that \\(\\boldsymbol{A}= \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\) for some (possibly non–square) matrix \\(\\boldsymbol{X}\\).\nDefine \\(\\Lambda^{1/2}\\) to be the diagonal matrix with \\(\\sqrt{\\lambda_p}\\) on the \\(p\\)–th diagonal.\n\nProve that, since \\(\\boldsymbol{A}= \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\), \\(\\lambda_p \\ge 0\\), and so \\(\\Lambda^{1/2}\\) is always real–valued.\nWhen the eignevalues are non–negative, we say that \\(\\boldsymbol{A}\\) is “positive semi–definite.” (Hint: using the fact that \\(\\lambda_p = \\boldsymbol{u}_p^\\intercal\\boldsymbol{A}\\boldsymbol{u}_p\\), show that \\(\\lambda_p\\) is the square of something.)\nShow that if we take \\(\\boldsymbol{Q}= \\boldsymbol{U}\\Lambda^{1/2} \\boldsymbol{U}^\\intercal\\) then \\(\\boldsymbol{A}= \\boldsymbol{Q}\\boldsymbol{Q}^\\intercal\\). We say that \\(\\boldsymbol{Q}\\) is a “matrix square root” of \\(\\boldsymbol{A}\\).\nShow that we also have \\(\\boldsymbol{A}= \\boldsymbol{Q}\\boldsymbol{Q}\\) (without the second transpose).\nShow that, if \\(\\boldsymbol{V}\\) is any orthonormal matrix (a matrix with orthonormal columns), then \\(\\boldsymbol{Q}' = \\boldsymbol{Q}\\boldsymbol{V}\\ne \\boldsymbol{Q}\\) also satisfies \\(\\boldsymbol{A}= \\boldsymbol{Q}' \\boldsymbol{Q}'^\\intercal\\). This shows that the matrix square root is not unique. (This fact can be thought of as the matrix analogue of the fact that \\(4 = 2 \\cdot 2\\) but also \\(4 = (-2) \\cdot (-2)\\)).\nShow that if \\(\\lambda_p &gt; 0\\) then \\(\\boldsymbol{Q}\\) is invertible.\nShow that, if \\(\\boldsymbol{Q}\\) is invertible, then the columns of \\(\\boldsymbol{X}\\boldsymbol{Q}^{-1}\\) are orthonormal. (Hint: show that \\((\\boldsymbol{X}\\boldsymbol{Q}^{-1})^\\intercal(\\boldsymbol{X}\\boldsymbol{Q}^{-1})\\) is the identity matrix.)\n\nSolutions\n(1)\nSuppose that \\(\\boldsymbol{v}\\) is an eigenvector with eigenvalue \\(\\lambda\\). Then\n\\[\n\\lambda \\left\\Vert\\boldsymbol{v}\\right\\Vert^2 = \\boldsymbol{v}^\\intercal\\boldsymbol{A}\\boldsymbol{v}= \\boldsymbol{v}^\\intercal\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\boldsymbol{v}=\n\\left\\Vert\\boldsymbol{X}\\boldsymbol{v}\\right\\Vert^2 \\ge 0.\n\\]\nSince \\(\\left\\Vert\\boldsymbol{v}\\right\\Vert^2 \\ge 0\\), we must have \\(\\lambda \\ge 0\\) as well.\n(2)\nWe already know that \\(\\Lambda^{1/2} \\Lambda^{1/2} = \\Lambda\\) because the matrices are diagonal. Because the eigenvector matrices are orthonormal,\n\\[\n\\boldsymbol{Q}\\boldsymbol{Q}^\\intercal=\n\\boldsymbol{U}\\Lambda^{1/2} \\boldsymbol{U}^\\intercal\\boldsymbol{U}\\Lambda^{1/2} \\boldsymbol{U}^\\intercal=\n\\boldsymbol{U}\\Lambda^{1/2} \\Lambda^{1/2} \\boldsymbol{U}^\\intercal=\n\\boldsymbol{U}\\Lambda \\boldsymbol{U}^\\intercal= \\boldsymbol{A}.\n\\]\n(3)\nShow directly that \\(\\boldsymbol{Q}\\) is symmetric.\n(4)\n\\[\n\\boldsymbol{Q}' \\boldsymbol{Q}'^\\intercal= \\boldsymbol{Q}\\boldsymbol{V}\\boldsymbol{V}^\\intercal\\boldsymbol{Q}^\\intercal= \\boldsymbol{Q}\\boldsymbol{Q}^\\intercal= \\boldsymbol{A},\n\\]\nsince \\(\\boldsymbol{V}^\\intercal= \\boldsymbol{V}^{-1}\\) and left and right inverses are the same.\n(5)\nThe inverse is given by the matrix with \\(1/\\sqrt{\\lambda_p}\\) on the diagonal, which can be verified by direct multiplication.\n(5)\n\\[\n\\begin{aligned}\n(\\boldsymbol{X}\\boldsymbol{Q}^{-1})^\\intercal(\\boldsymbol{X}\\boldsymbol{Q}^{-1}) ={}&\n(\\boldsymbol{X}(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1/2})^\\intercal\\boldsymbol{X}(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1/2}\n\\\\={}&\n(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1/2} \\boldsymbol{X}^\\intercal\\boldsymbol{X}(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1/2}\n\\\\={}&\n(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1/2} (\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{1/2} (\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{1/2} (\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1/2}\n\\\\={}&\n\\boldsymbol{I}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "assignments/hw4.html",
    "href": "assignments/hw4.html",
    "title": "STAT151A Homework 4.",
    "section": "",
    "text": "This homework is due on Gradescope on Friday October 25th at 9pm.\n\n1 Fixed effects and the FWL theorem\nSuppose we are interested in measuring the effect of class size on teaching evalutaions. Let \\(y_n\\) denote the teaching evaluation submitted by a particular student in a particular class and let \\(x_n\\) denote the class size for the \\(n\\)–th teaching evaluation.\nSuppose there are \\(K\\) students, and that each row was submitted by exactly one of the \\(K\\) students. Suppose also that each student submitted a teaching evaluation for each class they took, so that a particular student contributes to multiple rows. Let \\(z_n\\) denote a one–hot indicator recording which student submitted the \\(n\\)–th teaching evaluation. By \\(k(n)\\), denote the index of the student who submitted row \\(n\\). (For example, if student \\(3\\) wrote the review in row \\(10\\), then \\(k(10) = 3\\).)\nWe will be interested in \\(\\hat{\\beta}\\) in the regression \\(y_n \\sim x_n \\beta + \\boldsymbol{z}_n^\\intercal\\gamma\\).\n(a)\nLet \\(\\underset{\\boldsymbol{Z}}{\\boldsymbol{P}}\\) denote the projection matrix onto \\(\\boldsymbol{Z}\\), the \\(N \\times K\\) matrix of \\(\\boldsymbol{z}_n\\) regressors. Show that \\[\n\\left(\\underset{\\boldsymbol{Z}}{\\boldsymbol{P}} \\boldsymbol{Y}\\right)_n = \\frac{1}{N_{k(n)}} \\sum_{m: k(m) = k(n)} y_n =: \\bar{y}_{k(n)},\n\\] where \\(N_{k(n)}\\) is the number of rows in which student \\(k(n)\\) occurs and \\(\\bar{y}_{k}\\) is the average rating submitted by student \\(k\\). That is, the projection onto \\(\\boldsymbol{Z}\\) computes the average evaluation submitted by each student.\n(b)\nDefine \\(y'_n := y_n - \\bar{y}_{k(n)}\\) denote the students’ evaluations centered at each students’ mean evaluation, and let \\(\\boldsymbol{Y}' = (y'_1, \\ldots, y'_N)\\). Show that \\(\\boldsymbol{Y}' = \\underset{\\boldsymbol{Z}^\\perp}{\\boldsymbol{P}} \\boldsymbol{Y}\\).\n(c)\nLet \\(\\boldsymbol{X}= (x_1, \\ldots x_N)^\\intercal\\) denote the vector of class sizes. Let \\(\\bar{x}_{k}\\) denote the average size of class taken by student \\(k(n)\\). Let \\(x'_n := x_n - \\bar{x}_{k(n)}\\). Using the FWL theorem, show that \\(\\hat{\\beta}\\) in the regression \\(y_n \\sim x_n \\beta + \\boldsymbol{z}_n^\\intercal\\gamma\\) is equal to \\(\\hat{\\beta}\\) in the regression \\(y'_n \\sim x'_n \\beta\\).\n(d)\nSome students tend to give higher ratings than others, and some students tend to take larger classes than others.\nModels like this are sometime called “fixed effects models,” since they estimate the student-to-student variability with “fixed” regression parameters. How would you interpret \\(\\hat{\\gamma}_k\\), the \\(k\\)–th estimate coefficient in the regression \\(y_n \\sim x_n \\beta + \\boldsymbol{z}_n^\\intercal\\gamma\\)?\n\n\n2 Omitted variables in inference versus prediction\nSuppose we are interested in student performance as measured by fourth–year GPA, \\(y_n\\), centered at \\(0\\) and scaled to have unit variance. Suppose we imagine two regressors:\n\n\\(x_n\\): Student’s academic ability (in an abstract sense, maybe not precisely measurable or defined), and\n\\(z_n\\): Student’s performance on a standardized test before admission.\n\nSuppose that \\(y_n = \\beta x_n + \\varepsilon_n\\), where \\(\\varepsilon_n\\) is mean zero, finite variance, and independent of \\(x_n\\). This simplistic model is unrealistic, but we will use it for illustrative purposes, and assume for this problem that this model determines the “true” relationship between \\(x_n\\), \\(z_n\\), and \\(y_n\\). Note that \\(z_n\\) effectively has a zero coefficient in the “true” model — student score is causally determined entirely by “ability,” not by test score.\nAssume that we have centered and standardized \\(x_n\\) and \\(z_n\\), so that \\(\\mathbb{E}\\left[x_n\\right] = \\mathbb{E}\\left[z_n\\right] = 0\\), and \\(\\mathrm{Var}\\left(x_n\\right) = \\mathrm{Var}\\left(z_n\\right) = 1\\). However, \\(x_n\\) is highly correlated with \\(z_n\\), so that \\(\\mathbb{E}\\left[z_n x_n\\right] = 0.9\\).\n(a)\nGiven the true model, how will we change a students’ expected GPA if we:\n\nIncrease their test score \\(z_n\\) by helping them memorize the answers to the test?\nImprove their academic ability \\(x_n\\) by teaching them better time management skills?\n\n(b)\nIf the number of observations \\(N\\) is large, and we run the regression \\(y_n \\sim \\gamma z_n\\), what will the expected coefficient \\(\\mathbb{E}\\left[\\hat{\\gamma}\\vert \\boldsymbol{Z}\\right]\\) be? How does this compare to the “true” influence of \\(z_n\\) on GPA? How does this compare with the true influence of \\(x_n\\) on GPA?\n(c)\nAre we able to do effective inference using the regression \\(y_n \\sim \\gamma z_n\\)? Explain why or why not in intuitive terms.\n(d)\nLet \\(x_{*}\\), \\(z_{*}\\), and \\(y_{*}\\) denote a new observation not part of the training set. Using the regression \\(y_n \\sim \\gamma z_n\\), form the prediction \\(\\hat{y}_{*} = z_{*} \\hat{\\gamma}\\). Evaluate\n\n\\(\\mathrm{Var}\\left(\\hat{y}_{*} \\vert \\boldsymbol{Z}, \\boldsymbol{Y}\\right)\\)\n\\(\\mathbb{E}\\left[y_{*} \\hat{y}_{*} \\vert \\boldsymbol{Z}, \\boldsymbol{Y}\\right]\\)\n\nNote that \\(\\hat{\\gamma}\\) is not random conditional on \\(\\boldsymbol{Z}, \\boldsymbol{Y}\\), and recall that \\(\\mathrm{Var}\\left(y_*\\right) = 1\\). Using these, evalute the conditional correlation between the response and prediction:\n\\[\n\\textrm{Correlation}(y_*, \\hat{y}_* \\vert \\boldsymbol{Z}, \\boldsymbol{Y}) =\n  \\frac{\\mathbb{E}\\left[y_{*} \\hat{y}_{*} \\vert \\boldsymbol{Z}, \\boldsymbol{Y}\\right]}\n  {\\sqrt{\\mathrm{Var}\\left(\\hat{y}_{*} \\vert \\boldsymbol{Z}, \\boldsymbol{Y}\\right)} \\sqrt{\\mathrm{Var}\\left(y_* | \\boldsymbol{Z}, \\boldsymbol{Y}\\right)}}\n\\]\n(c)\nAre we able to do effective prediction using the regression \\(y_n \\sim \\gamma z_n\\), say, to identify which students are likely to have good GPAs using only test scores? Explain why or why not in intuitive terms.\n\n\n3 Regression to the mean with noisy test set data\nSuppose we are interesting in building models of the brain. In particular, we want to know whether it is easier to model the brain watching dog videos versus the brain watching cat videos.\nFor each of \\(n=1,\\ldots, N\\) people, we show them videos of animals playing while recording signals from their brain. We then build a classifier to predict, using the same signals, whether that individuals is watching a dog or a cat. We then evaluate the model on new dog and cat videos, measuring for person a test set error \\(\\varepsilon_{dog,n}\\) for the dog videos and \\(\\varepsilon_{cat,n}\\) for the cat videos.\nSince the test set is random, it is reasonable to model the errors as random and unbiased. For each \\(n\\), we thus have\n\\[\n\\mathbb{E}\\left[\\varepsilon_{dog,n}\\right] = \\mu_{dog,n}\n\\quad\\quad\\textrm{and}\\quad\\quad\n\\mathbb{E}\\left[\\varepsilon_{cat,n}\\right] = \\mu_{cat,n},\n\\]\nwhere \\(\\mathrm{Var}\\left(\\varepsilon_{cat,n}\\right) &gt; 0\\) and \\(\\mathrm{Var}\\left(\\varepsilon_{dog,n}\\right) &gt; 0\\)\n(a)\nIntuitively, what would it mean if \\(\\mu_{dog,n} = \\mu_{cat,n}\\) for each \\(n\\)?\n(b)\nSuppose that \\(\\mu_{dog,n} = \\mu_{cat,n} = \\mu_n\\) for each \\(n\\), and we run the regression \\(\\varepsilon_{dog,n} \\sim  \\beta\\varepsilon_{cat,n}\\). For very large \\(N\\), do you expect \\(\\hat{\\beta}\\) to be larger or smaller than \\(1\\)? Justify your answer.\n(c)\nAgain, suppose that \\(\\mu_{dog,n} = \\mu_{cat,n} = \\mu_n\\) for each \\(n\\), and we run the regression \\(\\varepsilon_{cat,n} \\sim \\gamma \\varepsilon_{dog,n}\\). For very large \\(N\\), do you expect \\(\\hat{\\gamma}\\) to be larger or smaller than \\(1\\)? Justify your answer.\n(d)\nSuppose the researcher runs the regression \\(\\varepsilon_{dog,n} \\sim \\beta \\varepsilon_{cat,n}\\) and gets \\(\\hat{\\beta}= 0.3\\). They then conclude that:\n\n\\(\\hat{\\beta}\\) is much less than one\nTherefore dog errors are lower than cat errors,\nTherefore dogs are easier to predict than cats.\n\nIs this a resonable conclusion? Why or why not?\n\n\n4 Simulations\nFor this problem, we will simulate data from a linear regression and confirm some of our mathematical results.\nDefine the following quantities and their corresponding R variables:\n\n\\(N\\): The number of observations\n\\(\\sigma\\): The residual standard deviation\n\\(\\boldsymbol{\\beta}\\): A \\(3\\)–dimensional regression coefficient\n\\(\\rho\\): The covariance between the regressors \\(x_{n1}\\) and \\(x_{n2}\\)\n\nWe will assume that:\n\n\\(x_{n1}\\) is normal with mean \\(1\\) and variance \\(1\\),\n\\(x_{n2}\\) is normal with mean \\(2\\) and variance \\(1\\),\n\\(\\mathrm{Cov}\\left(x_{n2}, x_{n2}\\right) = \\rho\\) (so that \\(x_{n1}\\) is not independent of \\(x_{n2}\\) if \\(\\rho \\ne 0\\)),\nThe regressor rows are independent of one another (\\(x_{n1}\\) and \\(x_{n2}\\) are independent of \\(x_{m1}\\) and \\(x_{m2}\\) for \\(m \\ne n\\)),\nThe residuals \\(\\varepsilon_n\\) are IID normal with mean \\(0\\) and variance \\(\\sigma^2\\), indepenent of all regressors, and\nFor each \\(n\\), \\(y_n = \\boldsymbol{x}_n^\\intercal\\boldsymbol{\\beta}+ \\varepsilon_n\\).\n\n(a)\nWrite a function to generate a regressors \\(\\boldsymbol{x}_1\\), \\(\\boldsymbol{x}_2\\), and \\(\\boldsymbol{Y}\\) given the inputs \\(N\\), \\(\\sigma\\), \\(\\boldsymbol{\\beta}\\), and \\(\\rho\\). You may use the functions rnorm (which generates standard normal random variables) and rmvnorm from the mvtnorm package which draws multivariate normal random variables.\nCheck that \\(\\boldsymbol{x}_1\\) and \\(\\boldsymbol{x}_2\\) have the correct covariance using cov.\n(b)\nWrite a function to compute \\(\\hat{\\boldsymbol{\\beta}}\\) for the regression \\(\\boldsymbol{Y}\\sim \\boldsymbol{X}\\boldsymbol{\\beta}\\) given \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Y}\\) as inputs.\nDonot use the lm function — use only matrix operations solve(), t(), and %*%. Your function may assume that \\(\\boldsymbol{X}\\) is full column rank. However, please do check that your answer is the same as that given by lm.\n(c)\nUsing (a) and (b), run the regression \\(y_n \\sim 1 + x_{n1} + x_{n2}\\) for the following inputs:\n\n\\(N = 5000\\)\n\\(\\sigma = 3\\)\n\\(\\boldsymbol{\\beta}= (1, 2, 3)^\\intercal\\)\n\\(\\rho = 0.8\\)\n\nHow does \\(\\hat{\\boldsymbol{\\beta}}\\) compare to the true value \\(\\boldsymbol{\\beta}\\)? Justify your observation mathematically.\n(d)\nRepeat (c), but with the regression \\(y_n \\sim 1 + x_{n1}\\). How do the estimated coefficients compare with the true intercept and coefficient for \\(x_{n1}\\)? Justify your observation mathematically.\n(e)\nRepeat (c), but with the regression \\(y_n \\sim 1 + x_{n1}\\) and with \\(\\rho = 0\\). How do the estimated coefficients compare with the true intercept and coefficient for \\(x_{n1}\\)? Justify your observation mathematically.\n(f)\nUsing the simulated data to illustrate a theoretical result of your choice from the class.\n\n\n5 Multivariate normal exercises\nLet \\(\\boldsymbol{x}\\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\right)\\) where\n\\[\n\\boldsymbol{\\mu}=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3\n\\end{pmatrix}\n\\quad\\textrm{and}\\quad\n\\boldsymbol{\\Sigma}=\n\\begin{pmatrix}\n1 &   0.5 & 0 \\\\\n0.5 & 2   & 0.1 \\\\\n0   & 0.1   & 4 \\\\\n\\end{pmatrix}.\n\\]\nLet\n\\[\n\\boldsymbol{v}=\n\\begin{pmatrix}\n0 \\\\ 1 \\\\ 0\n\\end{pmatrix}\n\\quad\n\\boldsymbol{a}=\n\\begin{pmatrix}\n2 \\\\ 1 \\\\ 1\n\\end{pmatrix}\n\\quad\n\\boldsymbol{A}=\n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n2 & 0 & 1 \\\\\n\\end{pmatrix}\n\\]\nEvaluate the following expressions.\n\n\\(\\mathbb{E}\\left[\\boldsymbol{v}^\\intercal\\boldsymbol{x}\\right]\\)\n\\(\\mathrm{Var}\\left(\\boldsymbol{v}^\\intercal\\boldsymbol{x}\\right)\\)\n\\(\\mathbb{E}\\left[\\boldsymbol{a}^\\intercal\\boldsymbol{x}\\right]\\)\n\\(\\mathrm{Var}\\left(\\boldsymbol{a}^\\intercal\\boldsymbol{x}\\right)\\)\n\\(\\mathbb{E}\\left[\\boldsymbol{v}\\boldsymbol{x}^\\intercal\\right]\\)\n\\(\\mathbb{E}\\left[\\boldsymbol{x}\\boldsymbol{x}^\\intercal\\right]\\)\n\\(\\mathbb{E}\\left[\\boldsymbol{x}^\\intercal\\boldsymbol{x}\\right]\\)\n\\(\\mathbb{E}\\left[\\mathrm{trace}\\left(\\boldsymbol{x}\\boldsymbol{x}^\\intercal\\right)\\right]\\)\n\\(\\mathbb{E}\\left[\\boldsymbol{A}\\boldsymbol{x}\\right]\\)\n\\(\\mathrm{Cov}\\left(\\boldsymbol{A}\\boldsymbol{x}\\right)\\)\n\\(\\mathbb{E}\\left[(\\boldsymbol{A}\\boldsymbol{x}) (\\boldsymbol{A}\\boldsymbol{x})^\\intercal\\right]\\)\n\\(\\mathbb{E}\\left[(\\boldsymbol{A}\\boldsymbol{x})^\\intercal(\\boldsymbol{A}\\boldsymbol{x})\\right]\\)"
  },
  {
    "objectID": "course_policies.html",
    "href": "course_policies.html",
    "title": "Syllabus and Course Structure",
    "section": "",
    "text": "Objectives\nBy the end of the course, you should be able to\n\nExpress standard regression analyses both mathematically and in R code\nCritically relate the intended use of a regression analysis to its methods and assumptions\nIdentify common practical and conceptual pitfalls of regression analysis, and to improve the analysis when possible\nCommunicate the process and results of a regression analysis simply and clearly for a broad audience, using well-organized prose, reproducible code, and effective data visualizations.\n\n\n\nAssignments, Exams, and Grading\n\nAttendance\nAttendance in lectures will be required and will contribute to the participation portion of the students’ grade. Laptops will not be permitted in lectures, and violation of this policy can constitute an absence for the purpose of the participation grade.\nIpads and phones will be permitted during lecture for note-taking as long as their use doesn’t inhibit participation.\nEach student will be given four lecture absences without losing any participation points, with the expectation that these absences will be used for illness and emergencies. Additional excused absences will be granted only with an exception granted by the Berkeley DSP office.\nAttendance at labs will be optional.\n\n\nGrading.\nThe weighting for the grades will be:\n\nHomework completion (each weighted equally): 25%\nHomework correctness (each weighted equally): 5%\nQuizzes (each weighted equally): 30%\nFinal exam: 15%\nFinal group project: 15%\nParticipation (primarily lecture attendance): 10%\n\nGrades will not be curved except where otherwise noted. Letter grades will be assigned according the weighted points earned. A score within [90-92%) will earn an A-, [92-98%) will earn an A, and [99-100%) will earn an A+. Scores in the 80’s will receive B’s, in the 70’s will receive C’s, in the 60’s will receive D’s, with the same thresholds for plusses and minuses. Scores below 60% will be considered failing. Grades will be non-negotiable.\n\n\nFinal exam.\nAn in-person pencil-and-paper final exam will be scheduled during the usual final exam week. Students will be allowed a one-page double-sided “cheatsheet” on the final exam.\n\n\nQuizzes.\nEvery two weeks we will have an thirty-minute in-class quiz, typically on the Tuesday following a homework due date. These quizzes will take the place of a sitdown midterm exam (i.e., there will be no midterm). No external materials, including cheatsheets, will be allowed during quizzes.\n\n\nHomework.\nHomework assignments will be due every two weeks on Fridays two weeks later at 9pm. I will try to release new homework on the website as soon as possible after the old one is due, typically the following Monday evening. Homework will typically consist of a combination of mathematical problems and data analysis in R. All homework will be due as a pdf via Gradescope unless otherwise noted. Students can use whatever tool they like to produce the pdf (latex, Rmd, Jupyter, scanned handwritten notes for mathematical problems, etc.).\nThe purpose of homework is for students to attempt to work through problems on their own, both to advance their own understanding, and to allow the instructors to monitor student learning. Neither of these objectives are served if students are copying answers. For that reason, thoughtful and complete homework answers will receive nearly full credit (80% of the available homework points) even if incorrect. We strongly encourage students to submit their own best efforts, even if imperfect, rather than copy a correct answer.\n\n\nFinal group project.\nStudents will form groups of up to three people to submit a final project consisting of an analysis of a real dataset applying principles and techniques from the course.\n\n\nTurning in assignments\nYou will be turning in your assignments on a platform called\nGradescope. This is also the platform where your assignments will be graded, so you can return there to get feedback on your work. You are welcome to file a regrade request if you notice that we made an error in applying the rubric to your work, but be sure to do so within a week of the grades being posted. We will not accept regrade requests past that point.\nIn order to provide flexibility around emergencies that might arise for you throughout the semester (for example, missing a quiz due to COVID), we will apply for everyone:\n\none emergency drop for quizzes\n\ntwo emergency drops for homework (applying to an entire two–week assignment)\n\nThis means that we will drop your lowest quiz score (which would be a 0 if you were absent) before computing your quiz average. For homework, we will drop your two lowest. Unless students are excused by official university policies, additional drops will not be given.\nWe strongly recommend that students reserve their emergency drops for real emergencies.\n\n\nLate Work\nLate work will not be accepted. If work is not submitted on time, it will receive a zero. It is entirely the students’ responsibility to turn work in on time. If there is any uncertainty concerning this policy, please discuss your concerns with the professor, not with the GSI or reader.\n\n\n\nPrerequisites\nThis course will assume familiarity with the material in STAT 135 or STAT 102. STAT 135 implies other prerequisite courses (STAT 134 and its prerequisites). In particular, you must have had linear algebra, so you should be familiar with basic matrix operations, vector subspaces and projections, rank and invertibility of matrices, and quadratic forms.\nThis semester of Stat151A will include labs and projects in the R language. Proficiency with R at the level of the is a prerequisite. Students with a strong background in another programming language (e.g. Python) will be permitted to enroll with the understanding that they will learn R on their own prior to the start of the class.\n\nRStudio\nThe software that we’ll be using for our data analysis is the free and open-source language called R that we’ll be interacting with via software called RStudio. If you have difficulty installing RStudio, please reach out to an instructor.\n\n\nCourse website\nAll of the assignments will be posted to the course website at https://stat151a.berkeley.edu/fall-2024/. This also holds the course notes, the syllabus, and links to Gradescope and RStudio.\n\n\n\nPolicies\n\nCourse Culture\nStudents taking STAT151A come from a wide range of backgrounds. We hope to foster an inclusive and supportive learning environment based on curiosity rather than competition. All members of the course community—the instructor, students, tutors, and readers—are expected to treat each other with courtesy and respect.\nYou will be interacting with course staff and fellow students in several different environments: in class, over the discussion forum, and in office hours. Some of these will be in person, some of them will be online, but the same expectations hold: be kind, be respectful, be professional.\nIf you are concerned about classroom environment issues created by other students or course staff, please come talk to the instructors about it.\n\n\nCollaboration policy\nYou are encouraged to collaborate with your fellow students on problem sets and labs, but the work you turn in should reflect your own understanding and all of your collaborators must be cited. The individual component of quizzes, reading questions, and exams must reflect only your work.\nResearchers don’t use one another’s research without permission; scholars and students always use proper citations in papers; professors may not circulate or publish student papers without the writer’s permission; and students may not circulate or post non-public materials (quizzes, exams, rubrics-any private class materials) from their class without the written permission of the instructor.\nThe general rule: you must not submit assignments that reflect the work of others unless they are a cited collaborator.\nThe following examples of collaboration are allowed and in fact encouraged!\n\nDiscussing how to solve a problem with a classmate.\nShowing your code to a classmate along with an error message or confusing output.\nPosting snippets of your code to the discussion forum when seeking help.\nHelping other students solve questions on the discussion with conceptual pointers or snippets of code that doesn’t whole hog give away the answer.\nGoogling the text of an error message.\nCopying small snippets of code from answers on Stack Overflow.\n\nThe following examples are not allowed:\n\nLeaving a representation of your assignment (the text, a screenshot) where students (current and future) can access it. Examples of this include websites like course hero, on a group text chain, over discord/slack, or in a file passed on to future students.\nAccessing and submitting solutions to assignments from other students distributed as above. This includes copying written answers from other students and slightly modifying the language to differentiate it.\nSearching or using generative AI to produce complete problem solutions.\nWorking on the final exam or individual quizzes in collaboration with other people or resources. These assignments must reflect individual work.\nSubmitting work on an exam that reflects consultation with outside resources or other people. Exams must reflect individual work.\n\nIf you have questions about the boundaries of the policy, please ask. We’re always happy to clarify.\n\n\nViolations of the collaboration policy\nThe integrity of our course depends on our ability to ensure that students do not violate the collaboration policy. We take this responsibility seriously and forward cases of academic misconduct to the Center for Student Conduct.\nStudents determined to have violated the academic misconduct policy by the Center for Student Conduct will receive a grade penalty in the course and a sanction from the university which is generally: (i) First violation: Non-Reportable Warning and educational intervention, (ii) Second violation: Suspension/Disciplinary Probation and educational interventions, (iii) Third violation: Dismissal.\nAgain, if you have questions about the boundaries of the collaboration policy, please ask!\n\n\nLaptop policy\nLaptops will not be permitted in lecture, but will be required for labs.\nIf you do not have access to a laptop, you can borrow one from the University library. See the UC Berkeley hardware lending program for more details. The Student Technology Equity Program is another good resource. Feel free to contact the instructor if you have concerns about your access to needed technology.\n\n\nCOVID policy\nMaintaining your health and that of the Berkeley community is of primary importance to course staff, so if you are feeling ill or have been exposed to illness, please do not come to class. All of the materials used in class will be posted to the course website. You’re encouraged to reach out to fellow students to discuss the class materials or stop by group tutoring or office hours to chat with a tutor or the instructor.\n\n\nAccomodations for students with disabilities\nStat 151A is a course that is designed to allow all students to succeed. If you have letters of accommodations from the Disabled Students’ Program, please share them with your instructor as soon as possible, and we will work out the necessary arrangements.\n\n\n\n\n\n\nNote\n\n\n\nThese course polices are based on a template and text generously shared by Andrew Bray. Thanks, Andrew!",
    "crumbs": [
      "Course Policies"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 151A: Linear Models",
    "section": "",
    "text": "RStudio\n\n  Gradescope\n\n  BCourses\n\n  ED\n\n\nNo matching items",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Statistics 151A: Linear Models",
    "section": "Instructors",
    "text": "Instructors\n\n\nInstructor: Ryan Giordano  Office: 389 Evans Hall Office hours:  Tuesdays 2-3pm  Fridays 10-11am  rgiordano@berkeley.edu pronouns: He / him\n\n\nGSI: Haodong Ling  Office: Evans 428 Office hours: Tuesdays from 4:00-5:30 PM Wednesdays from 11:00 AM-12:00 PM and 4:00-5:30 PM haodong_ling@berkeley.edu pronouns: He / him\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease reach out to Haodong if you need to be manually added to BCourses.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Statistics 151A: Linear Models",
    "section": "Course content",
    "text": "Course content\nThis website will contain lecture materials and assignments. Day-to-day announcements can be found in BCourses. Discussions can be found in ED. (See links above.)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Statistics 151A: Linear Models",
    "section": "Schedule",
    "text": "Schedule\nLectures will be held Aug 28 2024 – Dec 05 2024 on Tuesday and Thursday, 12:30 pm – 2:00 pm, in Social Science 20.\nLabs will be held on Wednesdays from 9:00 am – 11:00 am and 2:00pm – 4:00 pm in Evans 342.\n(Link to official course calendar.)\n\nLinear algebra review materials\nBasic linear algebra is a serious prerequisite for this course. You can find a summary of useful review materials here.\n\n\nR programming review materials\nThis course will be conducting in the R programming language, and basic proficiency is assumed. Here are some useful review materials:\n\nHaodong’s slides\nBerkeley SCF R bootcamp materials\nBerkeley D-Lab R fundamentals workshop",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#materials",
    "href": "index.html#materials",
    "title": "Statistics 151A: Linear Models",
    "section": "Materials",
    "text": "Materials\nUnelss otherwise noted, the primary materials for the course are the lecture notes, which will be posted to the course website in advance of class. The following textbooks are useful supplementary texts and are all freely available online:\n\nVDS: Veridical Data Science Yu, Barter\nLME: Linear Models and Extensions Ding\nROS: Regression and other Stories Gelman, Hill, Vehtari\nISL: An Introduction to Statistical Learning James, Witten, Hastie, Tibshirani\nRDS: R for Data Science, Wickham, Grolemund\nETM: Econometric Theory and Methods Davidson, MacKinnon\n\nI will additionally recommend optional readings from\n\nFPP: Statistics Freedman, Pisami, Purves.\n\nUnfortunately this book is not available digitally through any official channels.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#tentative-course-calendar",
    "href": "index.html#tentative-course-calendar",
    "title": "Statistics 151A: Linear Models",
    "section": "(Tentative) Course Calendar",
    "text": "(Tentative) Course Calendar\nThe following schedule will almost certainly change.\n\n\n\nCalendar (tentative)\n\n\n\n\n\n\n\n\n\n\nDate\nDay of week\nEvent\nTopic\nNotes\nSupplementary reading\n\n\n\n\nAug 29\nThursday\nLecture 1\nSample means as inference\n\n\n\n\nSep 3\nTuesday\nLecture 2\nSample means as loss minimization and projection\n\n\n\n\nSep 5\nThursday\nLecture 3\nInference and confounding with sample means\n\nROS 1.1-1.4\n\n\nSep 10\nTuesday\nLecture 4\nAmes Housing data for inference\n\nVDS 8.4\n\n\nSep 12\nThursday\nLecture 5\nOne-hot encoding and sample means\nHW 1 due Friday Sep 13\nVDS 10.2, ISL 3.3.1\n\n\nSep 17\nTuesday\nLecture 6\nMultilinear regression as loss minimization\n\nETM 1.4-1.5, LME 3.1\n\n\nSep 19\nThursday\nLecture 7\nExamples of the matrix form of linear regression\nQuiz 1 in class.\n\n\n\nSep 24\nTuesday\nLecture 8\nRedundant regressors\n\n\n\n\nSep 26\nThursday\nLecture 9\nLinear regression as projection\nHW 2 due Monday Sep 30\nETM 2.1-2.3, LME 3.1-3.3\n\n\nOct 1\nTuesday\nLecture 10\nTransformations of regressors\n\nVDS 10.3, ROS 10.1-10.4\n\n\nOct 3\nThursday\nLecture 11\nTransformations of responses\nQuiz 2 in class. Guest lecturer.\nETM 2.4, LMS 7-8\n\n\nOct 8\nTuesday\nLecture 12\nInfluence and Outliers\n\nLME 11\n\n\nOct 10\nThursday\nLecture 13\nInfluence and Outliers\nHW 3 due Friday Oct 11\nLME 12.2\n\n\nOct 15\nTuesday\nLecture 14\nThe FWL theorem\n\nLME 7\n\n\nOct 17\nThursday\nLecture 15\nStochastic assumptions on the residual\nQuiz 3 in class.\nETM 3.1-3.3\n\n\nOct 22\nTuesday\nLecture 16\nOmitted variables in inference and prediction\n\nLME 9.2\n\n\nOct 24\nThursday\nLecture 17\nRegression to the mean\nHW 4 due Friday Oct 25\nETM 8.2, FPP 10.4\n\n\nOct 29\nTuesday\nLecture 18\nConfidence intervals and hypothesis testing\nGuest lecturer.\nETM 4.1, ROS 4.5\n\n\nOct 31\nThursday\nLecture 19\nA hierarchy of assumptions\nQuiz 4 in class.\nETM 4.1, ROS 4.5\n\n\nNov 5\nTuesday\nLecture 20\nCoefficient tests under normality\n\nETM 4.4-4.5\n\n\nNov 7\nThursday\nLecture 21\nVariable selection and the F-test\nHW 5 due Friday Nov 8\nETM 4.4-4.5\n\n\nNov 12\nTuesday\nLecture 22\nTesting under machine learning assumptions\n\nLME 12, LME 6, ETM 5.5\n\n\nNov 14\nThursday\nLecture 23\nBias-variance tradeoff in prediction\nQuiz 5 in class.\nISL 2.2\n\n\nNov 19\nTuesday\nLecture 24\nRidge or L2 regression\n\nISL 6.2\n\n\nNov 21\nThursday\nLecture 25\nLASSO or L1 regression\n\nISL 6.2\n\n\nNov 26\nTuesday\nLecture 26\nThe difficulty of variable selection for inference\n\n\n\n\nNov 27\nThanksgiving\n(no class)\n\n\n\n\n\nNov 28\nThanksgiving\n(no class)\n\n\n\n\n\nNov 29\nThanksgiving\n(no class)\n\n\n\n\n\nDec 3\nTuesday\nLecture 27\nProject consultation\n\n\n\n\nDec 5\nThursday\nLecture 28\nTBD",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "lectures/3.5_LSAsLossMinExamples.html",
    "href": "lectures/3.5_LSAsLossMinExamples.html",
    "title": "Familiar regression examples in matrix form.",
    "section": "",
    "text": "\\(\\textcolor{white}{\\LaTeX}\\)"
  },
  {
    "objectID": "lectures/3.5_LSAsLossMinExamples.html#working-examples",
    "href": "lectures/3.5_LSAsLossMinExamples.html#working-examples",
    "title": "Familiar regression examples in matrix form.",
    "section": "Working examples",
    "text": "Working examples\nThe following examples will be used to illustrate the main ideas.\n\nThe sample mean\n\n\n\n\n\n\nNotation\n\n\n\nI will use \\(\\onev\\) to denote a vector full of ones. Usually it will be an \\(N\\)–vector, but sometimes its dimension will just be implicit. Similarly, \\(\\zerov\\) is a vector of zeros.\n\n\nWe showed earlier that the sample mean is a special case of the regression \\(\\y_n \\sim 1 \\cdot \\beta\\). This can be expressed in matrix notation by taking \\(\\X = \\onev\\) as a \\(N\\times 1\\) vector. We then have\n\\[\n\\X^\\trans \\X = \\onev^\\trans \\onev = \\sumn 1 \\cdot 1 = N,\n\\]\nso \\(\\X^\\trans \\X\\) is invertible as long as \\(N &gt; 0\\) (i.e., if you have at least one datapoint), with \\((\\X^\\trans \\X)^{-1} = 1/N\\). We also have\n\\[\n\\X^\\trans \\Y = \\onev^\\trans \\Y = \\sumn 1 \\cdot \\y_n = N \\ybar,\n\\]\nand so\n\\[\n\\betahat = (\\X^\\trans \\X)^{-1}  \\X^\\trans \\Y = (\\onev^\\trans \\onev)^{-1} \\onev^\\trans \\Y = \\frac{N \\ybar}{N} = \\ybar,\n\\]\nas expected.\n\n\nA single (possibly continuous) regressor\nSuppose that we regress \\(\\y_n \\sim \\x_n\\) where \\(\\x_n\\) is a scalar. Let’s suppose that \\(\\expect{\\x_n} = 0\\) and \\(\\var{\\x_n} = \\sigma^2 &gt; 0\\). We have\n\\[\n\\X = \\begin{pmatrix}\n\\x_1 \\\\\n\\x_2 \\\\\n\\vdots\\\\\n\\x_N\n\\end{pmatrix}\n\\]\nso\n\\[\n\\X^\\trans \\X = \\sumn \\x_n^2.\n\\]\nDepending on the distribution of \\(\\x_n\\), it may be possible for \\(\\X^\\trans \\X\\) to be non–invertible!\n\n\n\n\n\n\nExercise\n\n\n\nProduce a distribution for \\(\\x_n\\) where \\(\\X^\\trans \\X\\) is non–invertible with positive probability for any \\(N\\).\n\n\nHowever, as \\(N \\rightarrow \\infty\\), \\(\\frac{1}{N} \\X^\\trans \\X \\rightarrow \\sigma^2\\) by the LLN, and since \\(\\sigma^2 &gt; 0\\), \\(\\frac{1}{N} \\X^\\trans \\X\\) will be invertible with probability approaching one as \\(N\\) goes to infinity.\n\n\nOne–hot encodings\nWe discussed one-hot encodings in the context of the Ames housing data. Suppose we have a columns \\(k_n \\in \\{ g, e\\}\\) indicating whether a kitchen is “good” or “excellent”. A one–hot encoding of this categorical variable is given by\n\\[\n\\begin{aligned}\n\\x_{ng} =\n\\begin{cases}\n1 & \\textrm{ if }k_n = g \\\\\n0 & \\textrm{ if }k_n \\ne g \\\\\n\\end{cases}\n&&\n\\x_{ne} =\n\\begin{cases}\n1 & \\textrm{ if }k_n = e \\\\\n0 & \\textrm{ if }k_n \\ne e \\\\\n\\end{cases}\n\\end{aligned}.\n\\]\nWe can then regress \\(\\y_n \\sim \\beta_g \\x_{ng} + \\beta_e \\x_{ne} = \\x_n^\\trans \\betav\\). The corresponding \\(\\X\\) matrix might look like\n\\[\n\\begin{aligned}\n\\mybold{k} =\n\\begin{pmatrix}\ng \\\\\ne \\\\\ng \\\\\ng \\\\\n\\vdots\n\\end{pmatrix}\n&&\n\\X = (\\xv_g \\quad \\xv_e) =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 0 \\\\\n1 & 0 \\\\\n\\vdots\n\\end{pmatrix}\n\\end{aligned}\n\\]\nNote that \\(\\xv_g^\\trans \\xv_g\\) is just the number of entries with \\(k_n = g\\), and \\(\\xv_g^\\trans \\xv_e = 0\\) because a kitchen is either good or excellent but never both.\nWe then have\n\\[\n\\X^\\trans \\X =\n\\begin{pmatrix}\n\\xv_g^\\trans \\xv_g  & \\xv_g^\\trans \\xv_e \\\\\n\\xv_e^\\trans \\xv_g  & \\xv_e^\\trans \\xv_e \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\nN_g  & 0 \\\\\n0  & N_e \\\\\n\\end{pmatrix}.\n\\]\nThen \\(\\X^\\trans \\X\\) is invertible as long as \\(N_g &gt; 0\\) and \\(N_e &gt; 0\\), that is, as long as we have at least one observation of each kitchen type, and\n\\[\n\\left(\\X^\\trans \\X\\right)^{-1} =\n\\begin{pmatrix}\n\\frac{1}{N_g}  & 0 \\\\\n0  & \\frac{1}{N_e} \\\\\n\\end{pmatrix}.\n\\]\nSimilarly, \\(\\xv_g^\\trans \\Y\\) is just the sum of entries of \\(\\Y\\) where \\(k_n = g\\), with the analogous conclusion for \\(\\xv_e\\). From this we recover the result that\n\\[\n\\betavhat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\n=\n\\begin{pmatrix}\n\\frac{1}{N_g}  & 0 \\\\\n0  & \\frac{1}{N_e} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\sum_{n: k_n=g} \\y_n \\\\\n\\sum_{n: k_n=e} \\y_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{N_g} \\sum_{n: k_n=g} \\y_n \\\\\n\\frac{1}{N_e} \\sum_{n: k_n=e} \\y_n\n\\end{pmatrix}.\n\\]\nIf we let \\(\\ybar_e\\) and \\(\\ybar_g\\) denote the sample means within each group, we have shows that \\(\\betahat_g = \\ybar_g\\) and \\(\\betahat_e = \\ybar_e\\), as we proved before without using the matrix formulation."
  },
  {
    "objectID": "lectures/3.5_LSAsLossMinExamples.html#orthogonal-regressions-are-just-a-bunch-of-univariate-regressions",
    "href": "lectures/3.5_LSAsLossMinExamples.html#orthogonal-regressions-are-just-a-bunch-of-univariate-regressions",
    "title": "Familiar regression examples in matrix form.",
    "section": "Orthogonal regressions are just a bunch of univariate regressions",
    "text": "Orthogonal regressions are just a bunch of univariate regressions\nSuppose we have regressors such that the columns of regressors are orthonormal. This may seem strange at first, since we usually specify the rows of the regressors, not the columns. But in fact we have seen a near–example with one–hot encodings, which are defined row–wise, but which produce orthogonal column vectors when stacked in a matrix. If we divide a one–hot encoding by the square root of the number of ones in the whole dataset, we produce an normal column vector.\nLet’s call the design matrix with orthonormal columns \\(\\U\\). Then \\(\\U^\\trans \\U = \\id\\), the identity matrix, and so, in the regression \\(\\Y \\sim \\U \\betav\\),\n\\[\n\\betavhat = (\\U^\\trans \\U)^{-1} \\U^\\trans \\Y = \\id \\U^\\trans \\Y =\n\\begin{pmatrix}\n\\U_{\\cdot 1}^\\trans \\Y \\\\\n\\vdots \\\\\n\\U_{\\cdot P}^\\trans \\Y \\\\\n\\end{pmatrix}.\n\\]\nThis regression is particularly simple — each component of \\(\\betavhat\\) depends only on its corresponding column of \\(\\U\\), not on any of the other columns, and the multilinear regression has become \\(P\\) separate univariate regression problems.\nThis is of course the same answer we would have gotten if we had tried to write \\(\\Y\\) in the basis of the column vectors of \\(\\U\\):\n\\[\n\\begin{aligned}\n\\Y ={}& \\betahat_1 \\U_{\\cdot 1} + \\ldots + \\betahat_P \\U_{\\cdot P} = \\U \\betavhat \\Rightarrow \\\\\n\\U^\\trans \\Y ={}& \\U^\\trans \\U \\betavhat = \\betavhat.\n\\end{aligned}\n\\]\n\nUncorrelated, mean–zero regressors and orthogonality\nSuppose that we have regressors \\(\\x_{np}\\) that are all independent of one another, with \\(\\var{\\x_{np}} = \\sigma_p^2 &lt; \\infty\\) and \\(\\expect{\\x_{np}} = 0\\). If we regress \\(\\y_n \\sim \\betav^\\trans \\xv_n\\), the corresponding \\(\\X^\\trans \\X\\) matrix is given by\n\\[\n\\X^\\trans \\X =\n\\begin{pmatrix}\n\\sumn \\x_{n1}^2 & \\sumn \\x_{n1} \\x_{n2} & \\ldots \\\\\n\\sumn \\x_{n1} \\x_{n2} & \\sumn \\x_{n2}^2 & \\ldots \\\\\n\\sumn \\x_{n1} \\x_{np} & \\ldots & \\sumn \\x_{np}^2 \\\\\n\\end{pmatrix}.\n\\]\nIn general, the columns of \\(\\X\\) are not orthogonal. For example, \\(\\sumn \\x_{n1} \\x_{n2} \\ne 0\\), typically. But, but the LLN and indepedence,\n\\[\\meann \\x_{n1} \\x_{n2} \\rightarrow \\expect{\\x_{n1} \\x_{n2}} = \\expect{\\x_{n1}} \\expect{\\x_{n2}} = 0\\],\nwith analogous results for other indices. So, as \\(N\\rightarrow \\infty\\),\n\\[\n\\frac{1}{N} \\X^\\trans \\X =\n\\begin{pmatrix}\n\\sigma_1^2 & 0 & \\ldots \\\\\n0 & \\sigma_2^2 & \\ldots \\\\\n0 & \\ldots & \\sigma_p^2 \\\\\n\\end{pmatrix},\n\\]\na diagonal matrix. In this sense, \\(\\frac{1}{N} \\X\\) has asymptotically orthogonal columns.\nIn this sense, it is the dependence between different regressors within a single row that gives complexity to multilinear regression."
  },
  {
    "objectID": "lectures/3.5_LSAsLossMinExamples.html#different-ways-to-write-the-same-regression",
    "href": "lectures/3.5_LSAsLossMinExamples.html#different-ways-to-write-the-same-regression",
    "title": "Familiar regression examples in matrix form.",
    "section": "Different ways to write the same regression",
    "text": "Different ways to write the same regression\n\nOne–hot encodings and constants\nRecall in the Ames housing data, we ran the following two regressions:\n\\[\n\\begin{aligned}\n\\y_n \\sim{}& \\beta_e \\x_{ne} + \\beta_g \\x_{ng}  \\\\\n\\y_n \\sim{}& \\gamma_0  + \\gamma_g \\x_{ng} + \\res_n = \\z_n^\\trans \\gammav,\n\\end{aligned}\n\\] where I take \\(\\gammav = (\\gamma_0, \\gamma_g)^\\trans\\) and \\(\\z_n = (1, \\x_{ng})^\\trans\\).\nWe found using R that the best fits were given by\n\\[\n\\begin{aligned}\n\\betahat_e =& \\ybar_e  & \\betahat_g =& \\ybar_g \\\\\n\\gammahat_0 =& \\ybar_e  & \\gammahat_g =& \\ybar_g - \\ybar_e \\\\\n\\end{aligned}\n\\]\nWe can compute the latter by constructing the \\(\\Z\\) matrix whose rows are \\(\\z_n^\\trans\\). (We use \\(\\Z\\) to differentiate the \\(\\X\\) matrix from the previous example.) Using similar reasoning to the one–hot encoding, we see that\n\\[\n\\Z^\\trans \\Z =\n\\begin{pmatrix}\nN & N_g \\\\\nN_g & N_g\n\\end{pmatrix}.\n\\]\nThis is invertible as long as \\(N_g \\ne N\\), i.e., as long as there is at least one \\(k_n = e\\). We have\n\\[\n(\\Z^\\trans \\Z)^{-1} =\n\\frac{1}{N_g (N - N_g)}\n\\begin{pmatrix}\nN_g & -N_g \\\\\n-N_g & N\n\\end{pmatrix}\n\\quad\\textrm{and}\\quad\n\\Z^\\trans \\Y =\n\\begin{pmatrix}\n\\sumn \\y_n \\\\\n\\sum_{n: k_n=g} \\y_n \\\\\n\\end{pmatrix}\n\\]\nIt is possible (but a little tedious) to prove \\(\\gammahat_0 = \\ybar_e\\) and \\(\\gammahat_g = \\ybar_g - \\ybar_e\\) using these formulas. But an easier way to see it is as follows.\nNote that \\(\\x_{ne} + \\x_{ng} = 1\\). That means we can always re-write the regression with a constant as\n\\[\n\\y_n \\sim \\gamma_0 + \\gamma_g \\x_{ng} = \\gamma_0 (\\x_{ne} + \\x_{ng}) + \\gamma_g \\x_{ng} =\n\\gamma_0 \\x_{ne} + (\\gamma_0 + \\gamma_g) \\x_{ng}.\n\\]\nNow, we already know from the one–hot encoding case that the sum of squared residuals is minimized by setting \\(\\gammahat_0 = \\ybar_e\\) and \\(\\gammahat_0 + \\gammahat_g = \\ybar_g\\). We can then solve for \\(\\gammahat_g = \\ybar_g - \\ybar_e\\), as expected.\nThis is case where we have two regressions whose regressors are invertible linear combinations of one another:\n\\[\n\\zv_n =\n\\begin{pmatrix}\n1 \\\\\n\\x_{ng}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\x_{ne} + \\x_{ng} \\\\\n\\x_{ng}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\x_{ng} \\\\\n\\x_{ne}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0\\\\\n\\end{pmatrix}\n\\xv_n.\n\\]\nIt follows that if you can acheive a least squares fit with \\(\\xv_n^\\trans \\betavhat\\), you can achieve exactly the same fit with\n\\[\n\\betavhat^\\trans \\xv_n =  \n\\betavhat^\\trans\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0\\\\\n\\end{pmatrix}^{-1} \\zv_n,\n\\]\nwhich can be achieved by taking\n\\[\n\\gammavhat^\\trans =\n\\betavhat^\\trans\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0\\\\\n\\end{pmatrix}^{-1} \\Rightarrow\n\\gammavhat =\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0\\\\\n\\end{pmatrix}^{-T} \\betavhat\n=\n\\frac{1}{-1}\n\\begin{pmatrix}\n0 & -1 \\\\\n-1 & 1\\\\\n\\end{pmatrix} \\betavhat\n=\n\\begin{pmatrix}\n\\betahat_2 \\\\\n\\betahat_1 - \\betahat_2 \\\\\n\\end{pmatrix},\n\\]\nexactly as expected.\nWe will see this is an entirely general result: when regressions are related by invertible linear transformations of regressors, the fit does not change, but the optimal coefficients are linear transforms of one another."
  },
  {
    "objectID": "lectures/3.5_LSAsLossMinExamples.html#redundant-regressors-and-zero-eigenvalues",
    "href": "lectures/3.5_LSAsLossMinExamples.html#redundant-regressors-and-zero-eigenvalues",
    "title": "Familiar regression examples in matrix form.",
    "section": "Redundant regressors and zero eigenvalues",
    "text": "Redundant regressors and zero eigenvalues\nSuppose we run the (silly) regression \\(\\y \\sim \\alpha \\cdot 1 + \\gamma \\cdot 3 + \\res_n\\). That is, we regress on both the constant \\(1\\) and the constant \\(3\\). We have\n\\[\n\\X =\n\\begin{pmatrix}\n1 & 3 \\\\\n1 & 3 \\\\\n1 & 3 \\\\\n\\vdots\n\\end{pmatrix}\n=\n(\\onev \\quad 3 \\onev)\n\\]\nand so\n\\[\n\\X^\\trans \\X =\n\\begin{pmatrix}\n\\onev^\\trans \\onev & 3 \\onev^\\trans \\onev \\\\\n3 \\onev^\\trans \\onev & 9 \\onev^\\trans \\onev \\\\\n\\end{pmatrix}\n=\nN \\begin{pmatrix}\n1 & 3  \\\\\n3 & 9  \\\\\n\\end{pmatrix}\n\\]\nThis is not invertible (the second row is \\(3\\) times the first, and the determinant is \\(9 - 3 \\cdot 3 = 0\\)). So \\(\\betavhat\\) is not defined. What went wrong?\nOne way to see this is to define \\(\\beta = \\alpha + 3 \\gamma\\) and write\n\\[\n\\y_n = (\\alpha + 3 \\gamma) + \\res_n = \\beta + \\res_n.\n\\]\nThere is obviously only one \\(\\betahat\\) that minimizes \\(\\sumn \\res_n^2\\), \\(\\betahat = \\ybar\\). But there are an infinite set of choices for \\(\\alpha\\) and \\(\\gamma\\) satisfying\n\\[\n\\alpha + 3 \\gamma = \\betahat = \\ybar.\n\\]\nSpecifically, for any value of \\(\\gamma\\) we can take \\(\\alpha = \\ybar - 3 \\gamma\\), leaving \\(\\beta\\) unchanged. All of these choices for \\(\\alpha,\\gamma\\) acheive the same \\(\\sumn \\res_n^2\\)! So the least squares criterion cannot distinguish among them.\nIn general, this is what it means for \\(\\X^\\trans \\X\\) to be non–invertibile. It happens precisely when there are redundant regressors, and many regression coefficients that result in the same fit.\n\nAn eigenvalue perspective on the same result\nIn fact, \\(\\X^\\trans \\X\\) is invertible precisely when \\(\\X^\\trans \\X\\) has a zero eigenvalue. In the preceding example, we can see that\n\\[\n\\X^\\trans \\X\n\\begin{pmatrix}\n3 \\\\ -1\n\\end{pmatrix}\n=\nN \\begin{pmatrix}\n1 & 3  \\\\\n3 & 9  \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n3 \\\\ -1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\ 0\n\\end{pmatrix},\n\\]\nso \\((3, -1)^\\trans\\) is a zero eigenvector. (In general you might find this by numerical eigenvalue decomposition, but in this case you can just guess the zero eigenvalue.)\nGoing back to Equation 1, we see that this means that\n\\[\n\\X^\\trans \\Y =\n(\\X^\\trans \\X)\n\\begin{pmatrix}\n\\alphahat \\\\ \\gammahat\n\\end{pmatrix}\n=\nN \\begin{pmatrix}\n1 & 3  \\\\\n3 & 9  \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\alphahat \\\\ \\gammahat\n\\end{pmatrix}\n=\nN \\begin{pmatrix}\n1 & 3  \\\\\n3 & 9  \\\\\n\\end{pmatrix}\n\\left(\n\\begin{pmatrix}\n\\alphahat \\\\ \\gammahat\n\\end{pmatrix}\n+ C\n\\begin{pmatrix}\n3 \\\\ -1\n\\end{pmatrix}\n\\right)\n\\]\nfor any value of \\(C\\). This means there are an infinite set of “optimal” values, all of which set the gradient of the loss to zero, and all of which have the same value of the loss function (i.e. acheive the same fit). And you can check that these family of values are exactly the ones that satisfy \\(\\alpha + 3 \\gamma = \\betahat = \\ybar\\), since\n\\[\n\\alpha + 3 \\gamma =\n\\begin{pmatrix}\n1 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\n\\alpha \\\\ \\gamma\n\\end{pmatrix}\n\\quad\\quad\\textrm{and}\\quad\\quad\n\\begin{pmatrix}\n1 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\n3 \\\\ -1\n\\end{pmatrix} = 0.\n\\]\nSoon, we will see that this is a general result: when \\(\\X^\\trans \\X\\) is not invertible, that means there are many equivalent least squares fits, all characterized precisely by the zero eigenvectors of \\(\\X^\\trans \\X\\).\n\n\nZero variance regressors\nAn example of redundant regressors occurs when the sample variance of \\(\\x_n\\) is zero and a constant is included in the regression. Specifically, suppose that \\(\\overline{xx} - \\overline{x}^2 = 0\\).\n\n\n\n\n\n\nExercise\n\n\n\nProve that \\(\\overline{xx} - \\overline{x}^2 = 0\\) means \\(\\x_n\\) is a constant with \\(\\x_n = \\xbar\\). Hint: look at the sample variance of \\(\\x_n\\).\n\n\nLet’s regress \\(\\y_n \\sim \\beta_1 + \\beta_2 \\x_n\\).\nFor simplicity, let’s take \\(\\x_n = 3\\). In that case we can rewrite our estimating equation as\n\\[\n\\y_n = \\beta_1 + \\beta_2 \\x_n + \\res_n\n     = (\\beta_1 + \\beta_2 \\xbar) + \\res_n.\n\\]\nWe’re thus in the previous setting with \\(\\xbar\\) in place of the number \\(3\\)."
  },
  {
    "objectID": "lectures/4.5_LSAsProjectionProbabilistic.html",
    "href": "lectures/4.5_LSAsProjectionProbabilistic.html",
    "title": "Least squares as a projection: a probabilistic interpretation",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\n\\newcommand{\\f}{f}\n\\newcommand{\\fhat}{\\hat{f}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{#1}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\\def\\Q{\\mybold{Q}}\n\n\n\\def\\eps{\\varepsilon}\n\n$$\n\n\n\n\n\nProbabilistic interpretation (bonus content)\nWe showed in last lecture that if we have a regressor matrix \\(\\Z\\) such that the entries of \\(\\Z\\) are IID, mean zero, uncorrelated, and unit variance, then by the LLN,\n\\[\n\\begin{aligned}\n\\frac{1}{N} \\Z^\\trans \\Z \\rightarrow \\id.\n\\end{aligned}\n\\]\nFor a general \\(\\X\\), the rows \\(\\xv_n\\) are of course not uncorrelated, mean zero, nor unit variance. However, if we assume that the rows are independent, we can de-correlated them if we know the second moment matrix \\(\\Xcov := \\expect{\\xv_n \\xv_n^\\trans}\\). (Note that \\(\\Xcov\\) is not a covariance unless \\(\\expect{\\xv_n} = \\zerov\\).) If we define\n\\[\n\\zv_n = \\Xcov^{-1/2} \\xv_n,\n\\]\nthen\n\\[\n\\begin{aligned}\n\\expect{\\zv_n \\zv_n^\\trans}\n={}&\n\\expect{\\Xcov^{-1/2} \\xv_n \\xv_n^\\trans \\Xcov^{-1/2}}\n& \\textrm{(note that }\\Xcov^{-1/2}\\textrm{ is symmetric)}\n\\\\={}&\n\\Xcov^{-1/2} \\expect{ \\xv_n \\xv_n^\\trans} \\Xcov^{-1/2}\n& \\textrm{(note that }\\Xcov^{-1/2}\\textrm{ is not random)}\n\\\\={}&\n\\Xcov^{-1/2} \\Xcov \\Xcov^{-1/2}\n\\\\={}&\n(\\Xcov^{-1/2} \\Xcov^{1/2}) (\\Xcov^{1/2} \\Xcov^{-1/2})\n\\\\={}&\n\\id.\n\\end{aligned}\n\\]\nThat is, \\(\\zv_n\\) are almost like ``de–correlated’’ versions of \\(\\xv_n\\), except for the fact that \\(\\expect{\\zv_n \\zv_n^\\trans}\\) is a second moment matrix, not a correlation matrix, because \\(\\expect{\\zv_n} \\ne \\zerov\\).\nWe don’t know \\(\\Xcov\\), of course, but note that\n\\[\n\\begin{aligned}\n\\frac{1}{N} \\X^\\trans \\X \\rightarrow \\Xcov\n\\end{aligned}\n\\]\nby the LLN. So by regressing on\n\\[\n\\begin{aligned}\n\\uv_n :=  \\left( \\frac{1}{N} \\X^\\trans \\X \\right)^{-1/2} \\xv_n,\n\\end{aligned}\n\\]\nwe are regressing on a new linear combination of regressors whose second moment matrix should be approximately \\(\\id\\) for large \\(N\\). (Note that you cannot apply the LLN to \\(\\uv_n\\), since each entry depends on all the entires in \\(\\X\\), and so \\(\\uv_n\\) are not independent.) The regression on \\(\\U\\) gives\n\\[\n\\begin{aligned}\n\\U ={}& \\X  \\left( \\frac{1}{N} \\X^\\trans \\X \\right)^{-1/2} \\\\\n\\U^\\trans \\U ={}& N \\left( \\frac{1}{N} \\X^\\trans \\X \\right)^{-1/2}\n\\frac{1}{N} \\X^\\trans \\X  \\left( \\frac{1}{N} \\X^\\trans \\X \\right)^{-1/2} \\\\\n={}& N \\id.\n\\end{aligned}\n\\]\nSo \\(\\U\\) is in fact exactly an orthogonal design matrix."
  },
  {
    "objectID": "lectures/5.5_Transformations_Y.html",
    "href": "lectures/5.5_Transformations_Y.html",
    "title": "Transforming responses.",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\n\\newcommand{\\f}{f}\n\\newcommand{\\fhat}{\\hat{f}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{#1}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\\def\\Q{\\mybold{Q}}\n\n\n\\def\\eps{\\varepsilon}\n\n$$\n\n\n\n\n\\(\\,\\) \n\n\nGoals\n\nDiscuss transformations of responses\n\nEffect of re-scaling and the units of the coefficients\nExample: Kleiber’s scaling law\nThe log transformation\nInterpreting transformed regressions\n\n\n\n\n\nAnimal metabolic data\nLet’s look at data taken from Table 2 of Kleiber 1947, “Body Size and Metabolic Rate.”\nThe data consists of weight (in Kg) and metabolic rate (in kCal per day) for the following animals:\nMouse, Rate, Guinea pig, Rabbit, Cat, Macaque, Dog, Goat, Chimpanzee, Sheep, Human Woman, Cow, Beef heifers, Shrew, Swiss mice, Dwarf mouse, Rat (giant), Rat (growth hormone), Swine, Steer calves, Elephant, Porpoise, Whale.\nKleiber’s question is: is there a systematic relationship between weight and metabolic rate? We are interested in this question because it can shed light on fundamental features of animal biology, not because we need to know metaboloic rate for some animal who we managed to get onto a scale.\nQuestion: Is this an inference problem or a prediction problem?\n\n\n\nAnimal metabolic data\nLet’s look at a simple linear regression of metabolism on weight.\n\nlm_base_fit &lt;- lm(Metabol_kcal_per_day ~ 1 + Weight_kg, kleiber_df)\nprint(summary(lm_base_fit))\n\n\nCall:\nlm(formula = Metabol_kcal_per_day ~ 1 + Weight_kg, data = kleiber_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1636.2 -1527.6  -974.0  -297.7  6462.5 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1637.1946   417.0693   3.925 0.000401 ***\nWeight_kg      0.1524     0.0357   4.269 0.000149 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2461 on 34 degrees of freedom\nMultiple R-squared:  0.349, Adjusted R-squared:  0.3298 \nF-statistic: 18.22 on 1 and 34 DF,  p-value: 0.0001488\n\n\nThere is a relationship between weight and metabolism, and it’s statistically significant!\nClass dismissed?\n\n\n\nOutliers\nLet’s plot the data. What do you think of the fit now?\n\n\n\n\n\n\n\n\n\nMoral: Always plot your data.\nNote that a few points were flagged by Kleiber as not having comparable metabolic rates, including the whale. Let’s do the same analysis without them.\n\n\n\nNow without outliers\n\nlm_lin_fit &lt;- lm(Metabol_kcal_per_day ~ 1 + Weight_kg, kleiber_comp_df)\nprint(summary(lm_lin_fit)$coefficients)\n\n             Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 273.85249 101.7051223  2.692613 1.272021e-02\nWeight_kg    14.76166   0.5497794 26.850158 2.046497e-19\n\n\n\n\n\n\n\n\n\n\n\nWhat do you think of this fit?\n\n\n\nUnit change\nQuestion: We’re mixing up English units (kcal) and metric units (Kg). What would happen to the regression of we regress on Metabol_joule_per_day, using the conversion rate 4184 Joules per kcal?\n\nlm_joule_fit &lt;- lm(Metabol_joule_per_day ~ 1 + Weight_kg, kleiber_comp_df)\n\ncoefficients(lm_joule_fit)\n\n(Intercept)   Weight_kg \n  1145798.8     61762.8 \n\ncoefficients(lm_lin_fit)\n\n(Intercept)   Weight_kg \n  273.85249    14.76166 \n\n\nThe ratio of the new to the old coefficients is the unit change, as expected from the fact that \\(\\betavhat = (\\X^\\trans \\X)^{-1} \\Y\\).\n\ncoefficients(lm_joule_fit) / coefficients(lm_lin_fit)\n\n(Intercept)   Weight_kg \n       4184        4184 \n\njoule_per_kcal\n\n[1] 4184\n\n\n\n\n\nMore careful thought\nHere’s a chain of reasoning that leads to a different regression.\n\nDifferent animals’ density is approximately constant\n\n\\(\\Rightarrow\\) Animal weight in kg \\(\\propto\\) Animal volume in \\(m^3\\)\n\nDifferent animals’ shape is approximately the same\n\n\\(\\Rightarrow\\) Animal surface area in \\(m^2\\) \\(\\propto\\) Animal volume in \\((m^3)^{2/3}\\) \\(\\propto\\) Animal kg\\(^{2/3}\\)\n\nMetabolic rate is proportional to surface area\n\nFirst law of thermodynamics\nAll generated heat must be radiated\nRate of radiation is proportional to surface area\n\\(\\Rightarrow\\) Animal metabolic rate in Joules \\(\\propto\\) Animal surface area in \\(m^2\\)\n\n\nTestable hypothesis: Animal metabolic rate in Joules \\(\\propto\\) Animal kg\\(^{2/3}\\).\n\n\n\nTesting the hypothesis\nTestable hypothesis: Animal metabolic rate in Joules \\(\\propto\\) Animal kg\\(^{2/3}\\).\nQuestion: How can we test this with regression?\nWhat’s wrong with doing a regressor transform and regressing \\(\\y_n \\sim \\x_n^{2/3}\\), where \\(\\y_n\\) is metabolism and \\(\\x_n\\) is weight?\n\n\n\nTesting the hypothesis\nTestable hypothesis: Animal metabolic rate in Joules \\(\\propto\\) Animal kg\\(^{2/3}\\).\nA better idea:\n\\[\n\\begin{aligned}\n\\yhat_n ={} \\beta_0 \\x_n^{\\beta_1}\n\\quad\\quad\\Leftrightarrow\\quad\\quad\n\\log \\yhat_n = \\log \\beta_0 + \\beta_1 \\log \\x_n.\n\\end{aligned}\n\\]\nLet’s regress \\(\\log \\y_n \\sim \\log \\x_n\\), and see whether the coefficient is \\(\\betahat_1 \\approx 2/3\\).\nNote that the errors we’re trying to minimize mean something different! Compare\n\\[\n\\begin{aligned}\n\\y_n =& \\gamma_0 + \\gamma_1 \\x_n^{2/3} + \\eta_n\n\\end{aligned}\n\\]\nversus\n\\[\n\\begin{aligned}\n\\log \\y_n =& \\beta_0 + \\beta_1 \\x_n + \\res_n \\quad \\Rightarrow\\\\\n\\y_n ={}& \\exp(\\beta_0) \\x_n^{\\beta_1} \\exp(\\res_n).\n\\end{aligned}\n\\]\nMinimizing \\(\\sumn \\eta_n^2\\) is very different from minimizing \\(\\sumn \\res_n^2\\).\n\n\n\nLog fit\n\nlm_log_fit &lt;- lm(log10(Metabol_joule_per_day) ~ 1 + log10(Weight_kg), kleiber_comp_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare the two fits on their respective scales\n\n\n\n\n\n\n\n\n\n\n\n\nWhat about our hypothesis?\n\nsummary(lm_log_fit)$coefficients\n\n                  Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept)      5.4466814 0.013652364 398.95519 2.222861e-47\nlog10(Weight_kg) 0.7564294 0.009055443  83.53312 4.246511e-31\n\n\nNote that\n\\[\n0.7564294 \\ne 2 / 3 \\approx 0.666.\n\\]\nWe haven’t talked about standard errors yet, but note that\n\\[\n0.7564294\n- 0.666 =\n0.0897627,\n\\]\nwhich is very large relative to the reported “standard error.”\n(We will revisit this example later to investigate the assumptions behind the standard error, and find that they’re not likely to apply in this case.)\nIn fact, “Kleiber’s law” refers to the relationship\n\\[\n\\textrm{Metabolism} \\propto \\textrm{Weight}^{3/4},\n\\]\nwhich appears roughly consistent with what we’ve found here. The reason for this scaling has been the subject of a lot of research since then and is beyond the scope of the present lecture."
  },
  {
    "objectID": "lectures/6_FWLTheorem.html",
    "href": "lectures/6_FWLTheorem.html",
    "title": "The FWL theorem and coefficient interpretation.",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\n\\newcommand{\\f}{f}\n\\newcommand{\\fhat}{\\hat{f}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{#1}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\\def\\Q{\\mybold{Q}}\n\n\n\\def\\eps{\\varepsilon}\n\n$$\n\n\n\n\n\nGoals\n\nDiscuss the FWL theorem and some uses and consequences\n\nInclusion of a constant\nLinear regression as the marginal association\nThe FWL theorem for visualization\n\n\n\n\nSimple regression\nRecall our formulas for simple linear regression. If \\(\\y_n \\sim \\beta_1 + \\beta_2 \\x_n\\), then\n\\[\n\\begin{align*}\n\\betahat_1 = \\overline{y} - \\betahat_2 \\overline{x}\n\\quad\\textrm{and}\\quad\n\\betahat_2 ={}\n  \\frac{\\sumn \\left( \\y_n - \\ybar \\right) \\left(\\x_n - \\xbar \\right)}\n       {\\sumn \\left( \\x_n - \\xbar \\right)^2}.\n\\end{align*}\n\\]\nInterestingly, note that if we define \\(\\y'_n = \\y_n - \\ybar\\) and \\(\\x'_n = \\x_n - \\xbar\\), the “de–meaned” or “centered” versions of the response and regressor, then we could also have computed the regression \\(\\y'_n \\sim \\gamma_2 \\x'_n\\) and gotten the same answer:\n\\[\n\\gammahat_2 = \\frac{\\sumn \\y'_n \\x'_n}{\\sumn \\x'_n \\x'_n}\n=\n  \\frac{\\sumn \\left( \\y_n - \\ybar \\right) \\left(\\x_n - \\xbar \\right)}\n       {\\sumn \\left( \\x_n - \\xbar \\right)^2} =\n\\betahat_2.\n\\]\nIs this a coincidence? We’ll see today that it’s not.\n\n\nCorrelated regressors\nTake \\(\\X = (\\xv_1, \\ldots, \\xv_P)\\), where \\(\\xv_1 = \\onev\\), so that we are regressing on \\(P-1\\) regressors and a constant. If the regressors are all orthogonal to one another (\\(\\xv_k^\\trans \\xv_j = 0\\) for \\(k \\ne j\\)), then we know that\n\\[\n\\betahat = (\\X^\\trans\\X)^{-1} \\X^\\trans \\Y =\n\\begin{pmatrix}\n\\xv_1^\\trans \\xv_1 & 0 & \\ldots 0 \\\\\n0 & \\ddots &  0 \\\\\n0 &  \\ldots &  \\xv_P^\\trans \\xv_P  \\\\\n\\end{pmatrix}^{-1}\n\\begin{pmatrix}\n\\xv_1^\\trans \\Y \\\\\n\\vdots \\\\\n\\xv_P^\\trans \\Y \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{\\xv_1^\\trans \\Y}{\\xv_1^\\trans \\xv_1} \\\\\n\\vdots \\\\\n\\frac{\\xv_P^\\trans \\Y}{\\xv_P^\\trans \\xv_P} \\\\\n\\end{pmatrix}.\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nWhat is the limiting behavior of \\(\\frac{1}{N}\\X^\\trans \\X\\) when the \\(\\xv_n\\) are independent of one another? What if they are independent and \\(\\expect{\\xv_n} = 0\\), except for a constant \\(\\xv_{n1} = 1\\)?\n\n\nHowever, typically the regressors are not orthogonal to one another. When they are not, we can ask\n\nHow can we interpret the coefficients?\nHow does the relation between the regressors affect the \\(\\betavhat\\) covariance matrix?\n\n\n\nThe FWL theorem\nThe FWL theorem gives an expression for sub-vectors of \\(\\betavhat\\). Specifically, let’s partition our regressors into two sets:\n\\(\\y_n \\sim \\xv_n^\\trans \\beta = \\av_{n}^\\trans \\betav_a + \\bv_{n}^\\trans \\betav_b\\),\nwhere \\(\\betav^\\trans = (\\betav_a^\\trans, \\betav_b^\\trans)\\) and \\(\\xv_n^\\trans = (\\av_n^\\trans, \\bv_n^\\trans)\\). We can similarly partition our regressors matrix into two parts \\[\n\\X = (\\X_a \\, \\X_b).\n\\]\nA particular example to keep in mind is where\n\\[\n\\begin{aligned}\n\\xv_n^\\trans =& (\\x_{n2}, \\ldots, \\x_{n(P-1)}, 1)^\\trans \\\\\n\\bv_n =& (1) \\\\\n\\av_n^\\trans =& (\\x_{n2}, \\ldots, \\x_{n(P-1)})^\\trans \\\\\n\\end{aligned}\n\\]\nLet us ask what is the effect on \\(\\betav_a\\) of including \\(\\bv_n\\) as a regressor?\nThe answer is given by the FWL theorem. Recall that\n\\[\n\\resvhat = \\Y - \\X\\betavhat = \\Y - \\X_a \\betavhat_a - \\X_b \\betavhat_b,\n\\]\nand that \\(\\X^\\trans \\resvhat = \\zerov\\), so \\(\\X_a^\\trans \\resvhat = \\zerov\\) and \\(\\X_b^\\trans \\resvhat = \\zerov\\). Recall also the definition of the projection matrix perpendicular to the span of \\(\\X_b\\):\n\\[\n\\proj{\\X_b^\\perp} := \\id{} - \\X_b (\\X_b^\\trans \\X_b)^{-1} \\X_b^\\trans.\n\\]\nApplying \\(\\proj{\\X_b^\\perp}\\) to both sides of \\(\\resvhat = \\Y - \\X\\betavhat\\) gives\n\\[\n\\proj{\\X_b^\\perp} \\resvhat = \\resvhat = \\proj{\\X_b^\\perp} \\Y - \\proj{\\X_b^\\perp} \\X_a \\betavhat_a - \\proj{\\X_b^\\perp} \\X_b \\betavhat_b\n= \\proj{\\X_b^\\perp} \\Y - \\proj{\\X_b^\\perp} \\X_a \\betavhat_a.\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nVerify that \\(\\proj{\\X_b^\\perp} \\resvhat = \\resvhat\\) and \\(\\proj{\\X_b^\\perp} \\X_b \\betavhat_b = \\zerov\\).\n\n\nNow appying \\((\\proj{\\X_b^\\perp} \\X_a)^\\trans\\) to both sides of the preceding expression gives\n\\[\n\\begin{aligned}\n\\X_a^\\trans \\resvhat ={}& \\zerov = \\X_a^\\trans \\proj{\\X_b^\\perp} \\Y - \\X_a^\\trans \\proj{\\X_b^\\perp} \\X_a \\betavhat_a\n  \\quad \\Rightarrow \\\\\n\\left(\\proj{\\X_b^\\perp} \\X_a \\right)^\\trans\n  \\proj{\\X_b^\\perp} \\X_a \\betavhat_a  ={}&  \n\\left(\\proj{\\X_b^\\perp} \\X_a \\right)^\\trans \\proj{\\X_b^\\perp} \\Y\n\\end{aligned}\n\\]\nIf we assume that \\(\\X\\) is full-rank, then \\(\\proj{\\X_b^\\perp} \\X_a\\) must be full-rank as well, since otherwise one of the columns of \\(\\X_a\\) would be a linear combination of columns of \\(\\X_b\\). Therefore we can invert to get\n\\[\n\\betavhat_a = \\left((\\proj{\\X_b^\\perp} \\X_a)^\\trans \\proj{\\X_b^\\perp} \\X_a \\right)^{-1}\n\\left(\\proj{\\X_b^\\perp} \\X_a \\right)^\\trans \\proj{\\X_b^\\perp} \\Y.\n\\]\nThis is exactly the same as the linear regression\n\\[\n\\tilde{\\Y} \\sim \\tilde{\\X_a} \\betav_a \\quad\\textrm{where }\n\\tilde{\\X_a} := \\proj{\\X_b^\\perp} \\X_a \\textrm{ and } \\tilde{\\Y} := \\proj{\\X_b^\\perp} \\Y.\n\\]\nThat is, the OLS coefficient on \\(\\X_a\\) is the same as projecting all the responses and regressors to a space orthogonal to \\(\\X_b\\), and running ordinary regression.\nSee section 7.3 of Prof. Ding’s book for a more rigorous proof, which uses the Schur representation of sub-matrices of \\((\\X^\\trans \\X)^{-1}\\).\n\nThe special case of a constant regressor\nSuppose we want to regress \\(\\Y \\sim \\beta_0 + \\betav^\\trans \\xv_n\\). We’d like to know what \\(\\betavhat\\) is, and in particular, what is the effect of including a constant.\nWe can answer this with the FWL theorem by taking \\(\\bv_n = (1)\\) and \\(\\av_n = \\xv_n\\). Then \\(\\betavhat\\) will be the same as in the regression\n\\[\n\\tilde{Y} \\sim \\tilde{X} \\betav\n\\]\nwhere \\(\\tilde{Y} = \\proj{\\X_b^\\perp} \\Y\\) and \\(\\tilde{X} = \\proj{\\X_b^\\perp} \\X\\).\nA particular special case is useful for intuition. Take \\(\\xv_b\\) to simply be the constant regressor, \\(1\\). Then \\(\\X_b = \\onev\\), and\n\\[\n\\proj{\\X_b^\\perp} = \\id{} - \\onev (\\onev^\\trans \\onev)^{-1} \\onev^\\trans = \\id{} - \\frac{1}{N} \\onev \\onev^\\trans.\n\\]\n\\(\\onev^\\trans \\onev = \\sumn 1 \\cdot 1 = N\\)\nIf we take\n\\[\n\\begin{aligned}\n\\onev^\\trans \\Y =& \\sumn 1 \\cdot \\y_n = \\sumn \\y_n\\\\\n\\frac{1}{N} \\onev^\\trans \\Y =& \\meann 1 \\cdot \\y_n = \\meann \\y_n = \\ybar\\\\\n\\onev \\frac{1}{N} \\onev^\\trans \\Y =& \\onev \\ybar =\n\\begin{pmatrix}\n\\ybar \\\\\n\\vdots \\\\\n\\ybar\n\\end{pmatrix} \\\\\n\\proj{\\X_b^\\perp}  \\Y =\n\\left(\\id - \\onev \\frac{1}{N} \\onev^\\trans \\right) \\Y =&\n\\Y - \\begin{pmatrix}\n\\ybar \\\\\n\\vdots \\\\\n\\ybar\n\\end{pmatrix}\n= \\begin{pmatrix}\ny_1 - \\ybar \\\\\ny_2 - \\ybar \\\\\n\\vdots \\\\\ny_N - \\ybar\n\\end{pmatrix}\n\\end{aligned}\n\\]\nThe projection matrix \\(\\proj{\\X_b^\\perp}\\) thus simply centers a vector at its sample mean.\nSimilarly,\n\\[\n\\tilde{\\X_a} := \\proj{\\X_b^\\perp} \\X_a = \\X_a - \\frac{1}{N} \\onev \\onev^\\trans \\X_a\n= \\X_a - \\onev \\xbar^\\trans \\\\\n\\textrm{ where } \\xbar^\\trans := \\begin{pmatrix} \\meann \\x_{n1} & \\ldots & \\meann \\x_{n(P-1)} \\end{pmatrix},\n\\]\nso that the \\(n\\)–th row of \\(\\proj{\\X_b^\\perp} \\X_a\\) is simply \\(\\xv_n^\\trans - \\xbar^\\trans\\), and each regressor is centered. So\n\\[\n\\betavhat_a = (\\tilde{\\X_a}^\\trans \\tilde{\\X_a})^\\trans \\tilde{\\X_a}^\\trans \\tilde{\\Y},\n\\]\nthe OLS estimator where both the regressors and responses have been centered at their sample means. In this case, by the LLN,\n\\[\n\\frac{1}{N} \\tilde{\\X_a}^\\trans \\tilde{\\X_a} \\rightarrow \\cov{\\xv_n},\n\\]\nin contrast to the general case, where\n\\[\n\\frac{1}{N} \\X^\\trans \\X \\rightarrow \\expect{\\xv_n \\xv_n^\\trans} = \\cov{\\xv_n} + \\expect{\\xv_n}\\expect{\\xv_n^\\trans}.\n\\]\nFor this reason, when thinking about the sampling behavior of OLS coefficients where a constant is included in the regression, it’s enough to think about the covariance of the regressors, rather than the outer product.\n\n\n\n\n\n\nExercise\n\n\n\nDerive our simple least squares estimator of \\(\\betavhat_1\\) using the FWL theorem."
  },
  {
    "objectID": "lectures/7_InfluenceAndOutliers.html",
    "href": "lectures/7_InfluenceAndOutliers.html",
    "title": "Influence and outliers",
    "section": "",
    "text": "Discuss some was that extreme data can influence regression\n\nThere is no clear definition of an outlier\nThe (unbounded) influence of outlying \\(y_n\\)\n\nLook at residuals using fit$residuals\n\nThe influence of outyling \\(x_n\\) and the leverage score\n\nLook at leverage scores using hatvalues\n\nThe influence of removing a point (both leverage and residual)\n\nLook at the influence function using influence"
  },
  {
    "objectID": "lectures/7_InfluenceAndOutliers.html#outliers-in-the-births-data",
    "href": "lectures/7_InfluenceAndOutliers.html#outliers-in-the-births-data",
    "title": "Influence and outliers",
    "section": "Outliers in the births data",
    "text": "Outliers in the births data\nWe get pretty different slopes with and without those three very old fathers!\n\nage_threshold &lt;- 50\nreg_drop &lt;- lm(lm_form, births_df %&gt;% filter(fage &lt; age_threshold))\n\nsummary(reg_all)$coefficients\n\n               Estimate  Std. Error    t value      Pr(&gt;|t|)\n(Intercept) 7.104174083 0.193584607 36.6980319 7.158765e-180\nfage        0.004726717 0.006064235  0.7794416  4.359283e-01\n\nsummary(reg_drop)$coefficients\n\n              Estimate  Std. Error   t value      Pr(&gt;|t|)\n(Intercept) 6.94399958 0.202478955 34.294920 2.086328e-164\nfage        0.01010549 0.006382714  1.583259  1.137214e-01"
  },
  {
    "objectID": "lectures/7_InfluenceAndOutliers.html#unusual-responses-look-at-residuals",
    "href": "lectures/7_InfluenceAndOutliers.html#unusual-responses-look-at-residuals",
    "title": "Influence and outliers",
    "section": "Unusual responses (look at residuals)",
    "text": "Unusual responses (look at residuals)\nOutlier \\(\\y_n\\) will also tend to have outlier residuals.\nTo see this, let’s suppose there is one abberant value, \\(\\y_*\\), which is very large, and which we enumerate separately from the well-behaved \\(\\y_n\\).\nAssume that \\(\\xv_*\\) is not an outlier (i.e. the response is an outlier but the regressor is not). That means \\(\\frac{1}{N} (\\X^\\trans \\X + \\xv_* \\xv_*^\\trans) \\approx \\frac{1}{N} \\X^\\trans \\X\\). Let \\(\\betavhat_*= (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\\) denote the estimated coefficient without the outlier.\nWe can write the data together with the outlier as\n\\[\n\\begin{pmatrix}\n\\Y \\\\\n\\y_*\n\\end{pmatrix}\n\\quad\\quad\n\\begin{pmatrix}\n\\X \\\\\n\\xv_*^\\trans\n\\end{pmatrix}\n\\] We have\n\\[\n\\begin{aligned}\n\\reshat_* ={}& \\y_* - \\yhat_*\n\\\\={}& \\y^* - \\xv_*^\\trans \\betahat\n\\\\={}& \\y^* - \\xv_*^\\trans (\\X^\\trans \\X + \\xv_* \\xv_*^\\trans)^{-1} (\\X^\\trans \\Y + \\xv_* \\y_*)\n\\\\={}& \\y^* - \\xv_*^\\trans \\left(\n    \\frac{1}{N} \\left(\n    \\X^\\trans \\X + \\xv_* \\xv_*^\\trans\n    \\right)\n  \\right)^{-1}\n  \\frac{1}{N} (\\X^\\trans \\Y + \\xv_* \\y_*)\n\\\\\\approx{}& \\y^* - \\xv_*^\\trans \\left(\n    \\frac{1}{N} \\left(\n    \\X^\\trans \\X\n    \\right)\n  \\right)^{-1}\n  \\frac{1}{N} (\\X^\\trans \\Y + \\xv_* \\y_*)\n\\\\=& \\y^* - \\xv_*^\\trans \\betavhat_* +\n  \\frac{1}{N} \\xv_*^\\trans\n  \\left(\\frac{1}{N} \\X^\\trans \\X \\right)^{-1} \\xv_* \\y_*\n\\\\\\approx& \\y^* - \\xv_*^\\trans \\betavhat_*\n\\end{aligned}\n\\]\nThis means that although the \\(\\y_*\\) causes the \\(\\betahat\\) to grow very large in an attempt to fit it, its residual remains large, and with the same sign as \\(\\y_*\\).\nThis means you may be able to identify outlier responses by looking at a residual plot, e.g., a histogram of residuals, and seeing if any fitted residuals are atypical."
  },
  {
    "objectID": "lectures/7_InfluenceAndOutliers.html#microcredit-data",
    "href": "lectures/7_InfluenceAndOutliers.html#microcredit-data",
    "title": "Influence and outliers",
    "section": "Microcredit data",
    "text": "Microcredit data\nLet’s take a look at some data from a study of microcredit in Mexico. The goal of the study is to estimate the effect of microcredit, which was randomly allocated.\nFor example, we might try to measure the effect of the treatment on “temptation goods.” We run the model \\(\\textrm{Temptation spend} ~ 1 + \\textrm{treatment}\\):\n\nreg_mx &lt;- lm(temptation ~ treatment, mx_df)\nsummary(reg_mx)\n\n\nCall:\nlm(formula = temptation ~ treatment, data = mx_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n -4.656  -3.985  -1.925   1.667 101.306 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.65605    0.06635  70.177   &lt;2e-16 ***\ntreatment   -0.09604    0.09393  -1.022    0.307    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.044 on 16558 degrees of freedom\nMultiple R-squared:  6.314e-05, Adjusted R-squared:  2.746e-06 \nF-statistic: 1.045 on 1 and 16558 DF,  p-value: 0.3066\n\n\nHowever, we see that there are huge residuals:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThis is because spending has a very heavy tail!\n\n\n\n\n\n\n\n\n\nWe can produce big changes in the regression by dropping the 5 largest residuals (which is only 0.0301932 percent of the data):\n\nprint(\"With all datapoints:\")\n\n[1] \"With all datapoints:\"\n\nsummary(reg_mx)$coefficients\n\n               Estimate Std. Error   t value  Pr(&gt;|t|)\n(Intercept)  4.65605325 0.06634731 70.176975 0.0000000\ntreatment   -0.09604309 0.09393141 -1.022481 0.3065682\n\nprint(\"Without the largest residual datapoints:\")\n\n[1] \"Without the largest residual datapoints:\"\n\nsummary(reg_drop_mx)$coefficients\n\n              Estimate Std. Error   t value  Pr(&gt;|t|)\n(Intercept)  4.6359381 0.06444850 71.932439 0.0000000\ntreatment   -0.1053137 0.09124597 -1.154174 0.2484456"
  },
  {
    "objectID": "lectures/7_InfluenceAndOutliers.html#effect-of-high-leverage",
    "href": "lectures/7_InfluenceAndOutliers.html#effect-of-high-leverage",
    "title": "Influence and outliers",
    "section": "Effect of high leverage",
    "text": "Effect of high leverage\nWhat happens to a regression when you have very high leverage? Suppose that \\(h_n = 1\\). That means that the vector \\(\\ev_n\\), which has \\(1\\) in entry \\(n\\) and \\(0\\) otherwise, is an eigenvector of \\(\\proj{\\X}\\), and consequently \\(\\ev_n\\) is in the column span of \\(\\X\\). This means that the \\(n\\)–th entry of \\(\\proj{\\X} \\Y\\) is given by\n\\[\n\\yhat_n = \\left( \\proj{\\X} \\Y \\right)_n = \\y_n.\n\\]\nThis is the same as \\(\\y_n \\approx \\yhat_n = \\betavhat^\\trans \\xv_n\\), which forces the fit to pass through the point \\((\\xv_n, \\y_n)\\).\nIf there are \\(P\\) such high–leverage points, then \\(\\betavhat\\) is completely determined by these points.\nNote that, since \\(\\sumn h_n = P\\) and \\(0 \\le h_n \\le 1\\), there can only be \\(P\\) leverage points that are approximately equal to one.\nAlthough this intuition is for leverage scores that are exactly one, it applies to leverage scores that are large but not exactly one — outlier \\(\\xv_n\\), with large leverage scores, force the regression line to fit the point, unlike outlier \\(y_n\\) with large residuals. To see this, note that\n\\[\n\\begin{aligned}\n\\yhat_n ={}& \\left(\\proj{\\X} \\Y \\right)_{n} \\\\\n={}& \\sum_{m=1}^N \\left(\\proj{\\X}\\right)_{nm} \\y_m \\\\\n={}& \\sum_{m \\ne n} \\left(\\proj{\\X}\\right)_{nm} \\y_m + h_n \\y_n.\n\\end{aligned}\n\\]\nHow big can the off–diagonal entries \\(\\left(\\proj{\\X}\\right)_{nm}\\) be when \\(h_n\\) is large? Again using the \\(n\\)–th standard basis vector \\(\\ev_n\\), \\[\n\\begin{aligned}\n1 ={}& \\norm{\\ev_n}^2\n\\\\\\ge{}& \\norm{\\proj{\\X} \\ev_n}^2\n\\\\={}& \\ev_n^\\trans \\proj{\\X} \\proj{\\X} \\ev_n\n\\\\={}& \\sum_{m=1}^N \\left(\\proj{\\X}\\right)_{nm}^2\n\\\\={}& \\sum_{m \\ne n} \\left(\\proj{\\X}\\right)_{nm}^2 + h_n^2,\n\\end{aligned}\n\\] so that, when \\(h_n \\approx 1\\), \\[\n\\sum_{m \\ne n} \\left(\\proj{\\X}\\right)_{nm}^2 \\le 1 - h_n^2 \\approx 0,\n\\] and so \\[\n\\yhat_n \\approx h_n \\y_n.\n\\]"
  },
  {
    "objectID": "lectures/7_InfluenceAndOutliers.html#kleibers-whale-revisited",
    "href": "lectures/7_InfluenceAndOutliers.html#kleibers-whale-revisited",
    "title": "Influence and outliers",
    "section": "Kleiber’s whale revisited",
    "text": "Kleiber’s whale revisited\nRecall Kleiber’s dataset of animal sizes and metabolisms. Let’s look at the original linear regression with and without the whale:\n\n# Look at the data, find outliers\nlm_kl &lt;- lm(Metabol_kcal_per_day ~ Weight_kg + 1, kleiber_df)\nlm_drop_kl &lt;- lm(Metabol_kcal_per_day ~ Weight_kg + 1, kleiber_df %&gt;% filter(Animal != \"Whale\"))\n\n\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\nIn this case, the fact that the whale has high leverage was obvious. But we can also see this from the leverage scores:"
  },
  {
    "objectID": "lectures/7_InfluenceAndOutliers.html#bonus-content-data-dropping-generalization-to-nonlinear-estimators",
    "href": "lectures/7_InfluenceAndOutliers.html#bonus-content-data-dropping-generalization-to-nonlinear-estimators",
    "title": "Influence and outliers",
    "section": "Bonus content: Data dropping (generalization to nonlinear estimators)",
    "text": "Bonus content: Data dropping (generalization to nonlinear estimators)\nThe above results rely heavily on the special structure of linear regression: e.g. the fact that \\(\\betahat\\) is a linear combination of \\(\\Y\\), and that \\(\\Yhat\\) is a projection of \\(\\Y\\). In more general settings such results are not available. In order to motivate the use of such diagnostics in more general settings (not covered in this class), let me introduce a slightly different approach based on derivatives.\nSuppose we assign each datapoint a weight, \\(\\w_n\\), and write\n\\[\n\\betavhat(\\w) = \\argmin{\\beta} \\sumn \\w_n (\\y_n - \\xv_n^\\trans \\betav)^2.\n\\]\nI have written \\(\\betahat(\\wv)\\) because the optimal solution depends on the vector of weights, \\(\\wv = (\\w_1, \\ldots, \\w_N)^\\trans\\). When \\(\\wv = \\onev\\), we recover the original problem. When we set one of the entries to zero, we remove that datapoint from the problem. Using this, we can approximate the effect of removing a datapoint using the first-order Taylor series expansion:\n\\[\n\\betahat_{-n} \\approx\n\\betahat_{-n}^{linear} = \\betahat + \\frac{\\partial \\betahat(\\wv)}{\\partial \\w_n}\\vert_{\\w_n=1} (0 - 1).\n\\]\nOne can show that\n\\[\n\\frac{\\partial \\betahat(\\wv)}{\\partial \\w_n}\\vert_{\\w_n=1} = (\\X^\\trans \\X)^{-1} \\xv_n \\reshat_n.\n\\]\nNote that the Taylor series is a good approximation to the exact formula (given in the homework) when \\(h_n \\ll 1\\), which is expected when \\(h_n\\) goes to zero at rate \\(1/N\\):\n\\[\n\\betahat_{-n}  = \\betahat - (\\X^\\trans \\X)^{-1} \\xv_n \\frac{\\reshat_n}{1 - h_n} \\approx\n\\betahat - (\\X^\\trans \\X)^{-1} \\xv_n \\reshat = \\betahat_{-n}^{linear}.\n\\]\nIn more complicated nonlinear problems, the exact formula is unavailable, but the linear approximation is typically computed.\nFrom this (or the exact formula), we can see that the effect of extreme values on \\(\\betahat\\) is actually a product of both \\(\\reshat_n\\) and \\(\\xv_n\\). Large residuals will not have an effect when \\(\\xv_n = \\zerov\\), and outlier \\(\\xv_n\\) will not have an effect when \\(\\reshat_n = 0\\)."
  },
  {
    "objectID": "lectures/7_InfluenceAndOutliers.html#bonus-content-rank-one-updates-for-linear-regression-woodbury-formula",
    "href": "lectures/7_InfluenceAndOutliers.html#bonus-content-rank-one-updates-for-linear-regression-woodbury-formula",
    "title": "Influence and outliers",
    "section": "Bonus content: Rank-one updates for linear regression (Woodbury formula)",
    "text": "Bonus content: Rank-one updates for linear regression (Woodbury formula)\nThe proof of the data–dropping formula for linear regression uses the Woodbury formula, for which I now give a proof for completness. Let \\(U\\) and \\(V\\) be \\(N \\times K\\) matrices, and \\(\\id_N\\) and \\(\\id_K\\) the \\(N\\times N\\) and \\(K \\times K\\) identity matrices, respectively. Using the fact that\n\\[\n\\begin{aligned}\n(\\id_N + U V^\\trans)^{-1} (\\id_N + U V^\\trans) = \\id_N\n\\quad\\Rightarrow\\quad&\n(\\id_N + U V^\\trans)^{-1} = \\id_N - (\\id_N + U V^\\trans)^{-1} U V^\\trans & \\textrm{(i)}\\\\\n(\\id_N + U V^\\trans) U = U (\\id_K +  V^\\trans U)  \n\\quad\\Rightarrow\\quad&\nU (\\id_N + V^\\trans U )^{-1} = (\\id_K + U^\\trans V)^{-1} U  & \\textrm{(ii)}\n\\end{aligned}\n\\]\nwe have that\n\\[\n\\begin{aligned}\n(\\id_N + U V^\\trans)^{-1} ={}& \\id_N - (\\id_N + U V^\\trans)^{-1} U V^\\trans  & \\textrm{by (i)} \\\\\n={}& \\id_N - U  (\\id_K + V^\\trans U )^{-1} V^\\trans  & \\textrm{by (ii)}.\n\\end{aligned}\n\\]\nWe can use this to prove the Woodbury formula as a special case when \\(K=1\\).\n\\[\n\\begin{aligned}\n(A + \\uv \\vv^\\trans)^{-1} ={}&\n(A (\\id_N  + A^{-1} \\uv \\vv^\\trans))^{-1}\n\\\\={}&\n(\\id_N + (A^{-1} \\uv) \\vv^\\trans )^{-1} A^{-1}\n\\\\={}&\n(\\id_N - A^{-1} \\uv (1 + \\vv^\\trans A^{-1} \\uv)^{-1} \\vv^\\trans ) A^{-1}\n\\\\={}&\nA^{-1} - \\frac{A^{-1} \\uv \\vv A^{-1}}{1 + \\vv^\\trans A^{-1} \\uv}.\n\\end{aligned}\n\\]\nThis formula has importance for linear models far beyond the leave–one–out formula. For instance, the “kernel trick” in machine learning can be understood as an application of the Woodbury formula."
  },
  {
    "objectID": "lectures/9_RegressionToTheMean.html",
    "href": "lectures/9_RegressionToTheMean.html",
    "title": "Regression to the mean",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\n\\newcommand{\\f}{f}\n\\newcommand{\\fhat}{\\hat{f}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{#1}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\\def\\Q{\\mybold{Q}}\n\n\n\\def\\eps{\\varepsilon}\n\n$$\n\n\n\n\n\nGoals\n\nGain intuition for the phenomenon of regression to the mean\n\nEveryday intuition\nThe asymmetry of OLS\nThe effect on regression of noise in the regressors\n\n\n\n\nRegression to the mean\nFollow FPP on heights\n\n\nDunning Kruger effect\n\n\nFormal derivation\nSuppose that \\(\\z_n = \\x_n + \\etav\\) and \\(\\y_n = \\betav^\\trans \\xv_n + \\res_n\\), but we regress \\(\\y_n \\sim \\z_n\\).\n\n\nThe asymmetry of regression\nRegress on data uniformly distributed in the unit ball."
  },
  {
    "objectID": "lectures/TransformationLab.html",
    "href": "lectures/TransformationLab.html",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "",
    "text": "\\(\\LaTeX\\)"
  },
  {
    "objectID": "lectures/TransformationLab.html#linear-combinations-of-variables-in-simple-least-squares",
    "href": "lectures/TransformationLab.html#linear-combinations-of-variables-in-simple-least-squares",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "Linear combinations of variables in simple least squares",
    "text": "Linear combinations of variables in simple least squares\nTo answer question 1, let’s look at a slightly simpler example, where we perform simple least squares on heigh.\n\nreg_height &lt;- lm(bodyfat ~ Height, bodyfat_df)\nprint(reg_height$coefficients)\n\n(Intercept)      Height \n 33.4944938  -0.2044753 \n\nprint(sprintf(\"Error: %f\", mean(reg_height$residuals^2)))\n\n[1] \"Error: 69.199176\"\n\nreg_height_norm &lt;- lm(bodyfat ~ height_norm, bodyfat_df)\nprint(reg_height_norm$coefficients)\n\n(Intercept) height_norm \n 19.1507937  -0.7489636 \n\nprint(sprintf(\"Error: %f\", mean(reg_height_norm$residuals^2)))\n\n[1] \"Error: 69.199176\"\n\n\nWhy do these two have different coefficients, but the same fit? The answer comes from our projection result. Let’s let \\(\\x_n = \\textrm{Height}_n\\), and \\(\\z_n = \\textrm{height\\_norm}_n\\). Recall that R includes a constant in the regression by default, so our two regressions are:\n\\[\n\\begin{aligned}\n\\textrm{height: } && \\textrm{height\\_norm} \\\\\n\\y_n = \\beta_0 + \\beta_1 \\x_n + \\res_n &&\n\\y_n = \\gamma_0 + \\gamma_1 \\z_n + \\nu_n.\n\\end{aligned}\n\\]\nWhat’s the relationship between these two regressions? Well, taking \\(\\overline{x} := \\meann \\x_n\\) and \\(\\sigmahat_x := \\sqrt{\\meann (\\x_n - \\overline{x})^2}\\). In our case, \\(\\sigmahat_x &gt; 0\\), so we’ve set\n\\[\n\\z_n = \\frac{\\x_n - \\overline{x}}{\\sigmahat_x} =\n  \\frac{1}{\\sigmahat_x} \\x_n - \\frac{\\overline{x}}{\\sigmahat_x}.\n\\tag{1}\\]\nThat means we can write\n\\[\n\\begin{aligned}\n\\gamma_0 + \\gamma_1 \\z_n + \\nu_n ={}&\n\\gamma_0 + \\gamma_1 \\left( \\frac{1}{\\sigmahat_x} \\x_n -\n  \\frac{\\overline{x}}{\\sigmahat_x} \\right) + \\nu_n  \\\\\n={}&\n\\left(\\gamma_0 - \\frac{\\overline{x}}{\\sigmahat_x} \\gamma_1 \\right)  +\n   \\left( \\frac{\\gamma_1}{\\sigmahat_x}\\right) \\x_n  + \\nu_n.\n\\end{aligned}\n\\]\nBy identifying\n\\[\n\\gamma_0 - \\frac{\\overline{x}}{\\sigmahat_x} \\gamma_1 = \\beta_0 \\quad\\textrm{and}\\quad\n\\frac{\\gamma_1}{\\sigmahat_x} = \\beta_1,\n\\]\nwe see that the two regressions are exactly the same! From this two conclusions follow:\n\nIt is impossible for you to get a better fit with one than the other.\nIn general the coefficients will be different.\n\nIn fact, we can write down the rule to convert between the two. Let’s check that it works:\n\nxbar &lt;- mean(bodyfat_df$Height)\nsigmahat &lt;- sd(bodyfat_df$Height)\ngammahat_0 &lt;- reg_height_norm$coefficients[\"(Intercept)\"]\ngammahat_1 &lt;- reg_height_norm$coefficients[\"height_norm\"]\nbetahat_0 &lt;- reg_height$coefficients[\"(Intercept)\"]\nbetahat_1 &lt;- reg_height$coefficients[\"Height\"]\n\nprint(\"Intercept: \")\n\n[1] \"Intercept: \"\n\ncat(betahat_0, \" = \", gammahat_0 - gammahat_1 * xbar / sigmahat, \"\\n\")\n\n33.49449  =  33.49449 \n\ncat(betahat_1, \" = \", gammahat_1  / sigmahat, \"\\n\")\n\n-0.2044753  =  -0.2044753"
  },
  {
    "objectID": "lectures/TransformationLab.html#linear-combinations-of-variables-in-matrix-form",
    "href": "lectures/TransformationLab.html#linear-combinations-of-variables-in-matrix-form",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "Linear combinations of variables in matrix form",
    "text": "Linear combinations of variables in matrix form\nTo extend what we just did, let’s put our result in matrix notation. First, write\n\\[\n\\xv_n = \\begin{pmatrix}\n1 \\\\ \\x_n\n\\end{pmatrix}\n\\quad\\textrm{and}\\quad\n\\zv_n = \\begin{pmatrix}\n1 \\\\ \\z_n\n\\end{pmatrix}.\n\\]\nIn this notation, we can write Equation 1 as\n\\[\n\\zv_n =\n\\begin{pmatrix}\n1 & 0 \\\\\n-\\frac{\\overline{x}}{\\sigmahat_x} & \\frac{1}{\\sigmahat_x}\n\\end{pmatrix}\n\\xv_n\n=: \\A \\xv_n.\n\\]\nWe can see that \\(\\A\\) is invertible whenever \\(\\sigmahat_x &gt; 0\\). Using the matrix \\(\\A\\), we can write \\(\\Z = \\X \\A^\\trans\\), so\n\\[\n\\Y = \\Z \\gamma + \\etav =\n\\X \\A^\\trans \\gamma + \\etav =\n\\X  \\beta + \\resv,\n\\]\nwhich gives\n\\[\n\\etav = \\resv\n\\quad\\Leftrightarrow \\quad\n\\A^\\trans \\gamma =  \\beta\n\\quad\\Leftrightarrow \\quad\n\\gamma = (\\A^\\trans)^{-1} \\beta\n.\n\\]\nWe can confirm that this condition does indeed hold at the optimum of each regression:\n\na_mat = matrix(NA, 2, 2)\na_mat[1,1] &lt;- 1\na_mat[1,2] &lt;- 0\na_mat[2,1] &lt;- -xbar / sigmahat\na_mat[2,2] &lt;- 1 / sigmahat\nprint(t(a_mat) %*% reg_height_norm$coefficients %&gt;% as.numeric())\n\n[1] 33.4944938 -0.2044753\n\nprint(reg_height$coefficients)\n\n(Intercept)      Height \n 33.4944938  -0.2044753 \n\n\nNote that we were able to do this only because we included an intercept in the regression! In fact, we can show that we get different fits if we don’t include the intercept:\n\nreg_height_noint &lt;- lm(bodyfat ~ Height - 1, bodyfat_df)\nprint(sprintf(\"Error: %f\", mean(reg_height_noint$residuals^2)))\n\n[1] \"Error: 72.237550\"\n\nreg_height_norm_noint &lt;- lm(bodyfat ~ height_norm - 1, bodyfat_df)\nprint(sprintf(\"Error: %f\", mean(reg_height_norm_noint$residuals^2)))\n\n[1] \"Error: 435.952073\""
  },
  {
    "objectID": "lectures/TransformationLab.html#linear-combinations-of-variables-in-general",
    "href": "lectures/TransformationLab.html#linear-combinations-of-variables-in-general",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "Linear combinations of variables in general",
    "text": "Linear combinations of variables in general\nNote that the argument which we made in the special case above actually holds in general. Let’s go to \\(P\\)–dimensional regression, but otherwise retaining the notation \\(\\y_n \\sim \\z_n^\\trans\\gamma\\) and \\(\\y_n \\sim \\x_n^\\trans \\beta\\). If we can find an invertible \\(\\A\\) such that \\(\\z_n = \\A \\x_n\\), then we still have\n\\[\n\\Y = \\Z \\gamma + \\etav =\n\\X \\A^\\trans \\gamma + \\etav =\n\\X  \\beta + \\resv,\n\\]\n\\[\n\\etav = \\resv\n\\quad\\Leftrightarrow \\quad\n\\A^\\trans \\gamma =  \\beta\n\\quad\\Leftrightarrow \\quad\n\\gamma = (\\A^\\trans)^{-1} \\beta\n.\n\\]\nHow can we recognize a linear transformation from \\(\\x_n\\) to \\(\\z_n\\)?\nIt’s enough for each entry of \\(\\z_n\\) to be a linear transform \\(\\z_{np}= \\a_p^\\trans \\x_n\\), since we can then just stack the \\(\\a_p^\\trans\\) vectors to form \\(\\A\\).\n\nLinear transformations of \\(\\x_n\\) always look like \\(\\z_{np} = \\a_{p1} \\x_{n1} + \\ldots \\a_{pP} \\x_{nP}\\).\nEntries that are unchanged are always the (trivial) identity linear transformation \\(\\a_{p} = (0, 0, \\ldots, 0, 1,  0, \\ldots, 0)\\)\n\nNote that adding a constant is an affine, not a linear transformation. However, it can be a linear transformation if your model includes an intercept — or if some sum of the entries of \\(\\x_n\\) is always a constant.\nNot that it is not always easy to see whether a linear transformation is invertible! We will talk about that problem shortly.\nIn this way, one can see that the normalization of Question 1 is a linear transformation."
  },
  {
    "objectID": "lectures/TransformationLab.html#linear-transformations-and-column-spans",
    "href": "lectures/TransformationLab.html#linear-transformations-and-column-spans",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "Linear transformations and column spans",
    "text": "Linear transformations and column spans\nFinally, recall our projection result that states\n\\[\n\\Yhat = \\proj{\\S_\\X} \\Y = \\X (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y,\n\\]\nwhere \\(\\S_\\X\\) is the space of linear combinations of the columns of \\(\\X\\). If \\(\\A\\) is invertible, then the column span of \\(\\Z = \\X \\A^\\trans\\) is equal to the column span of \\(\\X\\). Consequently, a regression on an invertible linear combination of regressors cannot affect the fit.\nAlthough this intuition is clear enough if you’re comfortable with linear algebra, you can also check directly that\n\\[\n\\proj{\\S_\\Z} \\Y =\n\\Z (\\Z^\\trans \\Z)^{-1} \\Z^\\trans \\Y =\n\\X \\A^\\trans (\\A \\X^\\trans \\X \\A^\\trans)^{-1} \\A \\X^\\trans \\Y =\n\\X \\A^\\trans (\\A^\\trans)^{-1} (\\X^\\trans \\X)^{-1} \\A^{-1} \\A \\X^\\trans \\Y =\n\\X (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y =\n\\proj{\\S_\\X} \\Y.\n\\]"
  },
  {
    "objectID": "lectures/TransformationLab.html#redundant-variables-in-simple-regression",
    "href": "lectures/TransformationLab.html#redundant-variables-in-simple-regression",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "Redundant variables in simple regression",
    "text": "Redundant variables in simple regression\nOnce again, it will be useful to start with a simpler example. Suppose you have a regression \\(\\y_n \\sim \\xv_n^\\trans \\beta\\), with \\(\\xv_n = (1, \\x_n)^\\trans\\), where \\(\\sigmahat_x &gt; 0\\) (as defined above). Now suppose you want to run a new regression \\(\\y_n \\sim \\z_n^\\trans \\gamma\\) with\n\\[\n\\zv_n =\n\\begin{pmatrix}\n1 \\\\\n\\x_n \\\\\n\\x_n - 1\n\\end{pmatrix}.\n\\]\nNote that the new regression is the same as\n\\[\n\\begin{aligned}\n\\y_n ={}& \\gamma_1 + \\gamma_2 \\x_n + \\gamma_3 (\\x_n - 1) + \\eta_n \\\\\n={}& (\\gamma_1 - \\gamma_3) + (\\gamma_2 + \\gamma_3) \\x_n +  \\eta_n \\\\\n={}& \\beta_1 + \\beta_2 \\x_n +  \\res_n.\n\\end{aligned}\n\\]\nWe achieve the same fit — \\(\\res_n = \\eta_n\\) for all \\(n\\) — whenever\n\\[\n\\gamma_1 - \\gamma_3 = \\beta_1\n\\quad\\textrm{and}\\quad\n\\gamma_2 + \\gamma_3 = \\beta_2.\n\\]\nBy the arguments above, the two regressions must have the same optimal fit. But in this case, there are actually a whole infinite dimensional set of \\(\\gammav\\) that correspond to each \\(\\bv\\). In particular, for any \\(a\\), we can take\n\\[\n\\begin{aligned}\n\\gamma_1 &\\rightarrow \\gamma_1 + a \\\\\n\\gamma_2 &\\rightarrow \\gamma_2 - a \\\\\n\\gamma_3 &\\rightarrow \\gamma_3 + a,\n\\end{aligned}\n\\]\nand achieve exactly the same fit. In other words, the optimal fit \\(\\Yhat\\) is the same for the two regressions, but the optimal parameter\n\\[\n\\gammahat := \\argmin{\\gamma} \\meann \\eta_n^2\n\\]\nis not uniquely defined — there is a whole family of coefficients that acheive the same optimal fit."
  },
  {
    "objectID": "lectures/TransformationLab.html#redundant-variables-in-matrix-form",
    "href": "lectures/TransformationLab.html#redundant-variables-in-matrix-form",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "Redundant variables in matrix form",
    "text": "Redundant variables in matrix form\nHow can we understand this in matrix form? In this case, we can write the third element of the \\(\\zv_n\\) regressor as a linear combination of the other two:\n\\[\n\\zv_{n3} = \\x_n - 1 = \\zv_{n2} - \\zv_{n1}\n\\quad\\Leftrightarrow\\quad\n\\zv_{n3} - \\zv_{n2} + \\zv_{n1}  = 0\n\\quad\\Leftrightarrow\\quad\n\\zv_n^\\trans\n\\begin{pmatrix}\n1 \\\\\n-1 \\\\\n1\n\\end{pmatrix} =: \\zv_n^\\trans \\vv = 0.\n\\]\nSince this is true for every row, we have that \\(\\Z \\vv = \\zerov\\) as well, and so\n\\[\n\\vv^\\trans (\\Z^\\trans \\Z) \\vv = 0.\n\\]\nIn other words, \\(\\vv\\) is a non-zero vector in the nullspace of \\(\\Z^\\trans \\Z\\). It follows that \\(\\Z^\\trans \\Z\\) is not invertible, and the OLS coefficient \\((\\Z^\\trans\\Z)^{-1} \\Z^\\trans \\Y\\) is not defined!\nBut that does not prevent us from finding the optimal projection. That is, we can still find\n\\[\n\\textrm{Well-defined: }\\quad\n\\min_{\\gamma} \\norm{\\Y - \\Z \\gamma}_2^2\n\\quad\\quad\\quad\n\\quad\\quad\\quad\n\\textrm{Ill-defined: }\\quad\n\\argmin{\\gamma} \\norm{\\Y - \\Z \\gamma}_2^2.\n\\]\nSpecifically,\n\\[\n\\min_{\\gamma} \\norm{\\Y - \\Z \\gamma}_2^2 = \\norm{(\\id - \\proj{\\S_\\Z}) \\Y}_2^2\n\\]\nis the norm of the projection perpendicular to the space spanned by the columns of \\(\\Z\\), it’s just that this space is two-dimensional, not three-dimensional."
  },
  {
    "objectID": "lectures/TransformationLab.html#in-r",
    "href": "lectures/TransformationLab.html#in-r",
    "title": "Transformations of regressors: Some payoffs from the linear algebra perspective.",
    "section": "In R",
    "text": "In R\nIt turns out that R does not actually estimate the OLS fit by forming \\((\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\\). It uses an iterative procedure that is roughly similar to Gaussian elimination to estimate one component at a time. When it gets to a component that cannot be estimated beacuse \\(\\X^\\trans \\X\\) has a non-trivial nullspace, then the fit terminates, and it reports the values for the coefficients estimated up to that point, with NA for the rest.\nIn our example, the difference hw_diff is a linear combination of height_norm and weight_norm. Thus the regressors have a non-trivial nullspace, and the best fit \\(\\Yhat\\) is defined even though the regressors are not."
  },
  {
    "objectID": "lectures/linalg_review.html",
    "href": "lectures/linalg_review.html",
    "title": "Linear algebra review materials",
    "section": "",
    "text": "This course is about linear models. Furthermore it is, at least in part, a math course. So a strong foundation in linear algebra will be necessary. We will do our best to provide support to either learn or review linear algebra, but there is no substitute for working through the material yourself.\nBelow are some materials that I can recommend for self-study and / or review based on the MIT open courseware linear algebra series. The corresponding textbook, Strang’s Introduction to Linear Algebra, is also good, but is unfortunately not available for free online, and there are only a few hard copies available in the library.\nNot all the material in the MIT linear algebra is necessary for 151A. Based mostly on the titles, here are the sections of the reading that I recommend for 151A:\n\nAbsolutely necessary: Sections 3, 6, 10, 14\nImportant: Sections 7, 15, 16, 10 (video 9), 11 (video 9)\nVery useful: Sections 17, 21, 27 (video 25), 28 (video 25), 33 (video 30)\n\nNote that the numbers in the recommended reading don’t seem to line up with the videos, particularly later in the course. Where it seems the two diverge, I’ve tried to guess which video goes with the corresponding reading.\nAs is always the case with math, reading and listening is no substitute for doing.\nActually working through practice problems is the key to making linear algebra second nature!\nSo if you have a choice between, say, practicing doing some matrix multiplication yourself and watching another video on the singular value decomposition, I’d recommend the former until you’re very comfortable with the basics."
  },
  {
    "objectID": "quizzes/e1af10a2641b252b292978e406c39c2acfdeb232e4ab8ac82544b0019f28c6d9.html",
    "href": "quizzes/e1af10a2641b252b292978e406c39c2acfdeb232e4ab8ac82544b0019f28c6d9.html",
    "title": "STAT151A Quiz 2",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\n\\newcommand{\\f}{f}\n\\newcommand{\\fhat}{\\hat{f}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{#1}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\\def\\Q{\\mybold{Q}}\n\n\n\\def\\eps{\\varepsilon}\n\n$$\n\n\n\n\nPlease write your full name and email address here:\n\\[\\\\[2in]\\]\nAlso, please put your intials on each page in case the pages get separated.\n\\[\\\\[1in]\\]\nYou have 30 minutes for this quiz.\nThere are three questions, (1), (2), and (3), each weighted equally..\nThere are extra pages at the end if you need more space for solutions.\n\n\nQuestion 1\nConsider the regression \\(\\y_n \\sim \\beta_1 + \\beta_2 \\z_n + \\beta_3 r_n\\), where \\(n=1,\\ldots,N\\). Let \\(\\betav = (\\beta_1, \\beta_2, \\beta_3)^\\trans\\).\n(a) Write the regression in the form \\(\\Y \\sim \\X \\betav\\). Be precise about the dimensions and entries of the matrix \\(\\X\\).\n(b) In terms of \\(N\\) and the quantites defined in Equation 1, write expressions for \\(\\X^\\trans \\X\\) and \\(\\X^\\trans \\Y\\). \\[\n\\begin{aligned}\n\\ybar =& \\meann \\y_n & \\overline{yz} =& \\meann \\y_n \\z_n & \\overline{yr} :=& \\meann \\y_n r_n \\\\\n\\zbar =& \\meann \\z_n & \\overline{zr} =& \\meann \\z_n r_n & \\overline{r} :=& \\meann r_n \\\\\n\\overline{zz} =& \\meann \\z_n^2 & \\overline{rr} =& \\meann r_n^2 \\\\\n\\end{aligned}\n\\tag{1}\\]\n(c) Suppose now that:\n\n\\(\\z_n\\) and \\(r_n\\) are both random and IID. So \\(\\z_n\\) is independent of \\(r_n\\), and both \\(z_n\\) and \\(r_n\\) are independent of \\(\\z_m\\) and \\(r_m\\) for \\(m \\ne n\\),\n\\(\\expect{\\z_n} = \\expect{r_n} = 0\\), and\n\\(\\var{\\z_n} = \\sigma_z^2\\), and \\(\\var{r_n} = \\sigma_r^2\\).\n\nWhat is \\(\\lim_{N \\rightarrow \\infty}\\frac{1}{N} \\X^\\trans \\X\\)?\n\n\n\nQuestion 2\nConsider two categorical variables, \\(\\z_{n1}\\) and \\(\\z_{n2}\\), where \\(\\z_{n1}\\) is either “good” or “bad”, and \\(\\z_{n2}\\) is either “red” or “yellow”. Note, for example, that an observation can by “good” and “red” at the same time.\nHowever, an observation cannot be “good” and “bad” at the same time, nor can it be “red” and “yellow” at the same time.\nDefine the one-hot encodings \\[\n\\begin{aligned}\n\\x_{ng} ={}& 1 \\textrm{ when }\\z_{n1}\\textrm{ is ``good'' and }0\\textrm{ otherwise} \\\\\n\\x_{nb} ={}& 1 \\textrm{ when }\\z_{n1}\\textrm{ is ``bad'' and }0\\textrm{ otherwise} \\\\\n\\textrm{and}\\\\\n\\x_{nr} ={}& 1 \\textrm{ when }\\z_{n2}\\textrm{ is ``red'' and }0\\textrm{ otherwise} \\\\\n\\x_{ny} ={}& 1 \\textrm{ when }\\z_{n2}\\textrm{ is ``yellow'' and }0\\textrm{ otherwise}.\n\\end{aligned}\n\\]\nConsider the regression \\(\\y_n \\sim \\beta_1 \\x_{ng} + \\beta_2 \\x_{nr}\\), where \\(n=1,\\ldots,N\\). Let \\(\\betav = (\\beta_1, \\beta_2)^\\trans\\). That is, we are regressing only on the one–hot encodings for “good” and for “red”.\nLet \\(N_g\\) denote the number of rows with \\(\\z_{n1} =\\) “good”, \\(N_r\\) denote the number of rows with \\(\\z_{n2} =\\)“red”, and so on. Similarly, let \\(N_{gr}\\) denote the number of rows with both \\(\\z_{n1} =\\)“good” and \\(\\z_{n2} =\\)“red”.\n(a) Write \\(\\X^\\trans \\X\\) in terms of \\(N_g\\), \\(N_r\\), and \\(N_{gr}\\).\n(b) Write a formula in terms of \\(N_g\\), \\(N_r\\), and \\(N_{gr}\\) that tells when \\(\\X^\\trans \\X\\) is invertible.\nHint: recall that the determinant of the \\(2\\times 2\\) matrix \\(\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\) is given by \\(ad - bc\\).\n(c) Suppose that every “good” row is also “red”, and every “red” row is “good”. Is \\(\\X^\\trans \\X\\) invertible? Justify your answer.\n\n\n\nQuestion 3\nIn the setting of Question 2, consider the regression \\(\\y_n \\sim \\beta_0 + \\beta_1 \\x_{ng} + \\beta_2 \\x_{nb}\\). Note that a row is either “good” or “bad”, so that exactly one of \\(\\x_{ng}\\) or \\(\\x_{nb}\\) is equal to \\(1\\) for any particular observation \\(n\\). Let \\(\\betav = (\\beta_0, \\beta_1, \\beta_2)^\\trans\\), and \\(\\X\\) the corresponding regressor matrix.\n(a) Let \\(N_g\\) denote the number of rows with \\(\\z_{n1} =\\) “good” and \\(N_b\\) denote the number of rows with \\(\\z_{n1} =\\) “bad”. In terms of \\(N\\), \\(N_g\\), and \\(N_b\\), write an expression for \\(\\X^\\trans \\X\\).\n(b) Suppose that \\(\\ybar_g = \\frac{1}{N_g} \\sum_{\\textrm{good } n} \\y_n\\) and \\(\\ybar_b = \\frac{1}{N_b} \\sum_{\\textrm{bad } n} \\y_n\\) denote the average of \\(\\y_n\\) in “good” and “bad” rows, respectively. Find at least one \\(\\betavhat\\) that satisfies \\(\\X^\\trans \\X \\betavhat = \\X^\\trans \\Y\\).\n(c) Find another value \\(\\betahat'\\), different than the answer you gave in (b), such that \\(\\betavhat'\\) also satisfies \\(\\X^\\trans \\X \\betavhat' = \\X^\\trans \\Y\\).\n\nExtra space for answers (indicate clearly which problem you are working on)\n\nExtra space for answers (indicate clearly which problem you are working on)"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Statistics 151A: Linear Models",
    "section": "",
    "text": "Calendar (tentative)\n\n\n\n\n\n\n\n\n\n\nDate\nDay of week\nEvent\nTopic\nNotes\nSupplementary reading\n\n\n\n\nAug 29\nThursday\nLecture 1\nSample means as inference\n\n\n\n\nSep 3\nTuesday\nLecture 2\nSample means as loss minimization and projection\n\n\n\n\nSep 5\nThursday\nLecture 3\nInference and confounding with sample means\n\nROS 1.1-1.4\n\n\nSep 10\nTuesday\nLecture 4\nAmes Housing data for inference\n\nVDS 8.4\n\n\nSep 12\nThursday\nLecture 5\nOne-hot encoding and sample means\nHW 1 due Friday Sep 13\nVDS 10.2, ISL 3.3.1\n\n\nSep 17\nTuesday\nLecture 6\nMultilinear regression as loss minimization\n\nETM 1.4-1.5, LME 3.1\n\n\nSep 19\nThursday\nLecture 7\nExamples of the matrix form of linear regression\nQuiz 1 in class.\n\n\n\nSep 24\nTuesday\nLecture 8\nRedundant regressors\n\n\n\n\nSep 26\nThursday\nLecture 9\nLinear regression as projection\nHW 2 due Monday Sep 30\nETM 2.1-2.3, LME 3.1-3.3\n\n\nOct 1\nTuesday\nLecture 10\nTransformations of regressors\n\nVDS 10.3, ROS 10.1-10.4\n\n\nOct 3\nThursday\nLecture 11\nTransformations of responses\nQuiz 2 in class. Guest lecturer.\nETM 2.4, LMS 7-8\n\n\nOct 8\nTuesday\nLecture 12\nInfluence and Outliers\n\nLME 11\n\n\nOct 10\nThursday\nLecture 13\nInfluence and Outliers\nHW 3 due Friday Oct 11\nLME 12.2\n\n\nOct 15\nTuesday\nLecture 14\nThe FWL theorem\n\nLME 7\n\n\nOct 17\nThursday\nLecture 15\nStochastic assumptions on the residual\nQuiz 3 in class.\nETM 3.1-3.3\n\n\nOct 22\nTuesday\nLecture 16\nOmitted variables in inference and prediction\n\nLME 9.2\n\n\nOct 24\nThursday\nLecture 17\nRegression to the mean\nHW 4 due Friday Oct 25\nETM 8.2, FPP 10.4\n\n\nOct 29\nTuesday\nLecture 18\nConfidence intervals and hypothesis testing\nGuest lecturer.\nETM 4.1, ROS 4.5\n\n\nOct 31\nThursday\nLecture 19\nA hierarchy of assumptions\nQuiz 4 in class.\nETM 4.1, ROS 4.5\n\n\nNov 5\nTuesday\nLecture 20\nCoefficient tests under normality\n\nETM 4.4-4.5\n\n\nNov 7\nThursday\nLecture 21\nVariable selection and the F-test\nHW 5 due Friday Nov 8\nETM 4.4-4.5\n\n\nNov 12\nTuesday\nLecture 22\nTesting under machine learning assumptions\n\nLME 12, LME 6, ETM 5.5\n\n\nNov 14\nThursday\nLecture 23\nBias-variance tradeoff in prediction\nQuiz 5 in class.\nISL 2.2\n\n\nNov 19\nTuesday\nLecture 24\nRidge or L2 regression\n\nISL 6.2\n\n\nNov 21\nThursday\nLecture 25\nLASSO or L1 regression\n\nISL 6.2\n\n\nNov 26\nTuesday\nLecture 26\nThe difficulty of variable selection for inference\n\n\n\n\nNov 27\nThanksgiving\n(no class)\n\n\n\n\n\nNov 28\nThanksgiving\n(no class)\n\n\n\n\n\nNov 29\nThanksgiving\n(no class)\n\n\n\n\n\nDec 3\nTuesday\nLecture 27\nProject consultation\n\n\n\n\nDec 5\nThursday\nLecture 28\nTBD"
  },
  {
    "objectID": "lectures/LLNSimulationLab.html",
    "href": "lectures/LLNSimulationLab.html",
    "title": "Investigating the law of large numbers by simulation",
    "section": "",
    "text": "In this exercise / lecture, we’ll be simulating data to observe the law of large numbers (LLN) in action. We’ll proceed by:\nAlong the way, we’ll be particularly interested in ways the LLN — and results which rely on it — break down.\nlibrary(tidyverse)\nlibrary(mvtnorm)\n\ntheme_update(text = element_text(size=24))\noptions(repr.plot.width=12, repr.plot.height=6)\n\n── Attaching core tidyverse packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "lectures/LLNSimulationLab.html#some-vector-lln-questions",
    "href": "lectures/LLNSimulationLab.html#some-vector-lln-questions",
    "title": "Investigating the law of large numbers by simulation",
    "section": "Some vector LLN questions:",
    "text": "Some vector LLN questions:\n\nHow does the covariance matrix affect convergence? Try:\n\nHighly correlated observations\nA nearly singular matrix\n\nHow do transformations affect convergence? Some to try: inverse, eigenvalues, sum of squares of entries, eigenvectors.\nCan you find a covariance matrix for which the LLN does very badly with the inverse?"
  }
]