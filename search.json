[
  {
    "objectID": "quizzes/quizzes.html",
    "href": "quizzes/quizzes.html",
    "title": "Quizzes",
    "section": "",
    "text": "Forthcoming.",
    "crumbs": [
      "Quizzes"
    ]
  },
  {
    "objectID": "quizzes/quiz4_retake_solutions.html",
    "href": "quizzes/quiz4_retake_solutions.html",
    "title": "STAT151A Quiz 4 solutions",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[1in]\\]\nThis question will take the residuals of the training data to be random, and will consider variablity under sampling of the training data. The regressors for both the training data and test data will be taken as fixed.\nThe inverse of a 2x2 matrix is given by \\[\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}^{-1} =\n\\frac{1}{ad - bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}.\n\\]\nThe OLS estimator is given by \\(\\betahat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\\).\nYou have 20 minutes for this quiz.\nThere are three parts, (a), (b), and (c), each weighted equally..\n\n\n(a)\nFor this quiz, we will assume that \\(\\y_n = \\beta^\\trans \\xv_n + \\res_n\\) for some \\(\\beta\\), and that the residuals \\(\\res_n\\) are IID with \\(\\expect{\\res_n} = 0\\) and \\(\\expect{\\res_n^2} = 1\\), but not necessarily normal.\nLet \\(\\xv_n  = (1, \\z_{n})^\\trans\\), where \\(\\meann \\z_n = 0\\) and \\(\\meann \\z_n^2 = \\delta &gt; 0\\). That is, assume we are regressing on a constant and a single mean-zero regressor. For this question, take the regressors to be fixed (not random).\nFind the limiting distribution of \\(\\sqrt{N}(\\betahat - \\beta)\\) as \\(N \\rightarrow \\infty\\).\n\nStudents can use the fact that\n\\[\n\\sqrt{N}(\\betahat - \\beta) \\rightarrow\n  \\gauss{\\zerov, \\sigma^2 \\left(\\lim_{N\\rightarrow\\infty} \\meann \\xv_n \\xv_n^\\trans\\right)^{-1}}\n\\]\nwithout proving it. We have that\n\\[\n\\lim_{N\\rightarrow\\infty} \\meann \\xv_n \\xv_n^\\trans =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & \\delta\n\\end{pmatrix}\n\\]\nso\n\\[\n\\sqrt{N}(\\betahat - \\beta) \\rightarrow\n  \\gauss{\\zerov, \\begin{pmatrix}\n1 & 0 \\\\\n0 & \\delta^{-1}\n\\end{pmatrix}\n}\n\\]\n\n\n\n(b)\nDefine the expected prediction error \\[\n\\begin{aligned}\n\\y_\\new :={} \\beta^\\trans \\xv_{\\new} + \\res_\\new\n\\quad\\textrm{and}\\quad\n\\yhat_\\new :={}  \\betahat^\\trans \\xv_\\new.\n\\end{aligned}\n\\]\nUnder the conditions given in part (a), find the limiting distribution of\n\\[\n\\sqrt{N} (\\yhat_\\new - \\expect{\\y_\\new} )\n\\]\nas \\(N \\rightarrow \\infty\\), as a function of \\(\\xv_\\new\\). That is, the limiting distribution will depend on \\(\\xv_\\new\\), so please make the dependence explicit.\nYou may use your answer from part (a).\n\nSince \\(\\yhat_\\new  = \\betavhat^\\trans \\xv_\\new\\) and \\(\\expect{\\y_\\new} = \\beta^\\trans \\xv_\\new\\),\n\\[\n\\sqrt{N} (\\yhat_\\new - \\expect{\\y_\\new} ) =\n\\sqrt{N} \\xv_\\new^\\trans (\\betahat - \\beta) \\rightarrow\n  \\gauss{0, \\xv_\\new^\\trans \\begin{pmatrix}\n1 & 0 \\\\\n0 & \\delta^{-1}\n\\end{pmatrix} \\xv_\\new\n} =\n  \\gauss{0, 1 + \\delta^{-1} \\z_\\new^2\n}\n\\]\n\n\n\n(c)\nAssume the conditions and definitions given in (a) and (b). Assume that \\(\\delta \\ll 1\\) (that is, \\(\\delta\\) is much smaller than \\(1\\).)\nFind the limiting distribution of \\(\\sqrt{N} (\\yhat_\\new - \\expect{\\y_\\new} )\\) when\n\n\\(\\xv_\\new = (1, 0)\\) and\n\\(\\xv_\\new = (1, 1)\\).\n\nWhich of the two is larger?\nYou may use your answer from parts (a) and (b).\n\nPlugging in, we see that the distributions are\n\n\\(\\gauss{0, 1}\\)\n\\(\\gauss{0, 1 + \\delta^{-1}}\\).\n\nThe second has larger variance."
  },
  {
    "objectID": "quizzes/f4e48853e74f0a0081cdf9475a6deb7a.html",
    "href": "quizzes/f4e48853e74f0a0081cdf9475a6deb7a.html",
    "title": "STAT151A Quiz 4 (Mar 12th)",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[1in]\\]\nThis question will take the residuals of the training data to be random, and will consider variablity under sampling of the training data. The regressors for both the training data and test data will be taken as fixed.\nThe inverse of a 2x2 matrix is given by \\[\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}^{-1} =\n\\frac{1}{ad - bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}.\n\\]\nThe OLS estimator is given by \\(\\betahat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\\).\nYou have 20 minutes for this quiz.\nThere are three parts, (a), (b), and (c), each weighted equally..\n\n\n(a)\nFor this quiz, we will assume that \\(\\y_n = \\beta^\\trans \\xv_n + \\res_n\\) for some \\(\\beta\\), and that the residuals \\(\\res_n\\) are IID with \\(\\expect{\\res_n} = 0\\) and \\(\\expect{\\res_n^2} = 1\\), but not necessarily normal.\nLet \\(\\xv_n  = (1, \\z_{n})^\\trans\\), where \\(\\meann \\z_n = 0\\) and \\(\\meann \\z_n^2 = \\delta &gt; 0\\). That is, assume we are regressing on a constant and a single mean-zero regressor. For this question, take the regressors to be fixed (not random).\nFind the limiting distribution of \\(\\sqrt{N}(\\betahat - \\beta)\\) as \\(N \\rightarrow \\infty\\).\n\n\n\n(b)\nDefine the expected prediction error \\[\n\\begin{aligned}\n\\y_\\new :={} \\beta^\\trans \\xv_{\\new} + \\res_\\new\n\\quad\\textrm{and}\\quad\n\\yhat_\\new :={}  \\betahat^\\trans \\xv_\\new.\n\\end{aligned}\n\\]\nUnder the conditions given in part (a), find the limiting distribution of\n\\[\n\\sqrt{N} (\\yhat_\\new - \\expect{\\y_\\new} )\n\\]\nas \\(N \\rightarrow \\infty\\), as a function of \\(\\xv_\\new\\). That is, the limiting distribution will depend on \\(\\xv_\\new\\), so please make the dependence explicit.\nYou may use your answer from part (a).\n\n\n\n(c)\nAssume the conditions and definitions given in (a) and (b). Assume that \\(\\delta \\ll 1\\) (that is, \\(\\delta\\) is much smaller than \\(1\\).)\nFind the limiting distribution of \\(\\sqrt{N} (\\yhat_\\new - \\expect{\\y_\\new} )\\) when\n\n\\(\\xv_\\new = (1, 0)\\) and\n\\(\\xv_\\new = (1, 1)\\).\n\nWhich of the two is larger?\nYou may use your answer from parts (a) and (b)."
  },
  {
    "objectID": "quizzes/f5dddb3a5731f04712b8ebdca7fc4c00.html",
    "href": "quizzes/f5dddb3a5731f04712b8ebdca7fc4c00.html",
    "title": "STAT151A Quiz 2 (Feb 13th)",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[1in]\\]\nFor this quiz, we’ll consider the linear models\n\\[\n\\begin{aligned}\ny_n ={} \\betav^\\trans \\xv_n + \\res_n\n&\\quad\\textrm{and}\\quad\ny_n ={} \\gammav^\\trans \\zv_n + \\eta_n\n\\end{aligned}\n\\]\nwith\n\\[\n\\begin{aligned}\n\\xv_n ={} (1, \\x_n)^\\trans\n&\\quad\\textrm{and}\\quad\n\\zv_n ={} (1, \\z_n)^\\trans \\textrm{ where}\n\\\\\n\\overline{\\x} :={} \\meann \\x_n\n&\\quad\\textrm{and}\\quad\n\\z_n :={} \\x_n - \\overline{\\x}.\n\\end{aligned}\n\\]\nAssume that \\(\\x_n\\) is not a constant (i.e., for at least one pair \\(n\\) and \\(m\\), \\(\\x_n \\ne \\x_m\\).).\nLet \\(\\X\\) denote the \\(N \\times 2\\) matrix whose \\(n\\)–th row is \\(\\xv_n^\\trans\\), and \\(\\Z\\) denote the \\(N \\times 2\\) matrix whose \\(n\\)–th row is \\(\\zv_n^\\trans\\).\nRecall that the inverse of a 2x2 matrix is given by\n\\[\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}^{-1} =\n\\frac{1}{ad - bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}.\n\\]\nYou have 20 minutes for this quiz.\nThere are three parts, (a), (b), and (c), each weighted equally..\n\n\n(a)\nFind a \\(2 \\times 2\\) matrix \\(\\A\\) such that \\(\\Z = \\X \\A\\).\n\n\n\n(b)\nSuppose I tell you that the OLS estimate of \\(\\beta\\) is given by \\(\\betahat = (2, 3)\\), and that \\(\\overline{x} = 4\\). What is the value of \\(\\gammahat\\), the OLS estimate of \\(\\gamma\\)?\n\n\n\n(c)\nIn general, can you say whether one regression will provide a better fit than the other? That is, can you say which of \\(\\meann (\\y_n - \\zv_n^\\trans\\gammahat)^2\\) and \\(\\meann (\\y_n - \\xv_n^\\trans\\betahat)^2\\) is smaller? Argue why or why not."
  },
  {
    "objectID": "quizzes/c45bd141bc098d09ca957d5dd3885992.html",
    "href": "quizzes/c45bd141bc098d09ca957d5dd3885992.html",
    "title": "STAT151A Quiz 1 (Jan 30th)",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[2in]\\]\nFor this quiz, we’ll consider the linear model \\(y_n = \\beta_1 \\z_n + \\beta_2 \\w_n + \\res_n\\).\nNote that there is no intercept, and instead are two scalar regressors, \\(\\z_n\\) and \\(\\w_n\\).\nRecall that the inverse of a 2x2 matrix is given by\n\\[\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}^{-1} =\n\\frac{1}{ad - bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}.\n\\]\nYou have 20 minutes for this quiz.\nThere are three parts, (a), (b), and (c), each weighted equally..\n\n\n(a)\nWrite the set of equations\n\\[\ny_n = \\beta_1 \\z_n + \\beta_2 \\w_n + \\res_n\n\\]\nfor \\(n \\in \\{1, \\ldots, N\\}\\) in matrix form. That is, let \\(\\X\\) denote an \\(N \\times\n2\\) matrix, \\(\\Y\\) and \\(\\resv\\) length–\\(N\\) column vectors, and \\(\\betav= (\\beta_0, \\beta_1)^\\trans\\) a length–\\(2\\) column vector. Then express the matrices \\(\\Y\\), \\(\\X\\), and \\(\\resv\\) in terms of the scalars \\(\\y_n\\), \\(\\z_m\\), \\(\\w_n\\), and \\(\\res_n\\) so that \\(\\Y= \\X\\betav+ \\resv\\) is equivalent to the set of regression equations.\n\n\n\n(b)\nDefine the following quantities: \\[\n\\begin{aligned}\n    \\overline{z} :={}& \\meann \\z_n &\n    \\overline{w} :={}& \\meann \\w_n &\n    \\overline{y} :={}& \\meann \\y_n &\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n    \\overline{ww} :={}& \\meann \\w_n^2 &\n    \\overline{zw} :={}& \\meann \\z_n \\w_n &\n    \\overline{zz} :={}& \\meann \\z_n^2 &\n    \\overline{wy} :={}& \\meann \\w_n \\y_n &\n    \\overline{zy} :={}& \\meann \\z_n \\y_n.\n\\end{aligned}\n\\]\nIn terms of these quantities and \\(N\\) alone, write expressions for \\(\\X^\\trans \\X\\), \\(\\X^\\trans \\Y\\), and \\((\\X^\\trans \\X)^{-1}\\).\n\n\n\n(c)\nNow, for only this part of the quiz, assume that \\(\\overline{wz} = 0\\). Under this assumption, write an expression for the least squares solution \\(\\betahat\\) which minimizes \\[\n\\betahat := \\argmin{\\beta} \\sumn (\\y_n - \\beta_1 \\z_n - \\beta_2 \\w_n)^2.\n\\]"
  },
  {
    "objectID": "old_assignments/old_assignments.html",
    "href": "old_assignments/old_assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Hw1 code Hw1 math"
  },
  {
    "objectID": "old_assignments/hw6_math.html",
    "href": "old_assignments/hw6_math.html",
    "title": "STAT151A Homework 6: Due April 19th",
    "section": "",
    "text": "\\(\\,\\)\n\n1 Fit and regressors\nGiven a regression on \\(\\boldsymbol{X}\\) with \\(P\\) regressors, and the corresponding \\(\\boldsymbol{Y}\\), \\(\\hat{\\boldsymbol{Y}}\\), and \\(\\hat{\\varepsilon}\\), define the following quantities: \\[\n\\begin{aligned}\nRSS :={}& \\hat{\\varepsilon}^\\intercal\\hat{\\varepsilon}& \\textrm{(Residual sum of squares)}\\\\\nTSS :={}& \\boldsymbol{Y}^\\intercal\\boldsymbol{Y}& \\textrm{(Total sum of squares)}\\\\\nESS :={}& \\hat{\\boldsymbol{Y}}^\\intercal\\hat{\\boldsymbol{Y}}& \\textrm{(Explained sum of squares)}\\\\\nR^2 :={}& \\frac{ESS}{TSS}.\n\\end{aligned}\n\\]\n\na\n\nProve that \\(RSS + ESS = TSS\\).\nExpress \\(R^2\\) in terms of \\(TSS\\) and \\(RSS\\).\nWhat is \\(R^2\\) when we include no regressors? (\\(P = 0\\))\nWhat is \\(R^2\\) when we include \\(N\\) linearly independent regressors? (\\(P=N\\))\nCan \\(R^2\\) ever decrease when we add a regressor? If so, how?\nCan \\(R^2\\) ever stay the same when we add a regressor? If so, how?\nCan \\(R^2\\) ever increase when we add a regressor? If so, how?\nDoes a high \\(R^2\\) mean the regression is correctly specified? Why or why not?\nDoes a low \\(R^2\\) mean the regression is incorrectly specified? Why or why not?\n\n\n\nb\nThe next questions will be about the F-test statistic for the null \\(H_0: \\boldsymbol{\\beta}= \\boldsymbol{0}\\),\n\\[\n\\phi = \\hat{\\beta}^\\intercal(\\boldsymbol{X}^\\intercal\\boldsymbol{X}) \\hat{\\beta}/ (P \\hat{\\sigma}^2)\n\\]\n\nWrite the F-test statistic \\(\\phi\\) in terms of \\(TSS\\) and \\(RSS\\), and \\(P\\).\nCan \\(\\phi\\) ever decrease when we add a regressor? If so, how?\nCan \\(\\phi\\) ever stay the same when we add a regressor? If so, how?\nCan \\(\\phi\\) ever increase when we add a regressor? If so, how?\n\n\n\n\n2 Omitted variable bias\nFor this problem, let \\((\\boldsymbol{x}_n, \\boldsymbol{z}_n, y_n)\\) be IID random variables, where \\(\\boldsymbol{x}_n \\in \\mathbb{R}^{P_X}\\) and \\(\\boldsymbol{z}_n \\in \\mathbb{R}^{P_Z}\\). Suppose that \\(\\boldsymbol{x}_n\\) and \\(\\boldsymbol{z}_n\\) satisfy \\(\\mathbb{E}\\left[\\boldsymbol{x}_n \\boldsymbol{z}_n^\\intercal\\right] = \\boldsymbol{0}\\).\nLet \\(y_n = \\boldsymbol{x}_n^\\intercal\\beta + \\boldsymbol{z}_n^\\intercal\\gamma + \\varepsilon_n\\), where \\(\\varepsilon_n\\) is mean zero, unit variance, and indepdendent of \\(\\boldsymbol{x}_n\\) and \\(\\boldsymbol{z}_n\\).\n\na\nTake \\(P_X = P_Z = 1\\) (i.e. scalar regressors). Show that there exists \\(x_n\\) and \\(z_n\\) such that \\(\\mathbb{E}\\left[x_n z_n\\right] = 0\\) but \\(\\mathbb{E}\\left[z_n \\vert x_n\\right] \\ne 0\\) for some \\(x_n\\). (A single counterexample will be enough.)\n\n\nb\nNow return to the general case. Let \\(\\hat{\\beta}= (\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\) denote the OLS estimator from the regression on \\(\\boldsymbol{X}\\) alone.\nFor simplicity, assume that \\(\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{z}_n^\\intercal= \\boldsymbol{0}\\). (Note that, by the LLN, \\(\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{z}_n^\\intercal\\rightarrow \\boldsymbol{0}\\) as \\(N \\rightarrow \\infty\\), so this is a reasonable approximate assumption.)\nDerive an expression for \\(\\mathbb{E}\\left[\\hat{\\beta}\\right]\\), where the expectation is taken over \\(\\boldsymbol{X}\\), \\(\\boldsymbol{Y}\\), and \\(\\boldsymbol{Z}\\).\n\n\nc\nUsing (b), derive an expression for the bias for a fixed \\(\\boldsymbol{x}_\\mathrm{new}\\), i.e.\n\\[\n\\mathbb{E}\\left[y_\\mathrm{new}- \\boldsymbol{x}_\\mathrm{new}^\\intercal\\hat{\\beta}| \\boldsymbol{x}_\\mathrm{new}\\right],\n\\]\nin terms of \\(\\beta\\), \\(\\gamma\\), and the conditional expectation \\(\\mathbb{E}\\left[\\boldsymbol{z}_\\mathrm{new}\\vert \\boldsymbol{x}_\\mathrm{new}\\right]\\).\n\n\nd\nUsing your result from (c), show that the predictions are biased at \\(\\boldsymbol{x}_\\mathrm{new}\\) when omitting the variables \\(\\boldsymbol{z}_n\\) from the regression precisely when \\(\\gamma^\\intercal\\mathbb{E}\\left[\\boldsymbol{z}_n \\vert \\boldsymbol{x}_n\\right] \\ne 0\\). Using your result from (a), show that this bias can be expected to occur in general — that is, omitting variables can often induce biased predictions at a point.\n\n\n\n3 Estimating leave-one-out CV\nThis homework problem derives a closed-form estimate of the leave-one-out cross-validation error for regression. We will use the Sherman-Woodbury formula. Let \\(A\\) denote an invertible matrix, and \\(\\boldsymbol{u}\\) and \\(\\boldsymbol{v}\\) vectors the same length as \\(A\\). Then\n\\[\n(A + \\boldsymbol{u}\\boldsymbol{v}^\\intercal)^{-1} ={}\nA^{-1} - \\frac{A^{-1} \\boldsymbol{u}\\boldsymbol{v}^\\intercal A^{-1}}{1 + \\boldsymbol{v}^\\intercal A^{-1} \\boldsymbol{u}}.\n\\]\nWe will also use the following definition of a “leverage score,” \\(h_n := \\boldsymbol{x}_n^\\intercal(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{x}_n\\). We will discuss leverage scores more in the last lecture, but for now it’s enough that you know what it is. Note that \\(h_n = (\\boldsymbol{X}(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{X}^\\intercal)_{nn}\\) is the \\(n\\)–th diagonal entry of the projection matrix \\(\\underset{\\boldsymbol{X}}{\\boldsymbol{P}}\\).\nLet \\(\\hat{\\boldsymbol{\\beta}}_{-n}\\) denote the estimate of \\(\\hat{\\beta}\\) with the datapoint \\(n\\) left out. For leave-one-out CV, we want to estimate\n\\[\nMSE_{LOO} := \\frac{1}{N} \\sum_{n=1}^N(y_n - \\boldsymbol{x}_n^\\intercal\\hat{\\boldsymbol{\\beta}}_{-n})^2.\n\\]\nNote that doing so naively requries computing \\(N\\) different regressions. We will derive a much more efficient formula.\nLet \\(\\boldsymbol{X}_{-n}\\) denote the \\(\\boldsymbol{X}\\) matrix with row \\(n\\) left out, and \\(\\boldsymbol{Y}_{-n}\\) denote the \\(\\boldsymbol{Y}\\) matrix with row \\(n\\) left out.\n\na\nProve that\n\\[\n\\hat{\\boldsymbol{\\beta}}_{-n} = (\\boldsymbol{X}_{-n}^\\intercal\\boldsymbol{X}_{-n})^{-1} \\boldsymbol{X}_{-n}^\\intercal\\boldsymbol{Y}_{-n}\n= (\\boldsymbol{X}^\\intercal\\boldsymbol{X}- \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal)^{-1} (\\boldsymbol{X}^\\intercal\\boldsymbol{Y}- \\boldsymbol{x}_n y_n)\n\\]\n\n\nb\nUsing the Sherman-Woodbury formula, derive the following expression: \\[\n(\\boldsymbol{X}^\\intercal\\boldsymbol{X}- \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal)^{-1} =\n(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} + \\frac{(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} }{1 - h_n}\n\\]\n\n\nc\nCombine (a) and (b) to derive the following explicit expression for \\(\\hat{\\boldsymbol{\\beta}}_{-n}\\):\n\\[\n\\hat{\\boldsymbol{\\beta}}_{-n} = \\hat{\\boldsymbol{\\beta}}- (\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{x}_n \\frac{1}{1 - h_n} \\hat{\\varepsilon}_n\n\\]\n\n\nd\nUsing (c), derive the following explicit expression the leave-one-out error on the \\(n\\)–th observation:\n\\[\ny_n - \\boldsymbol{x}_n^\\intercal\\hat{\\boldsymbol{\\beta}}_{-n} = \\frac{\\hat{\\varepsilon}_n}{1 - h_n}.\n\\]\n\n\ne\nUsing (d), prove that\n\\[\nMSE_{LOO} := \\frac{1}{N} \\sum_{n=1}^N\\frac{\\hat{\\varepsilon}_n^2}{(1 - h_n)^2},\n\\]\nwhere \\(\\hat{\\varepsilon}_n = y_n - \\hat{y}_n\\) is the residual from the full regression without leaving any data out. Using this formula, \\(MSE_{LOO}\\) can be computed using only the original regression and \\((\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1}\\).\n\n\nf\nProve that \\(\\sum_{n=1}^Nh_n = P\\), and \\(0 \\le h_n \\le 1\\). Hint: if \\(\\boldsymbol{v}\\) is a vector with a \\(1\\) in entry \\(n\\) and \\(0\\) otherwise, then \\(h_n = \\boldsymbol{v}^\\intercal\\underset{\\boldsymbol{X}}{\\boldsymbol{P}} \\boldsymbol{v}\\), and projection cannot increase a vector’s norm. Recall also that \\(\\mathrm{trace}\\left(\\underset{\\boldsymbol{X}}{\\boldsymbol{P}}\\right) = P\\).\n\n\ng\nUsing (e) and (f), prove that \\(MSE_{LOO} &gt; RSS = \\frac{1}{N} \\sum_{n=1}^N\\hat{\\varepsilon}_n^2\\). That is, the \\(RSS\\) under-estimates the leave-one-out cross-validation error."
  },
  {
    "objectID": "old_assignments/hw5_math.html",
    "href": "old_assignments/hw5_math.html",
    "title": "STAT151A Homework 5: Due March 22nd",
    "section": "",
    "text": "\\(\\,\\)\n\n1 Reviewing the distribution of \\(\\hat{\\beta}\\) under different assumptions\nThis homework question will reference the following assumptions.\nRegressor assumptions:\n\nR1: The \\(N \\times P\\) matrix \\(\\boldsymbol{X}\\), which has \\(\\boldsymbol{x}_n^\\intercal\\) in the \\(n\\)–th row, is full column rank\nR2: The regressors \\(\\boldsymbol{x}_n\\) are deterministic, and \\(\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\rightarrow \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\), where \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\) is positive definite\nR3: The regressors \\(\\boldsymbol{x}_n\\) are IID, with positive definite covariance \\(\\mathrm{Cov}\\left(\\boldsymbol{x}_n\\right)\\), and \\(\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\rightarrow \\mathbb{E}\\left[\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\right]\\) in probability.\n\nModel assumptions (for all \\(n\\)):\n\nM1: There exists a \\(\\boldsymbol{\\beta}\\) such that \\(y_n = \\boldsymbol{\\beta}^\\intercal\\boldsymbol{x}_n + \\varepsilon_n\\) for all \\(n\\)\nM2: The residuals \\(\\varepsilon_n\\) are IID with \\(\\varepsilon_n \\vert \\boldsymbol{x}_n \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\) for some \\(\\sigma^2\\)\nM3: The residuals \\(\\varepsilon_n\\) are independent with \\(\\mathbb{E}\\left[\\varepsilon_n | \\boldsymbol{x}_n\\right] = 0\\) and \\(\\mathbb{E}\\left[\\varepsilon_n^2 | \\boldsymbol{x}_n\\right] = \\sigma^2\\)\nM4: The residuals \\(\\varepsilon_n\\) are independent with \\(\\mathbb{E}\\left[\\varepsilon_n | \\boldsymbol{x}_n\\right] = 0\\) and \\(\\mathbb{E}\\left[\\varepsilon_n^2 | \\boldsymbol{x}_n\\right] = \\sigma^2(x_n)\\), where \\(\\sigma^2(\\boldsymbol{x}_n)\\) may depend on \\(\\boldsymbol{x}_n\\).\nM5: The pairs \\((\\boldsymbol{x}_n, y_n)\\) are IID\nM6: For all finite vectors \\(\\boldsymbol{v}\\), \\(\\frac{1}{N} \\sum_{n=1}^N\\mathbb{E}\\left[y_n - \\boldsymbol{v}^\\intercal\\boldsymbol{x}_n)^2 \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\right] \\rightarrow \\boldsymbol{V}(\\boldsymbol{v}) &lt; \\infty\\), where each entry of the limiting matrix is finite. (The limit depends on \\(\\boldsymbol{v}\\), but importantly \\(\\boldsymbol{V}(\\boldsymbol{v})\\) is finite for all finite \\(\\boldsymbol{v}\\)).\n\nFor M2, M3, and M4 with \\(\\boldsymbol{x}_n\\) is deterministic, take the conditioning to mean “for that value of \\(\\boldsymbol{x}_n\\).”\nFor this homework, you may use the LLN, the CLT, the continuous mapping theorem, and properties of the multivariate normal distribution.\nThe term “limiting distribution” means the distribution that the quantity approaches as \\(N \\rightarrow \\infty\\).\nAssume R1 for all questions.\n\nFind the distribution of \\(\\hat{\\beta}\\) under M1, M2, and R2.\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M2, and R2.\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M2, and R3.\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M3, and R2.\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M3, and R3.\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M4, M6, and R2.\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M4, M6, and R3.\nUnder M5, M6, and R3, identify a \\(\\beta^*\\) such that \\(\\sqrt{N}(\\hat{\\beta}- \\beta^*)\\) converges to a nondegenerate, finite random variable, and find the limiting distribution.\nIn any of the above settings, what is the limiting distribution of \\((\\hat{\\beta}- \\beta)\\)? (The answer is the same no matter which setting you choose.)\n\n\n\n2 Investigating the assumptions\n\nWhat goes wrong if R1 is violated?\nWhat goes wrong if \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\) is not positive definite in R2?\nWhat goes wrong if \\(\\mathbb{E}\\left[\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\right]\\) is not positive definite in R3?\nIn terms of the limiting distributions, what is the practical difference between assumptions R2 and R3?\nAssume R1, R2, M1, and M3, except take \\(\\mathbb{E}\\left[\\varepsilon_n | \\boldsymbol{x}_n\\right] = \\delta \\ne 0\\). What happens to \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) as \\(N \\rightarrow \\infty\\)?\nAssume R1, R3, M5, and M6. In general, is it true that \\(\\mathbb{E}\\left[y_n - \\beta^* \\boldsymbol{x}_n\\right] = 0\\)?\n\n\n\n3 Confidence intervals for components of \\(\\hat{\\beta}\\)\nSuppose that \\(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}\\sim \\mathcal{N}\\left(\\boldsymbol{0}, \\boldsymbol{V}\\right)\\) for some \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{V}\\). Let \\(\\Phi(z) := \\mathbb{P}\\left(\\tilde{z} \\le z\\right)\\) where \\(\\tilde{z} \\sim \\mathcal{N}\\left(0,1\\right)\\).\nFix a vector \\(\\boldsymbol{a}\\) of the same length as \\(\\boldsymbol{\\beta}\\), and \\(0 \\le \\alpha \\le 1\\).\nIn terms of \\(\\Phi\\), \\(\\boldsymbol{V}\\), and \\(\\beta\\), find a scalar \\(b\\) such that\n\\[\n\\mathbb{P}\\left(-b  \\le \\boldsymbol{a}^\\intercal(\\hat{\\beta}- \\beta) \\le b\\right) = 1 - \\alpha.\n\\]\nIn particular, what is the result when \\(\\boldsymbol{a}= (0, \\ldots, 0, 1, 0, \\ldots, 0)\\) is a vector with \\(1\\) in location \\(k\\) and \\(0\\) elsewhere?"
  },
  {
    "objectID": "old_assignments/hw4_code.html",
    "href": "old_assignments/hw4_code.html",
    "title": "STAT151A Code homework 4: Due March 8th",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nThis coding assignment will use your work from homework 3 as a starting point. For the assignment, we’ll assume that\n\n\\(\\y_n = \\betav^\\trans \\xv_n + \\res_n\\) for all \\(n\\), including new datapoints\n\\(\\res_n \\sim \\gauss{0,\\sigma^2}\\)\nThe regressors \\(\\xv_n\\) are also random with covariance matrix \\(\\Xcov\\).\n\n\n1 Variability in the training set\nFix \\(N = 500\\), \\(P = 3\\), and set \\(\\betav\\) to some values you choose. Set \\(\\Xcov\\) to have correlation \\(0.9\\) off the diagonal and \\(1.0\\) on the diagonal. Set \\(\\sigma^2 = \\beta^\\trans \\Xcov \\beta\\).\nTake \\(\\xv_\\new\\) to be a single fixed draw from the distribution of regressors, and draw a large number (&gt; 5000) of \\(\\res_{\\new,i}\\), giving a large number of draws from \\(\\y_{\\new,i} | \\xv_\\new\\). The \\(\\y_{\\new,i}\\) should be normally distributed with mean \\(\\xv_\\new^\\trans \\beta\\) and variance \\(\\sigma^2\\).\n\n(a)\nDraw a single training set \\(\\X\\), \\(\\Y\\), and use it to construct an 80% interval for \\(\\y_\\new\\). Find the proportion of \\(\\y_{\\new,i}\\) that lie in the interval.\n\n\n(b)\nRepeat (a), but with 10 different training sets. You can keep the \\(\\y_{\\new,i}\\) the same. For each different training set, plot the corresponding intervals. Are they different from one another?\nBy a lot or a little?\n\n\n(c)\nRepeat (b), but with \\(N = 20\\). You can keep the \\(\\y_{\\new,i}\\) the same. How do the results compare to (b)? Why?\n\n\n(d)\nRepeat (b), but with \\(\\sigma\\) very small: specifically, set \\(\\sigma^2 = 0.01 \\beta^\\trans \\Xcov \\beta\\). You will need to draw new \\(\\y_{\\new,i}\\). How do the results compare to (b)?\n\n\n(e)\nRepeat (b), but now take \\(\\xv_\\new\\) to be the smallest eigenvector of \\(\\Xcov\\). (You can find the smallest eigenvector of \\(\\Xcov\\) using the R function eigen.)\nYou will need to draw new \\(\\y_{\\new,i}\\). How do the results compare to (b)?"
  },
  {
    "objectID": "old_assignments/hw3_code.html",
    "href": "old_assignments/hw3_code.html",
    "title": "STAT151A Code homework 3: Due February 23rd",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nIn this homework problem, we’ll simulate some data and check our predictions for a regression problem of the form \\(\\y_n = \\xv_n^\\trans \\betav + \\res_n\\), where \\(\\res_n \\sim \\gauss{0, \\sigma^2}\\), independetly of \\(\\xv_n\\).\nFor this problem, do not use lm() or other regression functions (except possibly to sanity check that you have done things correctly). You may (and are in fact encouraged) to use your solutions to past homeworks.\n\n1 Implementation\nImplement the following functions:\n# Generate a random covariance matrix.\n#\n# Args:\n#   dim: The dimension of the covariance matrix\n#\n# Returns:\n#   A valid dim x dim covariance matrix\nDrawCovMat &lt;- function(dim) {\n    # ... your code here ...\n}\n# Generate a random matrix of regressors.\n#\n# Args:\n#   n_obs: The number of regression observations\n#   cov_mat: A dim x dim valid covariance matrix\n#\n# Returns:\n#   A n_obs x dim matrix of normally distributed random regressors\n#   where the rows have covariance cov_mat\nSimulateRegressors &lt;- function(n_obs, cov_mat) {\n    # ... your code here ...\n}\n# Generate the response for a linear model.\n#\n# Args:\n#   x_mat: A n_obs x dim matrix of regressors\n#   beta: A dim-length vector of true regression coefficients\n#   sigma: The standard deviation of the residuals\n#\n# Returns:\n#   A n_obs-vector of responses drawn from the regression\n#   model y_n ~ x_n^T \\beta + \\epsilon_n, where \\epsilon_n\n#   is distributed N(0, sigma^2),\nSimulateResponses &lt;- function(x_mat, beta, sigma) {\n    # ... your code here ...\n}\n# Estimate the regression coefficient.\n#\n# Args:\n#   x: A n_obs x dim matrix of regressors\n#   y: A n_obs-length vector of responses\n#\n# Returns:\n#   A dim-length vector estimating the coefficient \n#   for the least squares regression y ~ x.\nGetBetahat &lt;- function(x, y) {\n    # ... your code here ...\n}\n# Estimate the residual standard deviation.\n#\n# Args:\n#   x: A n_obs x dim matrix of regressors\n#   y: A n_obs-length vector of responses\n#\n# Returns:\n#   An estimate of the residual standard deviation \n#   for the least squares regression y ~ x.\nGetSigmahat &lt;- function(x, y) {\n    # ... your code here ...\n}\n\n\n2 Draw and check\nChoose a dimension. Using a large \\(N\\), check that your functions are working correctly.\n\n\n3 Draw a training set and test set\nSimulate a training set \\(\\X\\) and \\(\\Y\\) with \\(N = 1000\\) and \\(P = 3\\), and values of \\(\\beta\\) and \\(\\sigma\\) that you choose. Use the training set to form estimates \\(\\betavhat\\) and \\(\\sigmahat\\). Then, draw a new set of \\(500\\) test data points, \\(\\X_\\new\\) and \\(\\Y_\\new\\).\nUse \\(\\betavhat\\), \\(\\sigmahat\\), and \\(\\X_\\new\\) to form an approximate 80% predictive interval for each response \\(\\Y_\\new\\). Compute what proportion of the time the \\(\\Y_\\new\\) actually lie in the interval. Is your prediction interval performing as expected? Why or why not?\n\n\n4 Vary the setting\nChoose three of the following questions to answer.\nExplore how the coverage of the test set varies when:\n\n\\(N\\) increases or decreases, all else fixed\nThe value of \\(\\sigma\\) increase and decreases, all else fixed\nThe values in \\(\\beta\\) increase and decreases, all else fixed\n\\(P\\) increase and decreases (and \\(\\beta\\) changes with it)\nThe distribution of the residuals changes (i.e., try something other than a normal)\nYou draw new values for the training set (but keep the test set fixed)\nYou draw new values for the test set (but keep the training set fixed)\nPick something else that you find interesting to vary!"
  },
  {
    "objectID": "old_assignments/hw2_code.html",
    "href": "old_assignments/hw2_code.html",
    "title": "STAT151A Code homework 2: Due February 9th",
    "section": "",
    "text": "In this homework, you’ll implement some linear algebra ideas in R.\nFor each problem, provide a solution in the form of a function, and then test it using following provided functions:\n# Check whether matrix `x` == `y` for all entries,\n# up to the tolerance `tol`.  If not, an error is raised.\nAssertMatricesEqual &lt;- function(x, y, tol=1e-9) {\n  if (!(all(dim(x) == dim(y)))) {\n    stop(\"The dimensions do not match.\")\n  }\n  err &lt;- max(abs(x - y))\n  if (err &gt; tol) {\n    stop(sprintf(\"The error %f is greater than the tolerance.\", err))\n  }\n}\n\n\n# Generate an `nrow` x `ncol` matrix with random standard normal entries.\nGenerateMatrix &lt;- function(nrow, ncol) {\n  return(rnorm(nrow * ncol) %&gt;% matrix(nrow, ncol))\n}\nFor example,\n# Should fail --- the matrices have the wrong dimension.\nAssertMatricesEqual(GenerateMatrix(3, 3), GenerateMatrix(4, 3))\n\n# Should fail --- the matrices are not equal.\nA &lt;- GenerateMatrix(3, 3)\nAssertMatricesEqual(GenerateMatrix(3, 3), A + 5)\n\n# Should pass\nAssertMatricesEqual(A, A)\n\n(Example) Starting from GenerateMatrix(), write a function that generates a random symmetric matrix.\n\nSolution:\nGenerateSymmetricMatrix &lt;- function(dim) {\n    a_mat &lt;- GenerateMatrix(dim, dim)\n    return(0.5 * (a_mat + t(a_mat)))\n}\n\na_sym &lt;- GenerateSymmetricMatrix(4)\nAssertMatricesEqual(a_sim, t(a_sim))\n\n(Example) Starting from AssertMatricesEqual(), write a function that checks whether a matrix is symmetric.\n\nSolution:\nCheckSymmetricMatrix &lt;- function(a) {\n    AssertMatricesEqual(a, t(a))\n}\n\nStarting from GenerateMatrix(), write a function to generate a random symmetric positive semi-definite matrix of a given size.\nStarting from GenerateMatrix(), write a function to generate a random symmetric positive definite matrix of a given size whose smallest eigenvalue is greater than one. (Hint: you can add something to your previous solution.)\nWrite a function that takes in a symmetric PSD matrix and returns an inverse computed from the eigendecomposition. You may use eigen().\nWrite a function that takes in a symmetric PSD matrix and returns a square root computed from the eigen-decomposition. You may use eigen().\nWrite a function that takes in a symmetric PSD matrix and returns a (possibly non-symmetric) square root computed from the cholesky decomposition. You may use chol().\nWrite a function that takes in a potentially non-square matrix and returns a projection matrix onto the column span. You may assume the matrix is full-rank.\nWrite a function that takes in a potentially non-square matrix and returns a projection matrix onto the row span. You may assume the matrix is full-rank.\nWrite a function that takes in a potentially non-square matrix and returns a projection matrix onto the orthogonal complement of the column span. You may assume the matrix is full-rank.\nWrite a function that takes in a potentially non-square matrix and returns an orthonormal basis for its column span. You may assume the matrix is full-rank.\nWrite a function that takes in a positive definite covariance matrix and returns a draw from the multivariate normal distribution with mean zero and the given covariance matrix. You may use only rnorm(n, mean=0, sd=1) to generate random variables. Verify that you have succeeded using Monte Carlo draws and AssertMatricesEqual() with an appropriate tolerance."
  },
  {
    "objectID": "old_assignments/hw1_code.html",
    "href": "old_assignments/hw1_code.html",
    "title": "STAT151A Code homework 1: Due January 26th",
    "section": "",
    "text": "For all questions below, provide answers in complete sentences, and include correct and readable code to support your answers.\n\n1 Spotify dataset\n\n(a)\nFind another variable (other than danceability) that is associated with popularity according to simple linear regression.\n\n\n(b)\nHow does this association change if you remove low-popularity tracks? You may define low-popularity tracks however you like, but briefly defend your choice.\n\n\n(c)\nIdentify a song that defies the relationship you found. (For example, having found a positive relationship between danceability and popularity, I might find a song that is highly popular but not ``danceable.’’)\nListen to the song on Spotify and comment on whether the result makes sense.\n\n\n\n2 Bodyfat dataset\n\n(a)\nChoose two variables (other than bodyfat). Use lm to regress bodyfat on these two variables and an intercept.\n\n\n(b)\nFor the regression in the previous example, construct your own \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Y}\\) matrices by hand (don’t use the output of lm). Using these, compute your own estimate \\(\\hat{\\beta}\\) and confirm that it matches the output of lm.\n\n\n(c)\nWrite a function in R that computes \\(\\hat{\\beta}\\) from \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Y}\\). Document the function’s inputs and outputs. As an example, you might follow the Function Documentation section of the Amazon R style guide.\n\n\n\n3 Aluminum dataset\n\n(a)\nRun the regression from Lecture 1 using all three specimens, both with and without an intercept term. Plot the results.\nFor convenience, here is a filter function to limit to the right set of data:\nfilter(Strain &lt; 0.0035, Strain &gt; 0.0001,\n       loading_type == \"T\", temp == 20, lot == \"A\")\nComment on whether an intercept should be included and why. When the intercept is estimated, how can it be interpreted?"
  },
  {
    "objectID": "lectures/linalg_review.html",
    "href": "lectures/linalg_review.html",
    "title": "Linear algebra review materials",
    "section": "",
    "text": "This course is about linear models. Furthermore it is, at least in part, a math course. So a strong foundation in linear algebra will be necessary. We will do our best to provide support to either learn or review linear algebra, but there is no substitute for working through the material yourself.\nBelow are some materials that I can recommend for self-study and / or review based on the MIT open courseware linear algebra series. The corresponding textbook, Strang’s Introduction to Linear Algebra, is also good, but is unfortunately not available for free online, and there are only a few hard copies available in the library.\nNot all the material in the MIT linear algebra is necessary for 151A. Based mostly on the titles, here are the sections of the reading that I recommend for 151A:\n\nAbsolutely necessary: Sections 3, 6, 10, 14\nImportant: Sections 7, 15, 16, 10 (video 9), 11 (video 9)\nVery useful: Sections 17, 21, 27 (video 25), 28 (video 25), 33 (video 30)\n\nNote that the numbers in the recommended reading don’t seem to line up with the videos, particularly later in the course. Where it seems the two diverge, I’ve tried to guess which video goes with the corresponding reading.\nAs is always the case with math, reading and listening is no substitute for doing.\nActually working through practice problems is the key to making linear algebra second nature!\nSo if you have a choice between, say, practicing doing some matrix multiplication yourself and watching another video on the singular value decomposition, I’d recommend the former until you’re very comfortable with the basics."
  },
  {
    "objectID": "lectures/Lecture1.html",
    "href": "lectures/Lecture1.html",
    "title": "Class organization",
    "section": "",
    "text": "This is a course about linear models. You probably all know what linear models are already — in short, they are models which fit straight lines through data, possibly high-dimensional data. Every setting we consider in this class will have the following attributes:\n\nA bunch of data points. We’ll index with \\(n = 1, \\ldots, N\\).\nEach datapoint consists of:\n\nA scalar-valued “response” \\(y_n\\)\nA vector-valued “regressor” \\(\\xv_n = (\\x_{n1}, \\ldots, \\x_{nP})\\).\n\n\n\n\n\n\n\n\nNotation\n\n\n\nThroughout the course, I will (almost) always use the letter “x” for regressors and the letter “y” for responses. There will always be \\(N\\) datapoints, and the regressors will be \\(P\\) dimensional. Vectors and matrices will be boldface.\nOf course, I may deviate from this (and any) notation convention by saying so explicitly.\n\n\nWe will be interested in what \\(\\xv_n\\) can tell us about \\(\\y_n\\). This setup is called a “regression problem,” and can be attacked with lots of models, including non-linear models. But we will focus on approaches to this problem that operate via fitting straight lines to the dependence of \\(y_n\\) on \\(\\xv_n\\).\nRelative to a lot of other machine learning or statistical procedures, linear models are relatively easy to analyze and understand. Yet they are also complex enough to exhibit a lot of the strengths and pitfalls of all machine learning and statistics. So really, this is only partly a course about linear models per se. I hope to make it a course about concepts in statistics in machine learning more generally, but viewed within the relatively simple framework of linear models. Some examples that I hope to touch on at least briefly include:\n\nAsymptotics under misspecification\nRegularization\nSparsity\nThe bias / variance tradeoff\nThe influence function\nDomain transfer\nDistributed learning\nConformal inference\nPermutation testing\nBayesian methods\nBenign overfitting\n\nOur results and conclusions will be expressed in formal mathematical statements and in software. For the purpose of this class, I view mathematics and coding as analogous to language, grammar, and style: you need to have a command of these things in order to say something. But the content of this course doesn’t stop and math and conding, just as learning language alone does not give you something to say. Linear models will be a mathematical and computational tool for communicating with and about the real world. Datasets can speak to us in the language of linear models, and we can communicate with other humans through the language of linear models. Learning to communicate effectively in this way is the most important content of this course, and is a skill that will remain relevent whether or not you ever interpret or fit another linear model in your life.\nWhether or not a statistical analysis is “good” cannot be evaluated outside a particular context. Why do we care about the conclusions of this analysis? What will they be used for? Who needs to understand our analysis? What are the consequences of certain kinds of errors? Outside of a classroom, you will probably never encounter a linear model without a real question and context attached to it. I will make a real effort in this class to respect this fact, and always present data in context, to the extent possible within a classroom setting. I hope you will in turn get in the habit of always thinking about the context of a problem, even when going through formal homework and lab exercises. For pedagogical reasons we may have to step into abstract territory at times, but I will make an effort to tie what we learn back to reality, and, in grading we’ll make sure to reward your efforts to do so as well. Just as there is not a “correct” essay in an English class, this will often mean that there are not “correct” analyses for a dataset, even though there are certainly better and worse approaches, as well as unambiguous errors."
  },
  {
    "objectID": "lectures/Lecture1.html#exploratory-data-analysis",
    "href": "lectures/Lecture1.html#exploratory-data-analysis",
    "title": "Class organization",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nSpotify example\nEDA is part of every project (start by plotting your data)\nOften a starting point for more detailed analyses\nAnything goes, but correpsondingly the results may not be that meaningful\nHelpful to have a formal understanding of what regression is doing\nLinear algebra perspective"
  },
  {
    "objectID": "lectures/Lecture1.html#prediction-problems",
    "href": "lectures/Lecture1.html#prediction-problems",
    "title": "Class organization",
    "section": "Prediction problems",
    "text": "Prediction problems\n\nBodyfat example\nHave some pairs of responses and explanatory variables\nGiven new explanatory variables, we want a “best guess” for an unseen response\nWe care about how our model fits the data we have, and how it extrapolates\nThe model itself (i.e., the slopes of the fitted line) doesn’t have much meaning\nCare about uncertainty in and calibration of our prediction\nLoss minimization perspective"
  },
  {
    "objectID": "lectures/Lecture1.html#inference-problems",
    "href": "lectures/Lecture1.html#inference-problems",
    "title": "Class organization",
    "section": "Inference problems",
    "text": "Inference problems\n\nAluminum stress-strain curve example\nWe have a question about the world that can’t be expressed as pure prediction\nSometimes we “reify” our model, even if tentatively\nSometimes has a causal intepretation: if we intervene in some aspect of the world, what will happen?\nWe need some notion of the subjective uncertainty of our estimates\nMaximum likelihood perspective\n\nThese perspectives are highly overlapping, and often a problem doesn’t fit neatly into one or the other. For example, good inference should give good predictions, and inference in a very tentatively reified model is close to exploratory data analysis. Still, I’ll lean on this division to help organize the course conceptually.\nWe can look at these examples in Lecture1_examples.ipynb."
  },
  {
    "objectID": "datasets/data.html",
    "href": "datasets/data.html",
    "title": "Datasets",
    "section": "",
    "text": "The following datasets are used in lectures, homework, and labs.",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "datasets/data.html#aluminum-dataset",
    "href": "datasets/data.html#aluminum-dataset",
    "title": "Datasets",
    "section": "Aluminum dataset",
    "text": "Aluminum dataset\nDataset containing stress-strain curves for commercially available aluminum samples at varying tempertures. The data accompanies B.S. Aakash, JohnPatrick Connors, Michael D. Shields, Variability in the thermo-mechanical behavior of structural aluminum, Thin-Walled Structures, Volume 144, 2019, 106122, ISSN 0263-8231. (link)\nPaper abstract: The nominal performance of structural aluminum alloys at elevated temperature has been thoroughly investigated in the past. Although it is well known that the performance of a given material specimen will differ from the nominal behavior, the extent of this variability has not been quantitied to date. This limits the ability to perform reliability and performance-based design and analysis for aluminum structures subjected to high temperatures (e.g. in structural fire engineering). This work presents an experimental investigation of the variability in the stress-strain behavior of AA 6061-T651 (as a model ductile aluminum alloy). We performed steady-state tensile tests on nine different batches of nominally identical material sourced from different suppliers/manufacturers at six different temperatures (20 °C, 100 °C, 150 °C, 200 °C, 250 °C, and 300 °C) under two different geometries to induce uniaxial tension and plane strain stress states in the gauge section. The results are investigated statistically to illustrate variability in the salient features of the stress-strain behavior of the material ranging from nonlinear elastic behavior to strain localization and ductile fracture. Some observations on material performance and its variability are made along the way. Overall, it is illustrated that variations between batches of material can be quite large and – especially as it relates to strain localization, necking, and material failure – variations can be very large even within a fixed batch of material. To encourage data of this nature to be expanded and integrated into research and practice to improve structural design and investigations, the full searchable dataset are publicly available with experimental details published concurrently through Data in Brief.\n\nLink to csv\nSource link (downloaded Dec 2023)",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "datasets/data.html#bodyfat-dataset",
    "href": "datasets/data.html#bodyfat-dataset",
    "title": "Datasets",
    "section": "Bodyfat dataset",
    "text": "Bodyfat dataset\nBodyfat and other physical measurements on a number of individuals.\nMeasurement standards are apparently those listed in Benhke and Wilmore (1974), pp. 45-48 where, for instance, the abdomen 2 circumference is measured “laterally, at the level of the iliac crests, and anteriorly, at the umbilicus”.\nThese data are used to produce the predictive equations for lean body weight given in the abstract “Generalized body composition prediction equation for men using simple measurement techniques”, K.W. Penrose, A.G. Nelson, A.G. Fisher, FACSM, Human Performance Research Center, Brigham Young University, Provo, Utah 84602 as listed in Medicine and Science in Sports and Exercise, vol. 17, no. 2, April 1985, p. 189.\n\nLink to csv\nSource link (downloaded Dec 2023)",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "datasets/data.html#spotify-dataset",
    "href": "datasets/data.html#spotify-dataset",
    "title": "Datasets",
    "section": "Spotify dataset",
    "text": "Spotify dataset\nThis dataset consists of roughly 30,000 Songs from the Spotify API with black-box machine learning quantifications of musical features. No guarantees are made on how the tracks were sampled.\n\nLink to csv\nSource link (downloaded Dec 2023)",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Calendar",
    "section": "",
    "text": "We can just embed the iframe html:"
  },
  {
    "objectID": "assignments/assignments.html",
    "href": "assignments/assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Homework 1 (tentative and in progress — do not begin yet)",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/hw1_math.html",
    "href": "assignments/hw1_math.html",
    "title": "STAT151A Homework 1 (prerequisites review)",
    "section": "",
    "text": "1 Linear systems\nWrite the following system of equations in matrix form. Say whether each system has no solutions, a single solution, or an infinite number of solutions, and how you know.\n\n\\[\n\\begin{aligned}\na_1 + 2 a_2 ={}& 1 \\\\\na_1 + 3 a_2 ={}& 0 \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\na_1 + 2 a_2 + 3 a_3 ={}& 1 \\\\\na_1 + 3 a_2 + 3 a_3 ={}& 0 \\\\\n2 a_1 + 4 a_2 + 3 a_3 ={}& 5 \\\\\n\\end{aligned}\n\\]\n\n\n\\[\n\\begin{aligned}\na_1 + 2 a_2 + 3 a_3 ={}& 1 \\\\\na_1 + 3 a_2 + 3 a_3 ={}& 0 \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\na_1 + 2 a_2  ={}& 1 \\\\\na_1 + 3 a_2  ={}& 0 \\\\\n2 a_1 + 4 a_2  ={}& 5 \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\na_1 + 2 a_2 ={}& 1 \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\na_1 + 2 a_2 ={}& 1 \\\\\na_1 + 2 a_2 ={}& 1 \\\\\na_1 + 2 a_2 ={}& 1 \\\\\na_1 + 2 a_2 ={}& 1 \\\\\na_1 + 2 a_2 ={}& 1 \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\na_1 + 2 a_2 ={}& 1 \\\\\na_1 + 2 a_2 ={}& 2 \\\\\na_1 + 2 a_2 ={}& 3 \\\\\na_1 + 2 a_2 ={}& 4 \\\\\na_1 + 2 a_2 ={}& 5 \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\na_1 + 2 a_2 ={}& 1 \\\\\na_1 + 3 a_2 ={}& 1 \\\\\na_1 + 4 a_2 ={}& 1 \\\\\na_1 + 5 a_2 ={}& 1 \\\\\na_1 + 6 a_2 ={}& 5 \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n5 a_1 + 2 a_2 ={}& 1 \\\\\n10 a_1 + 4 a_2 ={}& 1 \\\\\n\\end{aligned}\n\\]\n\n\n\n2 Dimensions of linear algebra expressions\nFor this problem, I will use the following definitions.\n\n\\(\\boldsymbol{X}\\) denotes an \\(N \\times P\\) matrix\n\\(\\boldsymbol{y}\\) denotes an \\(N\\)–vector (i.e. an \\(N \\times 1\\) matrix)\n\\(\\boldsymbol{1}\\) denotes a \\(N\\)–vector containing all ones\n\\(\\boldsymbol{A}\\) denotes a \\(P\\times P\\) matrix\n\\(\\boldsymbol{\\beta}\\) denotes a \\(P\\)–vector\n\nI will take \\(N &gt; P &gt; Q &gt; 1\\). A transpose is denoted with a superscript \\(\\intercal\\), and an inverse by a superscipt \\(-1\\). A matrix trace is denoted \\(\\mathrm{trace}\\left(\\right)\\). You may assume that each matrix is full column rank.\nFor each expression, write the dimension of the result, or write “badly formed” if the expression is not a valid matrix expression.\nTip: For this assignment, and throughout the class, it can be very helpful to write the dimensions of a matrix or vector underneath to help make sure your matrix expressions are valid. For example, we can write \\(\\underset{PN}{\\boldsymbol{X}^\\intercal} \\underset{NP}{\\boldsymbol{X}}\\), which we can see is valid because the \\(N\\)’s are next to one another. Similarly, we can see immediately that the expression \\(\\underset{NP}{\\boldsymbol{X}} \\underset{NP}{\\boldsymbol{X}}\\) is invalid because \\(P\\) is next to \\(N\\) in the matrix multiplication, which is not allowed.\n\n\\(\\boldsymbol{X}^\\intercal\\boldsymbol{y}\\)\n\\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\)\n\\(\\boldsymbol{X}+ \\boldsymbol{y}\\)\n\\(\\boldsymbol{\\beta}\\boldsymbol{\\beta}^\\intercal\\)\n\\(\\boldsymbol{\\beta}^\\intercal\\boldsymbol{\\beta}\\)\n\\(\\boldsymbol{y}^\\intercal\\boldsymbol{y}\\)\n\\(\\boldsymbol{X}^\\intercal\\boldsymbol{y}\\)\n\\(\\left(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^\\intercal\\)\n\\(\\left(\\boldsymbol{X}^\\intercal\\boldsymbol{y}\\right)^\\intercal\\)\n\\(\\left(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1}\\)\n\\(\\left(\\boldsymbol{X}^\\intercal\\boldsymbol{y}\\right)^{-1}\\)\n\\(\\boldsymbol{X}^{-1}\\)\n\\(\\boldsymbol{y}^{-1} \\boldsymbol{y}\\)\n\\((\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{X}^\\intercal\\)\n\\((\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{X}^\\intercal\\boldsymbol{y}\\)\n\\(\\left( (\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{X}^\\intercal\\boldsymbol{y}\\right)^\\intercal\\)\n\\(\\boldsymbol{y}^\\intercal\\boldsymbol{X}(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1}\\)\n\\(\\boldsymbol{X}\\beta\\)\n\\(\\boldsymbol{y}- \\boldsymbol{X}\\beta\\)\n\\(\\boldsymbol{y}- \\boldsymbol{X}^\\intercal\\beta\\)\n\\(\\boldsymbol{y}^\\intercal- \\beta^\\intercal\\boldsymbol{X}^\\intercal\\)\n\\(\\boldsymbol{\\beta}- (\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1}  \\boldsymbol{X}^\\intercal\\boldsymbol{y}\\)\n\\(\\boldsymbol{X}\\left( \\boldsymbol{\\beta}- (\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1}  \\boldsymbol{X}^\\intercal\\boldsymbol{y}\\right)\\)\n\\(\\boldsymbol{1}^\\intercal\\boldsymbol{y}\\)\n\\(\\boldsymbol{y}- (\\boldsymbol{1}^\\intercal\\boldsymbol{y}) \\boldsymbol{y}\\)\n\\(\\boldsymbol{X}(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{X}^\\intercal\\)\n\\(\\boldsymbol{X}^\\intercal(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{X}\\)\n\\(\\boldsymbol{X}^\\intercal(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{X}\\)\n\\(\\boldsymbol{\\beta}^\\intercal(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1}\\boldsymbol{\\beta}\\)\n\\(\\left( \\boldsymbol{\\beta}^\\intercal(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1}\\boldsymbol{\\beta}\\right)^{-1}\\)\n\\(\\mathrm{trace}\\left( \\boldsymbol{\\beta}^\\intercal(\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1}\\boldsymbol{\\beta}\\right)\\)\n\\(\\mathrm{trace}\\left( (\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1}\\boldsymbol{\\beta}\\boldsymbol{\\beta}^\\intercal\\right)\\)\n\n\n\n3 Orthonormal vectors and bases\nFor this problem, assume that \\(\\boldsymbol{u}= (u_1, u_2)^\\intercal\\) and \\(\\boldsymbol{v}= (v_1, v_2)^\\intercal\\) are orthonormal. That is, \\(\\boldsymbol{u}^\\intercal\\boldsymbol{u}= \\boldsymbol{v}^\\intercal\\boldsymbol{v}= 1\\), and \\(\\boldsymbol{u}^\\intercal\\boldsymbol{v}= 0\\). Let \\(\\boldsymbol{a}= (a_1, a_2)^\\intercal\\) denote a generic \\(2\\)–dimensional vector.\n\nWrite an expression for the length of \\(\\boldsymbol{a}\\) in terms of its entries \\(a_1\\) and \\(a_2\\).\nWrite an expression for the length of \\(\\boldsymbol{a}\\), using only matrix operations (i.e., without explicit reference to the entries of \\(\\boldsymbol{a}\\)).\nWrite an explicit expression for a vector pointing in the same direction as \\(\\boldsymbol{a}\\) but with unit length. (Hint: show that, for a scalar \\(\\alpha\\), the length of \\(\\alpha \\boldsymbol{a}\\) is \\(\\alpha^2\\) times the length of \\(\\boldsymbol{a}\\), and then make a clever choice for \\(\\alpha\\).)\nWrite an explicit expression for a vector pointing in the same direction as \\(\\boldsymbol{v}\\) but with the same length as \\(\\boldsymbol{a}\\).\nSuppose that I tell you that \\(\\boldsymbol{a}= \\alpha \\boldsymbol{u}+ \\gamma \\boldsymbol{v}\\). Find an explicit expression for \\(\\alpha\\) in terms of \\(\\boldsymbol{a}\\) and \\(\\boldsymbol{u}\\) alone.\nFind explicit expressions for scalars \\(\\alpha\\) and \\(\\gamma\\) such that \\(\\boldsymbol{a}= \\alpha \\boldsymbol{u}+ \\gamma \\boldsymbol{v}\\).\nLet \\(\\begin{pmatrix} \\boldsymbol{u}& \\boldsymbol{v}\\end{pmatrix} := \\begin{pmatrix}\nu_1 & v_1 \\\\ u_2 & v_2\\end{pmatrix}\\) denote the \\(2 \\times 2\\) matrix with \\(\\boldsymbol{u}\\) in the first column and \\(\\boldsymbol{v}\\) in the second column. Show that \\(\\begin{pmatrix} \\boldsymbol{u}& \\boldsymbol{v}\\end{pmatrix}^\\intercal= \\begin{pmatrix} \\boldsymbol{u}& \\boldsymbol{v}\\end{pmatrix}^{-1}\\).\nObserve that \\(\\alpha \\boldsymbol{u}+ \\gamma \\boldsymbol{v}=\n\\begin{pmatrix}\n\\boldsymbol{u}&\n\\boldsymbol{v}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\alpha \\\\\n\\gamma\n\\end{pmatrix}.\\) Using this, write an explicit expression for the coefficient vector \\((\\alpha, \\gamma)^\\intercal\\) in terms of \\(\\begin{pmatrix} \\boldsymbol{u}& \\boldsymbol{v}\\end{pmatrix}\\) and \\(\\boldsymbol{a}\\).\nSuppose I tell you that that \\(\\boldsymbol{u}= (1, 0)^\\intercal\\). In terms of \\(a_1\\) and \\(a_2\\), what is \\(\\alpha\\) in the decomposition \\(\\boldsymbol{a}= \\alpha \\boldsymbol{u}+ \\gamma \\boldsymbol{v}\\)?\n\n\n\n4 Eigenvalues and eigenvectors of square symmetric matrices\nLet \\(\\boldsymbol{A}\\) denote a \\(P \\times P\\) symmetric, square matrix. Recall that an “eigenvalue” \\(\\lambda_k\\) of and its associated “eigenvector” \\(\\boldsymbol{u}_k\\) of \\(\\boldsymbol{A}\\) satisfy \\(\\boldsymbol{A}\\boldsymbol{u}_k = \\lambda_k \\boldsymbol{u}_k\\). (In this definition, \\(\\boldsymbol{u}_k\\) must be non–degenerate, i.e., it must have some nonzero entries.)\nLet \\(\\boldsymbol{U}= (\\boldsymbol{u}_1 \\ldots \\boldsymbol{u}_P)\\) denote the \\(P \\times P\\) matrix with eigenvector \\(\\boldsymbol{u}_k\\) in the \\(k\\)–th column, and let \\(\\Lambda\\) denote the \\(P \\times P\\) diagonal matrix with \\(\\lambda_k\\) in the \\(k\\)–th diagonal entry. Let \\(\\boldsymbol{a}\\) denote a generic \\(P\\)–vector.\n\nIf \\(\\boldsymbol{A}\\) is the identity matrix (i.e., the matrix with ones on the diagonal and zeros elsewhere), what are its eigenvalues?\nIf \\(\\boldsymbol{A}\\) is the zero matrix (i.e., the matrix containing only zeros), what are its eigenvalues?\nIf \\(\\boldsymbol{A}\\) is a diagonal matrix with the entries \\(a_1, \\ldots, a_P\\) on the diagonal, what are its eigenvalues?\nLet us prove that the eigenvectors can be taken to be unit vectors without loss of generality.\n\nShow that, if \\(\\boldsymbol{u}_k\\) is an eigenvector, then \\(\\alpha \\boldsymbol{u}_k\\) is also an eigenvector with the same eigenvalue as \\(\\boldsymbol{u}_k\\) for any scalar \\(\\alpha \\ne 0\\).\nIn particular, show that \\(\\boldsymbol{u}_k' = \\boldsymbol{u}_k / \\sqrt{\\boldsymbol{u}_k^\\intercal\\boldsymbol{u}_k}\\) is also an eigenvector with eigenvalue \\(\\lambda_k\\), and that \\((\\boldsymbol{u}_k')^\\intercal\\boldsymbol{u}'_k = 1\\).\n\nLet us prove that, for a general symmetric matrix, eigenvectors with distinct eigenvalues are orthogonal. That is, if \\(\\lambda_k \\ne \\lambda_j\\) for some \\(k \\ne j\\), we will show that \\(\\boldsymbol{u}_k^\\intercal\\boldsymbol{u}_j = 0\\). Carefully justify each step in the following proof.\n\nWe have \\(\\boldsymbol{u}_j^\\intercal\\boldsymbol{A}\\boldsymbol{u}_k = (\\boldsymbol{u}_j^\\intercal\\boldsymbol{A}\\boldsymbol{u}_k)^\\intercal\\). (Why?)\nWe also have \\((\\boldsymbol{u}_j^\\intercal\\boldsymbol{A}\\boldsymbol{u}_k)^\\intercal= \\boldsymbol{u}_k^\\intercal\\boldsymbol{A}^\\intercal\\boldsymbol{u}_j\\). (Why?)\nWe then have \\(\\boldsymbol{u}_j^\\intercal\\boldsymbol{A}\\boldsymbol{u}_k = \\boldsymbol{u}_k^\\intercal\\boldsymbol{A}\\boldsymbol{u}_j\\). (Why?)\nWe then have \\(\\lambda_k \\boldsymbol{u}_j^\\intercal\\boldsymbol{u}_k = \\lambda_j \\boldsymbol{u}_k^\\intercal\\boldsymbol{u}_j\\). (Why?)\nWe then have \\(\\boldsymbol{u}_k^\\intercal\\boldsymbol{u}_j = 0\\). (Why?)\nNote (not graded): If the eigenvalues are not distinct, then the corresponding eigenvectors may not be orthogonal. However, one can always find an orthogonal set of eigenvectors for each repeated eigenvalue, though we won’t prove this here. See a linear algebra text for the proof, or try yourself! Hint: if \\(\\lambda_k = \\lambda_j\\) for some \\(k \\ne j\\), then \\(\\alpha \\boldsymbol{u}_k + \\gamma \\boldsymbol{u}_j\\) is also an eigenvector with eigenvalue \\(\\lambda_k\\) for any \\(\\alpha\\) and \\(\\gamma\\).\n\nSuppose that each \\(\\lambda_k \\ne 0\\). Show that the inverse of \\(\\Lambda\\) is given by the diagonal matrix with \\(1  / \\lambda_k\\) in the \\(k\\)–th diagonal entry and zero elsewhere.\nSuppose that \\(\\lambda_k = 0\\) for some \\(k\\). Show that \\(\\Lambda\\) is not invertible. Hint: find a vector \\(\\boldsymbol{b}\\) such that \\(\\Lambda \\boldsymbol{a}= \\boldsymbol{b}\\) has no solution. From this it would follow that \\(\\Lambda\\) is not invertible by contradiction for, if \\(\\Lambda\\) were invertible, \\(\\boldsymbol{a}= \\Lambda^{-1} \\boldsymbol{b}\\) would be a solution.\nAssume that \\(\\boldsymbol{A}\\) has \\(P\\) orthonormal eigenvectors (it always does; we have proved some parts of this assertion above). We will prove that we then have \\(\\boldsymbol{A}= \\boldsymbol{U}\\Lambda \\boldsymbol{U}^\\intercal\\). Carefully justify each step in the following proof.\n\nFor any \\(\\boldsymbol{a}\\), we can write \\(\\boldsymbol{a}= \\sum_{p=1}^P \\alpha_p \\boldsymbol{u}_p\\) for some scalars \\(\\alpha_p\\). (Why?)\nUsing the previous expansion, we thus have \\(\\boldsymbol{A}\\boldsymbol{a}= \\sum_{p=1}^P \\lambda_p \\alpha_p \\boldsymbol{u}_p\\). (Why?)\nLet \\(\\boldsymbol{\\alpha}= (\\alpha_1, \\ldots, \\alpha_P)^\\intercal\\). Then \\(\\boldsymbol{a}= \\boldsymbol{U}\\boldsymbol{\\alpha}\\). (Why?)\nSimilarly, we have \\(\\boldsymbol{A}\\boldsymbol{a}= \\boldsymbol{U}\\Lambda \\boldsymbol{\\alpha}\\). (Why?)\nWe also have \\(\\boldsymbol{u}_p^\\intercal\\boldsymbol{a}= \\alpha_p\\). (Why?)\nWe then have \\(\\boldsymbol{\\alpha}= \\boldsymbol{U}^\\intercal\\boldsymbol{a}\\). (Why?)\nCombining, we have \\(\\boldsymbol{A}\\boldsymbol{a}= \\boldsymbol{U}\\Lambda \\boldsymbol{U}^\\intercal\\boldsymbol{a}\\). (Why?)\nSince the preceding expression is true for any \\(\\boldsymbol{a}\\), we must have \\(\\boldsymbol{A}= \\boldsymbol{U}\\Lambda \\boldsymbol{U}^\\intercal\\). (Why? Hint: if you take \\(\\boldsymbol{a}\\) to be \\(1\\) in the first entry and zeros elsewhere, then we have that the first columns match. Continue in this fashion to show that each entry of the matrices are the same.)\n\n\n\n\n5 Statistical asymptotics\nFor this problem, suppose that \\(x_n\\) are independent and identically distributed random variables, with \\(\\mathbb{E}\\left[x_n\\right] = 3\\), and \\(\\mathrm{Var}\\left(x_n\\right) = 4\\). You should not assume that the \\(x_n\\) are normally distributed.\nRecall that \\(\\mathrm{Var}\\left(x_n\\right) = \\mathbb{E}\\left[x_n^2\\right] - \\mathbb{E}\\left[x_n\\right]^2\\). Also recall that, for any scalar \\(\\alpha\\), \\(\\mathbb{E}\\left[\\alpha x_n\\right] = \\alpha \\mathbb{E}\\left[x_n\\right]\\), and \\(\\mathrm{Var}\\left(\\alpha x_n\\right) = \\mathbb{E}\\left[(\\alpha x_n)^2\\right] - \\mathbb{E}\\left[\\alpha x_n\\right]^2 = \\alpha^2 \\mathrm{Var}\\left(x_n\\right)\\).\nFor each limiting statement, state clearly whether your answer is a constant, a random variable, or that the limit does not exist. If the limit is a random variable, give its distribution. If it is a constant, give its value. If it does not exist, argue why. You may refer to known results from probability theory, particularly the law of large numbers and the central limit theorem.\n\nFor any particular (finite) value of \\(N\\), is \\(\\frac{1}{N} \\sum_{n=1}^Nx_n\\) a constant or a random variable?\nFor any particular (finite) value of \\(N\\), is \\(\\frac{1}{N} \\sum_{n=1}^N\\mathbb{E}\\left[x_n\\right]\\) a constant or a random variable?\nCompute the expectation \\(\\mathbb{E}\\left[x_1 + x_2\\right]\\).\nCompute the expectation \\(\\mathbb{E}\\left[\\frac{1}{2} (x_1 + x_2)\\right]\\).\nCompute the expectation \\(\\mathbb{E}\\left[\\frac{1}{3} (x_1 + x_2 + x_3)\\right]\\).\nCompute the expectation \\(\\mathbb{E}\\left[\\frac{1}{N} \\sum_{n=1}^Nx_n\\right]\\).\nCompute the variance \\(\\mathrm{Var}\\left(x_1 + x_2\\right)\\).\nCompute the variance \\(\\mathrm{Var}\\left(\\frac{1}{2} (x_1 + x_2)\\right)\\).\nCompute the variance \\(\\mathrm{Var}\\left(\\frac{1}{3} (x_1 + x_2 + x_3)\\right)\\).\nCompute the variance \\(\\mathrm{Var}\\left(\\frac{1}{N} \\sum_{n=1}^Nx_n\\right)\\).\nAs \\(N \\rightarrow \\infty\\), what (if anything) does \\(\\frac{1}{N} \\sum_{n=1}^Nx_n\\) converge to?\nAs \\(N \\rightarrow \\infty\\), what (if anything) does \\(\\frac{1}{N} \\sum_{n=1}^N(x_n - 3)\\) converge to?\nAs \\(N \\rightarrow \\infty\\), what (if anything) does \\(\\frac{1}{\\sqrt{N}} \\sum_{n=1}^N(x_n - 3)\\) converge to?\nAs \\(N \\rightarrow \\infty\\), what (if anything) does \\(\\frac{1}{\\sqrt{N}} \\sum_{n=1}^Nx_n\\) converge to?\nAs \\(N \\rightarrow \\infty\\), what (if anything) does \\(\\sum_{n=1}^Nx_n\\) converge to?\nAs \\(N \\rightarrow \\infty\\), what (if anything) does \\(\\sum_{n=1}^N(x_n - 3)\\) converge to?\n(Bonus question — will not be graded) What is the limit as \\(N \\rightarrow \\infty\\) of \\(\\frac{1}{N} \\sum_{n=1}^N(4 \\cdot (-1)^n + 3)\\)? In contrast, suppose that \\(x_n\\) is generated by randomly flipping a coin and setting \\(x_n = 3 + 4\\) if the coin comes up heads and \\(x_n = 3 - 4\\) if it comes up tails. What is different about the convergence of \\(\\frac{1}{N} \\sum_{n=1}^N(4 \\cdot (-1)^n + 3)\\) and the convergence of \\(\\frac{1}{N} \\sum_{n=1}^Nx_n\\)?"
  },
  {
    "objectID": "course_policies.html",
    "href": "course_policies.html",
    "title": "Syllabus and Course Structure",
    "section": "",
    "text": "Objectives\nBy the end of the course, you should be able to\n\nExpress standard regression analyses both mathematically and in R code\nCritically relate the intended use of a regression analysis to its methods and assumptions\nIdentify common practical and conceptual pitfalls of regression analysis, and to improve the analysis when possible\nCommunicate the process and results of a regression analysis simply and clearly for a broad audience, using well-organized prose, reproducible code, and effective data visualizations.\n\n\n\nAssignments, Exams, and Grading\n\nAttendance\nAttendance in lectures will be required and will contribute to the participation portion of the students’ grade. Laptops will not be permitted in lectures, and violation of this policy can constitute an absence for the purpose of the participation grade.\nIpads and phones will be permitted during lecture for note-taking as long as their use doesn’t inhibit participation.\nEach student will be given four lecture absences without losing any participation points, with the expectation that these absences will be used for illness and emergencies. Additional excused absences will be granted only with an exception granted by the Berkeley DSP office.\nAttendance at labs will be optional.\n\n\nGrading.\nThe weighting for the grades will be:\n\nHomework completion (each weighted equally): 25%\nHomework correctness (each weighted equally): 5%\nQuizzes (each weighted equally): 30%\nFinal exam: 15%\nFinal group project: 15%\nParticipation (primarily lecture attendance): 10%\n\nGrades will not be curved except where otherwise noted. Letter grades will be assigned according the weighted points earned. A score within [90-92%) will earn an A-, [92-98%) will earn an A, and [99-100%) will earn an A+. Scores in the 80’s will receive B’s, in the 70’s will receive C’s, in the 60’s will receive D’s, with the same thresholds for plusses and minuses. Scores below 60% will be considered failing. Grades will be non-negotiable.\n\n\nFinal exam.\nAn in-person pencil-and-paper final exam will be scheduled during the usual final exam week. Students will be allowed a one-page double-sided “cheatsheet” on the final exam.\n\n\nQuizzes.\nEvery two weeks we will have an twenty-minute in-class quiz, typically on the Tuesday following a homework due date. These quizzes will take the place of a sitdown midterm exam (i.e., there will be no midterm). No external materials, including cheatsheets, will be allowed during quizzes.\n\n\nHomework.\nHomework assignments will be due every two weeks on Fridays two weeks later at 9pm. I will try to release new homework on the website as soon as possible after the old one is due, typically the following Monday evening. Homework will typically consist of a combination of mathematical problems and data analysis in R. All homework will be due as a pdf via Gradescope unless otherwise noted. Students can use whatever tool they like to produce the pdf (latex, Rmd, Jupyter, scanned handwritten notes for mathematical problems, etc.).\nThe purpose of homework is for students to attempt to work through problems on their own, both to advance their own understanding, and to allow the instructors to monitor student learning. Neither of these objectives are served if students are copying answers. For that reason, thoughtful and complete homework answers will receive nearly full credit (80% of the available homework points) even if incorrect. We strongly encourage students to submit their own best efforts, even if imperfect, rather than copy a correct answer.\n\n\nFinal group project.\nStudents will form groups of up to three people to submit a final project consisting of an analysis of a real dataset applying principles and techniques from the course.\n\n\nTurning-in assignments\nYou will be turning in your assignments on a platform called\nGradescope. This is also the platform where your assignments will be graded, so you can return there to get feedback on your work. You are welcome to file a regrade request if you notice that we made an error in applying the rubric to your work, but be sure to do so within a week of the grades being posted. We will not accept regrade requests past that point.\nIn order to provide flexibility around emergencies that might arise for you throughout the semester (for example, missing a quiz due to COVID), we will apply for everyone:\n\none emergency drop for quizzes\n\ntwo emergency drops for homework (applying to an entire two–week assignment)\n\nThis means that we will drop your lowest quiz score (which would be a 0 if you were absent) before computing your quiz average. For homework, we will drop your two lowest. Unless students are excused by official university policies, additional drops will not be given.\nWe strongly recommend that students reserve their emergency drops for real emergencies.\n\n\nLate Work\nLate work will not be accepted. If work is not submitted on time, it will receive a zero.\nIt is entirely the students’ responsibility to turn work in on time. If there is any uncertainty concerning this policy, please discuss your concerns with the professor, not with the GSI or reader.\n\n\n\nPrerequisites\nThis course will assume familiarity with the material in STAT 135 or STAT 102. STAT 135 implies other prerequisite courses (STAT 134 and its prerequisites). In particular, you must have had linear algebra, so you should be familiar with basic matrix operations, vector subspaces and projections, rank and invertibility of matrices, and quadratic forms.\nThis semester of Stat151A will include labs and projects in the R language. Proficiency with R at the level of the is a prerequisite. Students with a strong background in another programming language (e.g. Python) will be permitted to enroll with the understanding that they will learn R on their own prior to the start of the class.\n\n\nMaterials\nUnelss otherwise noted, the primary materials for the course are the lecture notes, which will be posted to the course website in advance of class. The following textbooks are useful supplementary texts and are all freely available online:\n\nVeridical Data Science Yu, Barter\nLinear Models and Extensions Ding\nRegression and other Stories Gelman, Hill, Vehtari\nAn Introduction to Statistical Learning James, Witten, Hastie, Tibshirani\nR for Data Science, Wickham, Grolemund\nEconometric Theory and Methods Davidson, MacKinnon\n\n\nRStudio\nThe software that we’ll be using for our data analysis is the free and open-source language called R that we’ll be interacting with via software called RStudio. If you have difficulty installing RStudio, please reach out to an instructor.\n\n\nCourse website\nAll of the assignments will be posted to the course website at https://stat151a.berkeley.edu/fall-2024/. This also holds the course notes, the syllabus, and links to Gradescope and RStudio.\n\n\n\nPolicies\n\nCourse Culture\nStudents taking STAT151A come from a wide range of backgrounds. We hope to foster an inclusive and supportive learning environment based on curiosity rather than competition. All members of the course community—the instructor, students, tutors, and readers—are expected to treat each other with courtesy and respect.\nYou will be interacting with course staff and fellow students in several different environments: in class, over the discussion forum, and in office hours. Some of these will be in person, some of them will be online, but the same expectations hold: be kind, be respectful, be professional.\nIf you are concerned about classroom environment issues created by other students or course staff, please come talk to the instructors about it.\n\n\nCollaboration policy\nYou are encouraged to collaborate with your fellow students on problem sets and labs, but the work you turn in should reflect your own understanding and all of your collaborators must be cited. The individual component of quizzes, reading questions, and exams must reflect only your work.\nResearchers don’t use one another’s research without permission; scholars and students always use proper citations in papers; professors may not circulate or publish student papers without the writer’s permission; and students may not circulate or post non-public materials (quizzes, exams, rubrics-any private class materials) from their class without the written permission of the instructor.\nThe general rule: you must not submit assignments that reflect the work of others unless they are a cited collaborator.\nThe following examples of collaboration are allowed and in fact encouraged!\n\nDiscussing how to solve a problem with a classmate.\nShowing your code to a classmate along with an error message or confusing output.\nPosting snippets of your code to the discussion forum when seeking help.\nHelping other students solve questions on the discussion with conceptual pointers or snippets of code that doesn’t whole hog give away the answer.\nGoogling the text of an error message.\nCopying small snippets of code from answers on Stack Overflow.\n\nThe following examples are not allowed:\n\nLeaving a representation of your assignment (the text, a screenshot) where students (current and future) can access it. Examples of this include websites like course hero, on a group text chain, over discord/slack, or in a file passed on to future students.\nAccessing and submitting solutions to assignments from other students distributed as above. This includes copying written answers from other students and slightly modifying the language to differentiate it.\nSearching or using generative AI to produce complete problem solutions.\nWorking on the final exam or individual quizzes in collaboration with other people or resources. These assignments must reflect individual work.\nSubmitting work on an exam that reflects consultation with outside resources or other people. Exams must reflect individual work.\n\nIf you have questions about the boundaries of the policy, please ask. We’re always happy to clarify.\n\n\nViolations of the collaboration policy\nThe integrity of our course depends on our ability to ensure that students do not violate the collaboration policy. We take this responsibility seriously and forward cases of academic misconduct to the Center for Student Conduct.\nStudents determined to have violated the academic misconduct policy by the Center for Student Conduct will receive a grade penalty in the course and a sanction from the university which is generally: (i) First violation: Non-Reportable Warning and educational intervention, (ii) Second violation: Suspension/Disciplinary Probation and educational interventions, (iii) Third violation: Dismissal.\nAgain, if you have questions about the boundaries of the collaboration policy, please ask!\n\n\nLaptop policy\nLaptops will not be permitted in lecture, but will be required for labs.\nIf you do not have access to a laptop, you can borrow one from the University library. See the UC Berkeley hardware lending program for more details. The Student Technology Equity Program is another good resource. Feel free to contact the instructor if you have concerns about your access to needed technology.\n\n\nCOVID policy\nMaintaining your health and that of the Berkeley community is of primary importance to course staff, so if you are feeling ill or have been exposed to illness, please do not come to class. All of the materials used in class will be posted to the course website. You’re encouraged to reach out to fellow students to discuss the class materials or stop by group tutoring or office hours to chat with a tutor or the instructor.\n\n\nAccomodations for students with disabilities\nStat 151A is a course that is designed to allow all students to succeed. If you have letters of accommodations from the Disabled Students’ Program, please share them with your instructor as soon as possible, and we will work out the necessary arrangements.\n\n\n\n\n\n\nNote\n\n\n\nThese course polices are based on a template and text generously shared by Andrew Bray. Thanks, Andrew!",
    "crumbs": [
      "Course Policies"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 151A: Linear Models",
    "section": "",
    "text": "RStudio\n\n  Gradescope\n\n  BCourses\n\n  ED\n\n\nNo matching items",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Statistics 151A: Linear Models",
    "section": "Instructors",
    "text": "Instructors\n\n\nInstructor: Ryan Giordano  Office: 389 Evans Hall Office hours: TBD rgiordano@berkeley.edu pronouns: He / him\n\n\nGSI: Haodong Ling  Office: TBD Evans Hall Office hours: TBD haodong_ling@berkeley.edu pronouns: He / him\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease reach out to Haodong if you need to be manually added to BCourses.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Statistics 151A: Linear Models",
    "section": "Course content",
    "text": "Course content\nThis website will contain lecture materials and assignments. Day-to-day announcements can be found in BCourses. Discussions can be found in ED. (See links above.)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Statistics 151A: Linear Models",
    "section": "Schedule",
    "text": "Schedule\nLectures will be held Aug 28 2024 – Dec 05 2024 on Tuesday and Thursday, 12:30 pm – 2:00 pm, in Social Science 20.\nLabs will be held on Wednesdays from 9:00 am – 11:00 am and 2:00pm – 4:00 pm in Evans 342.\n(Link to official course calendar.)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#linear-algebra-review-materials",
    "href": "index.html#linear-algebra-review-materials",
    "title": "Statistics 151A: Linear Models",
    "section": "Linear algebra review materials",
    "text": "Linear algebra review materials\nBasic linear algebra is a serious prerequisite for this course. You can find a summary of useful review materials here.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#tentative-course-calendar",
    "href": "index.html#tentative-course-calendar",
    "title": "Statistics 151A: Linear Models",
    "section": "(Tentative) Course Calendar",
    "text": "(Tentative) Course Calendar\nThe following schedule will almost certainly change.\n\n\n   Week 1\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Aug 29:\n           \n           Lecture 1 \n               \n               Sample averages for prediction and inference \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 2\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Sep 3:\n           \n           Lecture 2 \n               \n               Simple linear regression as loss minimization \n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Sep 5:\n           \n           Lecture 3 \n               \n               The law of large numbers and the central limit theorem \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 3\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Sep 10:\n           \n           Lecture 4 \n               \n               Simple linear regression as projection \n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Sep 12:\n           \n           Lecture 5 \n               \n               Multivariate linear regression \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 4\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Sep 17:\n           \n           Lecture 6 \n               \n               Regression as conditional expectation \n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Sep 19:\n           \n           Lecture 7 \n               \n               Explained and residual variance \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 5\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Sep 24:\n           \n           Lecture 8 \n               \n               Regression to the mean \n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Sep 26:\n           \n           Lecture 9 \n               \n               The FWL theorem \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 6\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Oct 1:\n           \n           Lecture 10 \n               \n               Transformations of variables \n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Oct 3:\n           \n           Lecture 11 \n               \n               The `lm` function in `R` \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 7\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Oct 8:\n           \n           Lecture 12 \n               \n               Influence and Outliers \n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Oct 10:\n           \n           Lecture 13 \n               \n               Omitted variables in inference and prediction \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 8\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Oct 15:\n           \n           Lecture 14 \n               \n               Confidence intervals and hypothesis testing \n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Oct 17:\n           \n           Lecture 15 \n               \n               Coefficient tests under normality \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 9\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Oct 22:\n           \n           Lecture 16 \n               \n               Testing under machine learning assumptions \n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Oct 24:\n           \n           Lecture 17 \n               \n               Misspecification and the sandwich covariance matrix \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 10\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Oct 29:\n           \n           Lecture 18 \n               \n               Bayesian inference \n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Oct 31:\n           \n           Lecture 19 \n               \n               Linear regression with Markov Chain Monte Carlo \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 11\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Nov 5:\n           \n           Lecture 20 \n               \n               Random effects models \n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Nov 7:\n           \n           Lecture 21 \n               \n               Interpreting random effects models \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 12\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Nov 12:\n           \n           Lecture 22 \n               \n               Bias-variance tradeoff in prediction \n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Nov 14:\n           \n           Lecture 23 \n               \n               Variable selection and the F-test \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 13\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Nov 19:\n           \n           Lecture 24 \n               \n               Ridge or L2 regression \n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Nov 21:\n           \n           Lecture 25 \n               \n               LASSO or L1 regression \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 14\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Nov 26:\n           \n           Lecture 26 \n               \n               Project consultation \n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Nov 27:\n           \n           Holiday  \n               \n               Thanksgiving \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n   Week 15\n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Dec 3:\n           \n           Lecture 27 \n               \n               Project consultation \n           \n           \n               \n          \n               \n           \n        \n   \n\n        \n\n           \n           \n           Dec 5:\n           \n           Lecture 28 \n               \n               TBD \n           \n           \n               \n          \n               \n           \n        \n   \n   \n   \n  \n\n\nNo matching items",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "lectures/lectures.html",
    "href": "lectures/lectures.html",
    "title": "Lectures and Labs",
    "section": "",
    "text": "Linear algebra review materials\n\n\n\nLecture 1: Aug 29th:\n\nSample averages for prediction and inference\n\n\n\n\nLecture 2: Sep 3rd:\n\nSimple linear regression as loss minimization\n\nLecture 3: Sep 5th:\n\nThe law of large numbers and the central limit theorem",
    "crumbs": [
      "Lectures"
    ]
  },
  {
    "objectID": "old_assignments/final_project.html",
    "href": "old_assignments/final_project.html",
    "title": "STAT151A Final Project",
    "section": "",
    "text": "\\(\\,\\)\nBelow are the details of your final project. As usual, these may be subject to change with notice."
  },
  {
    "objectID": "old_assignments/final_project.html#alternative-projects",
    "href": "old_assignments/final_project.html#alternative-projects",
    "title": "STAT151A Final Project",
    "section": "Alternative projects",
    "text": "Alternative projects\nThoughtful deviations from this template are welcome. Some possible examples are:\n\nStudying an advanced regression topic (e.g. double descent, regression trees, Bayesian methods) using real or simulated data\nReproducing an existing study, then changing some of the methods to investigate the stability of the results\nStudy the numerical stability of R’s lm with a detailed analysis of its core linear algebra routines\n\nAlternative proposals should still meaningfully engage with the content of the course. For example, a project that simply fits deep neural networks to some data is inappropriate, but a project that meaningfully compares a deep neural network to regression techniques on the same dataset could be appropriate.\nIf you want to deviate from the template, please describe your proposal in detail on the project form."
  },
  {
    "objectID": "old_assignments/final_project.html#more-detailed-guidelines",
    "href": "old_assignments/final_project.html#more-detailed-guidelines",
    "title": "STAT151A Final Project",
    "section": "More detailed guidelines",
    "text": "More detailed guidelines\nHere are some more guidelines for the project proposals, as well as what we will look for in a good project.\n\nDatasets\nDatasets should be openly available. Furthermore, they should come with detailed and clear descriptions of how the data were collected. Ideally, the information about data collection is in the form of a published paper or study.\n\nGood example: The bodyfat dataset\nBad example: The marketing dataset in An Introduction to Statistical Learning (no real information about how it was collected)\nBad example: Proprietary data from your aunt’s internet startup (not open access)\n\nI hope that good datasets may become part of future versions of this course.\nHere are some potential places to look for datasets:\n\nKaggle\nUCI ML repository\nOpenintro\nFox regression book\nIEEE Dataport (unfortunately, many cost money)\nROS textbook\n\nPlease feel free to share other suggestions with me and with the rest of the class!\n\n\nQuestions\nQuestions should be about the real world, not framed directly in terms of statistical analyses.\n\nGood example: Can we increase household income by giving cash to poor families?\nGood example: Can we produce a useful predictor of bodyfat from simpler measurements?\nBad example: Is there an association between \\(x_n\\) and \\(y_n\\) in this dataset?\nBad example: Is coefficient \\(\\beta_1\\) in such-and-such a regression statistically significant?\n\nPlease be clear about whether your problem is an inference or a prediction problem (or both, or neither).\n\n\nAttempted answers\nIn order to attempt to answer your question with linear regression, you have to connect your real-world question with a regression analysis. Please be very clear about\n\nWhat assumptions you need to connect your regression to your question\nWhether those assumptions are reasonable\n\nClear thinking will be more important here than definitive answers to your question.\n\n\nCritical analysis\nFinally, please connect the results of your analysis to your question. Here are some of the kinds of questions you might ask:\n\nWhat is the answer to your question?\nWere you not able to answer your question due to some limitation you found in the data?\nHow might you collect different data to successfully answer your question?\nWhat different statitsical analyses might answer the question better?\nWhat evidence is there that your assumptions are satisfied?\nWhat evidence could you imagine collecting to establish that your assumptions were satisfied?\n\nAgain, clear thinking will be more important here than definitive answers to your question."
  },
  {
    "objectID": "old_assignments/hw1_math.html",
    "href": "old_assignments/hw1_math.html",
    "title": "STAT151A Homework 1: Due Jan 26th",
    "section": "",
    "text": "1 Simple regression in matrix form\nConsider the simple linear model \\(y_n = \\beta_0 + \\beta_1 z_n + \\varepsilon_n\\).\nLet \\(\\bar{y}:= \\frac{1}{N} \\sum_{n=1}^Ny_n\\) and \\(\\bar{z}:= \\frac{1}{N} \\sum_{n=1}^Nz_n\\). Recall that the ordinary least squares estimates are given by \\[\n\\begin{aligned}\n    \\hat{\\beta}_1 = \\frac{\\frac{1}{N} \\sum_{n=1}^N(z_n - \\bar{z}) (y- \\bar{y})}{\\frac{1}{N} \\sum_{n=1}^N(z_n - \\bar{z})^2}\n    \\quad\\textrm{and}\\quad\n    \\hat{\\beta}_0 = \\bar{y}- \\hat{\\beta}_1 \\bar{z}.\n\\end{aligned}\n\\]\n\n(a)\nWrite the set of equations\n\\[\ny_n = \\beta_0 + \\beta_1 z_n + \\varepsilon_n\n\\]\nfor \\(n \\in \\{1, \\ldots, N\\}\\) in matrix form. That is, let \\(\\boldsymbol{X}\\) denote an \\(N \\times\n2\\) matrix, \\(\\boldsymbol{Y}\\) and \\(\\boldsymbol{\\varepsilon}\\) denote \\(N \\times 1\\) matrices, \\(\\boldsymbol{b}= (\\beta_0, \\beta_1)^\\intercal\\), and express the matrices \\(\\boldsymbol{Y}\\), \\(\\boldsymbol{X}\\), and \\(\\boldsymbol{\\varepsilon}\\) in terms of the scalars \\(y_n\\), \\(z_m\\), and \\(\\varepsilon_n\\) so that \\(\\boldsymbol{Y}= \\boldsymbol{X}\\boldsymbol{b}+ \\boldsymbol{\\varepsilon}\\) is equivalent to the set of regression equations.\n\n\n(b)\nDefine \\[\n\\begin{aligned}\n    \\overline{zz} := \\frac{1}{N} \\sum_{n=1}^Nz_n^2\n    \\quad\\textrm{and}\\quad\n    \\overline{zy} := \\frac{1}{N} \\sum_{n=1}^Nz_n y_n\n\\end{aligned}\n\\]\nWrite an explict expressions for \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\), \\(\\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\), and \\(\\left(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1}\\), all in terms of \\(\\bar{y}\\), \\(\\bar{z}\\), \\(\\overline{zz}\\), \\(\\overline{zy}\\), and \\(N\\). Verify that the inverse is correct by direct multiplication.\n\n\n(c)\nCompute \\((\\boldsymbol{X}^\\intercal\\boldsymbol{X})^{-1} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\). Show that the first row is equal to \\(\\hat{\\beta}_0\\) and the second row is equal to \\(\\hat{\\beta}_1\\) as given by the ordinary least squares formula in the problem statement above.\n\n\n\n2 Mean zero residuals.\nConsider the model \\(y_n = \\beta z_n + \\varepsilon_n\\). Let \\(\\hat{\\beta}\\) denote the least squares estimator and \\(\\hat{\\varepsilon}_n = y_n - \\hat{\\beta}z_n\\).\n\n(a)\nSuppose \\(z_n\\) is not a constant. Is it necessarily the case that \\(\\frac{1}{N} \\sum_{n=1}^N\\hat{\\varepsilon}_n = 0\\)? Prove your answer.\n\n\n(b)\nSuppose \\(z_n\\) is a constant, but \\(z_n \\equiv 5\\) for every \\(n \\in \\{1, \\ldots, N\\}\\). Is it necessarily the case that \\(\\frac{1}{N} \\sum_{n=1}^N\\hat{\\varepsilon}_n = 0\\)? Prove your answer.\n\n\n(c)\nNow the model \\(y_n = \\beta_1 z_{n1} + \\beta_2 z_{n2} + \\varepsilon_n\\). Suppose that \\(z_{n1} = 1\\) is \\(n\\) is even, and is \\(0\\) otherwise. Similarly, suppose that \\(z_{n2} = 1\\) is \\(n\\) is odd, and is \\(0\\) otherwise. Let \\(N\\) be even. Is it necessarily the case that \\(\\frac{1}{N} \\sum_{n=1}^N\\hat{\\varepsilon}_n = 0\\)? Prove your answer.\n\n\n\n3 Inner products and covariances\nLet \\(\\boldsymbol{z}= (z_1, \\ldots, z_N)\\) and \\(\\boldsymbol{y}= (y_1, \\ldots, y_N)\\). Let \\(\\boldsymbol{X}\\) denote an \\(N \\times P\\) matrix whose \\(n\\)–th row is the transpose of the \\(P\\)-vector \\(\\boldsymbol{x}_n^\\intercal\\).\n(Note: this question involves limits of random variables, and there are many distinct ways that random variables can converge to limits. If you’re familiar with these different modes of probabilisitic convergence, feel free to state what mode of convergence applies, but if you are not, don’t worry — modes of convergence will not matter much for this class, and you can state your result heuristically.)\nFor a set of quantities (numbers, vectors, pairs of vectors, etc), the “empirical distribution” over that set refers to drawing an element with replacement from the set with equal probability given to each entry. For example, if \\(\\mathcal{Z}'\\) is a drawn from the empirical distribution over the set \\(\\{z_1, \\ldots, z_N \\}\\), then \\(\\mathbb{P}\\left(\\mathcal{Z}' = z_n\\right) = 1/N\\) for each \\(n\\). Similarly, if \\((\\mathcal{Z}', \\mathcal{Y}')\\) is drawn from the empirical distribution over the pairs \\(\\{(z_1, y_1), \\ldots, (z_N, y_N)\\}\\), then \\(\\mathbb{P}\\left((\\mathcal{Z}', \\mathcal{Y}') = (z_n, y_n)\\right) = 1/N\\) for all \\(n\\).\n(Hint: it may help to recall that the bootstrap uses draws from the empirical distribution, and that, in the empirical distribution, the elements of the set are fixed and not random.)\n\n(a)\nLet \\((\\mathcal{Z}', \\mathcal{Y}')\\) denote a draw from the empirical distribution over the set \\(\\{(y_1, z_1), \\ldots, (y_N, z_N)\\}\\).\nProve that \\(\\frac{1}{N} \\boldsymbol{z}^\\intercal\\boldsymbol{y}= \\mathbb{E}\\left[\\mathcal{Z}' \\mathcal{Y}'\\right]\\). Then prove that \\(\\frac{1}{N} \\boldsymbol{1}^\\intercal\\boldsymbol{z}= \\mathbb{E}\\left[\\mathcal{Z}'\\right]\\) as a special case.\n\n\n(b)\nNow suppose that the entries of \\(\\boldsymbol{z}\\) are independent and identically distributed (IID) realizations of the random variable \\(\\mathcal{Z}\\), and that the entries of \\(\\boldsymbol{y}\\) are similarly IID realizations of a random variable \\(\\mathcal{Y}\\). Assuming that \\(\\mathbb{E}\\left[|\\mathcal{Z}|\\right] &lt; \\infty\\) and \\(\\mathbb{E}\\left[|\\mathcal{Y}|\\right] &lt; \\infty\\), prove that\n\\[\n\\frac{1}{N} \\boldsymbol{z}^\\intercal\\boldsymbol{y}\\rightarrow\n    \\mathbb{E}\\left[\\mathcal{Z} \\mathcal{Y}\\right]\n    \\textrm{ as }N \\rightarrow \\infty\n\\]\n(Hint: don’t prove this from scratch, appeal to a probability theorem.)\n\n\n(c)\nUsing only inner products involving \\(\\boldsymbol{y}\\), \\(\\boldsymbol{z}\\), and \\(\\boldsymbol{1}\\), write an expression for \\(\\mathrm{Cov}\\left(\\mathcal{Y}', \\mathcal{Z}'\\right)\\). Prove that the expression converges with probability one to \\(\\mathrm{Cov}\\left(\\mathcal{Y}, \\mathcal{Z}\\right)\\). (Hint: again, use your previous results and a theorem from probability.)\n\n\n(d)\nNow, let \\((\\mathcal{X}', \\mathcal{Y}')\\) denote a draw from the empirical distribution over \\(\\{(x_1, y_1), \\ldots, (x_N, y_N) \\}\\). (Recall that the vector \\(x_n\\) is a length–\\(P\\) column vector, and \\(x_n^\\intercal\\) is the \\(n\\)–th row of the matrix \\(\\boldsymbol{X}\\).)\n\\[\n\\begin{aligned}\n\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}= \\mathbb{E}\\left[\\mathcal{X}' \\mathcal{X}'^\\intercal\\right]\n\\quad\\textrm{and}\\quad\n\\frac{1}{N} \\boldsymbol{X}^\\intercal y= \\mathbb{E}\\left[\\mathcal{X}' \\mathcal{Y}'\\right].\n\\end{aligned}\n\\]\n\n\n(e)\nNow, suppose that rows of \\(\\boldsymbol{X}\\) are IID realizations of the random \\(P\\)–vector \\(\\mathcal{X}\\), and that \\(\\mathbb{E}\\left[|\\mathcal{X}_p|\\right] &lt; \\infty\\) for each \\(p \\in \\{ 1, \\ldots, P \\}\\). Assume, as above, that \\(\\mathbb{E}\\left[|\\mathcal{Y}|\\right] &lt; \\infty\\).\nProve that, as \\(N \\rightarrow \\infty\\),\n\\[\n\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\rightarrow\n    \\mathbb{E}\\left[\\mathcal{X} \\mathcal{X}^\\intercal\\right]\n\\quad\\textrm{and}\\quad\n\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\rightarrow\n    \\mathbb{E}\\left[\\mathcal{X} \\mathcal{Y}\\right],\n\\]\nwhere both limits are with probability one.\n\n\n(f)\nNow assume that, for each \\(p \\in \\{1, \\ldots, P\\}\\) and \\(q \\in \\{1, \\ldots, P\\}\\), \\(\\mathbb{E}\\left[\\left|\\mathcal{X}'_p\\right| \\left|\\mathcal{X}'_q\\right| \\mathcal{Y}^2\\right] &lt; \\infty\\). Prove that, as \\(N \\rightarrow \\infty\\),\n\\[\n\\frac{1}{\\sqrt{N}}\n\\left( \\boldsymbol{X}^\\intercal\\boldsymbol{Y}- \\mathbb{E}\\left[\\boldsymbol{X}^\\intercal\\boldsymbol{Y}\\right] \\right) \\rightarrow \\mathcal{Z},\n\\]\nwhere \\(\\mathcal{Z}\\) is a multivariate normal random variable. What is the covariance of \\(\\mathcal{Z}\\)? (Hint: again, appeal to a probability theorem.)"
  },
  {
    "objectID": "old_assignments/hw2_math.html",
    "href": "old_assignments/hw2_math.html",
    "title": "STAT151A Homework 2: Due February 9th",
    "section": "",
    "text": "1 Transformation of variables\nConsider two different regressions, \\(\\boldsymbol{Y}\\sim \\boldsymbol{X}\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{Y}\\sim \\boldsymbol{Z}\\boldsymbol{\\alpha}\\), with the same \\(\\boldsymbol{Y}\\), where \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\) are both \\(N \\times P\\) and are both full-rank. Let the \\(n\\)–th row of \\(\\boldsymbol{X}\\) be written \\(\\boldsymbol{x}_n^\\intercal\\), and the \\(n\\)–th row of \\(\\boldsymbol{Z}\\) be \\(\\boldsymbol{z}_n^\\intercal\\).\n\n(a)\nSuppose \\(\\boldsymbol{x}_n = \\boldsymbol{A}\\boldsymbol{z}_n\\) for an invertible \\(\\boldsymbol{A}\\) and for all \\(n = 1,\\ldots,N\\). Find an expression for \\(\\hat{\\alpha}\\) in terms of \\(\\hat{\\beta}\\) that does not explicitly use \\(\\boldsymbol{Y}\\), \\(\\boldsymbol{X}\\), or \\(\\boldsymbol{Z}\\).\n\n\n(b)\nSuppose that, for all \\(n=1,\\ldots,N\\), \\(\\boldsymbol{x}_n = f(\\boldsymbol{z}_n)\\) for some invertible but non-linear function \\(f(\\cdot)\\). In general, can you find an expression for \\(\\hat{\\alpha}\\) in terms of \\(\\hat{\\beta}\\) that does not explicitly use \\(\\boldsymbol{Y}\\), \\(\\boldsymbol{X}\\), or \\(\\boldsymbol{Z}\\)? Prove why or why not. (To prove that you cannot, finding a single counterexample is enough.)\n\n\n(c)\nNow consider only the regression \\(\\boldsymbol{Y}\\sim \\boldsymbol{X}\\boldsymbol{\\beta}\\), but suppose we are not interested in \\(\\boldsymbol{\\beta}\\), but rather some other \\(\\boldsymbol{\\gamma}= \\phi(\\boldsymbol{\\beta})\\), where \\(\\phi\\) is an invertible function. Prove that the least squares estimator of \\(\\boldsymbol{\\gamma}\\) is given by \\(\\hat{\\boldsymbol{\\gamma}}= \\phi(\\hat{\\boldsymbol{\\beta}})\\).\n\n\n(d)\nProve that result (a) is special case of the result (c). (Hint: find the corresponding \\(\\phi\\).)\n\n\n\n2 Spaces of possible estimators.\nConsider the simple linear model \\(y_n = \\beta_0 + \\beta_1 z_n + \\varepsilon_n\\). Assume that \\(\\frac{1}{N} \\sum_{n=1}^Nz_n \\ne 0\\).\n\n(a)\nFix \\(\\beta_0 = \\frac{1}{N} \\sum_{n=1}^Ny_n\\) and find a value of \\(\\beta_1\\) such that \\(\\frac{1}{N} \\sum_{n=1}^N\\varepsilon_n = 0\\). How does your answer depend on whether or not \\(\\frac{1}{N} \\sum_{n=1}^Nz_n = 0\\)?\n\n\n(b)\nFix \\(\\beta_1 = 10,000,000\\) and find a value of \\(\\beta_0\\) such that \\(\\frac{1}{N} \\sum_{n=1}^N\\varepsilon_n = 0\\).\n\n\n(c)\nIn general, how many different choices of \\(\\beta_0\\) and \\(\\beta_1\\) can you find that satisfy \\(\\frac{1}{N} \\sum_{n=1}^N\\varepsilon_n = 0\\)? Are all of them reasonable? Are any of them reasonable?\n\n\n(d)\nFind an \\(N\\)–dimensional vector \\(\\boldsymbol{v}\\) such that \\[\n\\frac{1}{N} \\sum_{n=1}^N\\varepsilon_n = 0 \\quad\\Leftrightarrow\\quad \\boldsymbol{v}^\\intercal\\boldsymbol{\\varepsilon}= \\boldsymbol{0}.\n\\]\n\n\n(e)\nSuppose I give you a general \\(N\\)–dimensional vector \\(\\boldsymbol{v}\\) and a scalar \\(a\\). How many different choices of \\(\\beta_0\\) and \\(\\beta_1\\) can you find such that \\(\\boldsymbol{v}^\\intercal\\boldsymbol{\\varepsilon}= a\\)?\n\n\n(f) (Optional — this will not be graded)\nSuppose I give you two different vectors, \\(\\boldsymbol{v}_1\\) and \\(\\boldsymbol{v}_2\\). Under what circumstances can you find \\(\\beta_0\\) and \\(\\beta_1\\) such that\n\\[\n\\begin{aligned}\n\\boldsymbol{v}_1^\\intercal\\boldsymbol{\\varepsilon}= \\boldsymbol{0}\n\\quad\\textrm{and}\\quad\n\\boldsymbol{v}_2^\\intercal\\boldsymbol{\\varepsilon}= \\boldsymbol{0}?\n\\end{aligned}\n\\]\nWhen are there infinitely many solutions? When is there only one solution? (Hint: what if \\(\\boldsymbol{v}_1^\\intercal\\boldsymbol{1}= \\boldsymbol{v}_2^\\intercal\\boldsymbol{1}= 0\\)?)\n\n\n(g)\nNow, consider the general linear model \\(\\boldsymbol{Y}= \\boldsymbol{X}\\boldsymbol{\\beta}+ \\varepsilon\\). Prove that there always exists \\(\\boldsymbol{\\beta}\\) and \\(\\varepsilon\\) so that the \\(\\boldsymbol{Y}= \\boldsymbol{X}\\boldsymbol{\\beta}+ \\varepsilon\\).\n\n\n(h) (Optional — this will not be graded)\nSuppose, for the general linear model, that the matrix \\(\\boldsymbol{X}\\) is full-rank (that is, of rank \\(P\\), where \\(P\\) is the number of columns of \\(\\boldsymbol{X}\\)). Suppose I give you a \\(N \\times D\\) matrix \\(\\boldsymbol{V}\\), and ask you to find \\(\\boldsymbol{\\beta}\\) such that \\(\\boldsymbol{V}^\\intercal\\boldsymbol{\\varepsilon}= \\boldsymbol{0}\\). Under what circumstances are there no solutions? A single solution? An infinite set of solutions? (Hint: you already answered this question for \\(P = 2\\), now you just need to state the result in matrix form.)\n\n\n\n3 Collinear regressors\nSuppose that \\(\\boldsymbol{X}\\) does not have full column rank — that is, \\(\\boldsymbol{X}\\) is \\(N \\times P\\) but has column rank \\(Q &lt; P\\).\n\n(a)\nHow many solutions \\(\\hat{\\beta}\\) are there to the least-squares problem \\[\n\\hat{\\beta}:= \\underset{\\beta}{\\mathrm{argmin}}\\, \\left\\Vert\\boldsymbol{Y}- \\boldsymbol{X}\\beta\\right\\Vert_2^2?\n\\]\n\n\n(b)\nRelate the solutions \\(\\hat{\\beta}\\) from part (a) to spaces spanned by eigenvectors of \\(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\). Among the solutions, identify the one with the smallest norm, \\(\\left\\Vert\\hat{\\beta}\\right\\Vert_2^2\\).\n\n\n(c)\nSuppose that \\(\\boldsymbol{X}'\\) is a full column-rank \\(N \\times Q\\) matrix with the same column span as \\(\\boldsymbol{X}\\), and let \\(\\hat{\\gamma}\\) be the OLS estimator for the regression \\(\\boldsymbol{Y}\\sim \\boldsymbol{X}'\\gamma\\). Compare the fits \\(\\hat{\\boldsymbol{Y}}= \\boldsymbol{X}\\hat{\\beta}\\) and \\(\\hat{\\boldsymbol{Y}}' = \\boldsymbol{X}' \\hat{\\gamma}\\), and compare the sum of squared residuals for the two regressions."
  },
  {
    "objectID": "old_assignments/hw3_math.html",
    "href": "old_assignments/hw3_math.html",
    "title": "STAT151A Homework 3: Due February 23rd",
    "section": "",
    "text": "\\(\\,\\)\n\n1 Normal intervals\nFor these problems, assume I give you a computer program that can compute the function \\(\\Phi(z) = \\mathbb{P}\\left(\\tilde{z} \\le z\\right)\\) where \\(\\tilde{z}\\) is a standard scalar-valued random variable.\nLet \\(\\tilde{x}\\) denote a scalar-valued \\(N(\\mu, \\sigma^2)\\) random variable. Using only \\(\\Phi(z)\\) and elementary arithmetic, construct functions that evaluate the following:\n\n(a)\n\\(a \\mapsto \\mathbb{P}\\left(\\tilde{x} \\le a\\right)\\)\n\n\n(b)\n\\(b \\mapsto \\mathbb{P}\\left(\\tilde{x} \\ge b\\right)\\)\n\n\n(c)\n\\(a, b \\mapsto \\mathbb{P}\\left(b \\le \\tilde{x} \\le a\\right)\\)\n\n\n(d)\n\\(a \\mapsto \\mathbb{P}\\left(\\left|\\tilde{x}\\right| \\le a\\right)\\)\n\n\n(e)\n\\(a \\mapsto \\mathbb{P}\\left(\\left|\\tilde{x}\\right| \\ge a\\right)\\)\n\n\n(f)\n\\(a \\mapsto \\mathbb{P}\\left(\\left|\\tilde{x}\\right| &gt; a\\right)\\)\n\n\n(g)\n\\(a \\mapsto \\mathbb{P}\\left(\\left|\\tilde{x}\\right| = a\\right)\\)\n\n\n\n2 Multivariate CLT\nLet \\(\\tilde{\\boldsymbol{x}}_n\\) denote an IID sequence of random variables in \\(\\mathbb{R}^{P}\\) (not necessarily normal), each with zero mean and finite covariance matrix \\(\\boldsymbol{\\Sigma}\\). Let \\(\\boldsymbol{a}\\in \\mathbb{R}^{P}\\) denote a fixed vector.\n\n(a)\nUsing the univariate CLT, find the limiting distribution of\n\\[\n\\frac{1}{\\sqrt{N}} \\sum_{n=1}^N\\boldsymbol{a}^\\intercal\\tilde{\\boldsymbol{x}_n}.\n\\]\n\n\n(b)\nUsing the multivariate CLT and the continuous mapping theorem, find the limiting distribution of\n\\[\n\\boldsymbol{a}^\\intercal\\left( \\frac{1}{\\sqrt{N}} \\sum_{n=1}^N\\tilde{\\boldsymbol{x}_n}\\right).\n\\]\n\n\n(c)\nNow, suppose that \\(P = 2\\) and\n\\[\\boldsymbol{\\Sigma}=\n\\begin{pmatrix}\n1 & -1 \\\\\n-1 & 1\n\\end{pmatrix}.\n\\]\nNote that we can write \\[\n\\tilde{\\boldsymbol{x}}_n =\n\\begin{pmatrix}\n\\tilde{\\boldsymbol{x}}_{n1} \\\\\n\\tilde{\\boldsymbol{x}}_{n2}\n\\end{pmatrix},\n\\] where \\(\\tilde{\\boldsymbol{x}}_{n1}\\) and \\(\\tilde{\\boldsymbol{x}}_{n2}\\) are scalars. Find the limiting distributions of each of the following expressions:\n\\[\n\\begin{aligned}\n\\frac{1}{\\sqrt{N}} \\sum_{n=1}^N\\tilde{\\boldsymbol{x}}_{n1} \\rightarrow& \\textrm{?}\\\\\n\\frac{1}{\\sqrt{N}} \\sum_{n=1}^N\\tilde{\\boldsymbol{x}}_{n2} \\rightarrow& \\textrm{?} \\\\\n\\frac{1}{\\sqrt{N}} \\sum_{n=1}^N(\\tilde{\\boldsymbol{x}}_{n1} + \\tilde{\\boldsymbol{x}}_{n2}) \\rightarrow& \\textrm{?}\n\\end{aligned}\n\\]\n(This result demonstrates why it’s not enough to only look at the marginal distribution of the vector components when using a multivariate CLT.)\n\n\n\n3 Valid covariance matrices\nSuppose I were to tell you that the vector-valued random variable \\(\\boldsymbol{x}\\) has a covariance matrix \\(\\mathrm{Cov}\\left(\\boldsymbol{x}\\right) = \\boldsymbol{\\Sigma}\\) where \\(\\boldsymbol{\\Sigma}\\) is not positive semi-definite (i.e., \\(\\boldsymbol{\\Sigma}\\) has at least one negative eigenvalue). Show that, if this were true, you could construct a scalar-valued random variable with negative variance, which is impossible.\n(It follows from this argument every covariance matrix must be postive semi-definite.)"
  },
  {
    "objectID": "old_assignments/hw4_math.html",
    "href": "old_assignments/hw4_math.html",
    "title": "STAT151A Homework 4: Due March 8th",
    "section": "",
    "text": "\\(\\,\\)\n\n1 Chi squared random variables\nLet \\(s\\sim \\mathcal{\\chi}^2_{K}\\). Prove that\n\n\\(\\mathbb{E}\\left[s\\right] = K\\)\n\\(\\mathrm{Var}\\left(s\\right) = 2 K\\) (hint: if \\(z\\sim \\mathcal{N}\\left(0,\\sigma^2\\right)\\), then \\(\\mathbb{E}\\left[z^4\\right] = 3\\sigma^4\\))\nIf \\(a_n \\sim \\mathcal{N}\\left(0,\\sigma^2\\right)\\) IID for \\(1,\\ldots,N\\), then \\(\\frac{1}{\\sigma^2} \\sum_{n=1}^Na_n^2 \\sim \\mathcal{\\chi}^2_{N}\\)\n\\(\\frac{1}{K} s\\rightarrow 1\\) as \\(K \\rightarrow \\infty\\)\n\\(\\frac{1}{\\sqrt{K}} (s- K) \\rightarrow \\mathcal{N}\\left(0, 2\\right)\\) as \\(K \\rightarrow \\infty\\)\nLet \\(\\boldsymbol{a}\\sim \\mathcal{N}\\left(\\boldsymbol{0}, \\boldsymbol{I}\\right)\\) where \\(a\\in \\mathbb{R}^{K}\\). Then \\(\\left\\Vert\\boldsymbol{a}\\right\\Vert_2^2 \\sim \\mathcal{\\chi}^2_{K}\\)\nLet \\(\\boldsymbol{a}\\sim \\mathcal{N}\\left(\\boldsymbol{0}, \\boldsymbol{\\Sigma}\\right)\\) where \\(a\\in \\mathbb{R}^{K}\\). Then \\(\\boldsymbol{a}^\\intercal\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{a}\\sim \\mathcal{\\chi}^2_{K}\\)\n\n\n\n2 Predictive variance for different regressors\nThis question will take the residuals of the training data to be random, and will consider variablity under sampling of the training data. The regressors for both the training data and test data will be taken as fixed.\nLet \\(\\boldsymbol{x}_n  = (x_{n1}, x_{n2})^\\intercal\\) be IID normal regressors, with\n\n\\(\\mathbb{E}\\left[x_{n1}\\right] = \\mathbb{E}\\left[x_{n2}\\right] = 0\\),\n\\(\\mathrm{Var}\\left(x_{n1}\\right) = \\mathrm{Var}\\left(x_{n2}\\right) = 1\\), and\n\\(\\mathrm{Cov}\\left(x_{n1}, x_{n2}\\right) = 0.99\\).\n\n(Note there is no intercept.)\nAssume that \\(y_n = \\beta^\\intercal\\boldsymbol{x}_n + \\varepsilon_n\\) for some \\(\\beta\\), and that the residuals \\(\\varepsilon_n\\) are IID with mean \\(0\\), variance \\(\\sigma^2 = 2\\), and are independent of \\(\\boldsymbol{x}_n\\).\n\n(a)\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\).\n\n\n(b)\nDefine the expected prediction error \\[\n\\hat{y}_\\mathrm{new}- \\mathbb{E}\\left[y_\\mathrm{new}\\right] :=  (\\hat{\\beta}- \\beta)^\\intercal x_\\mathrm{new},\n\\]\nand approximate the limiting variance \\(\\mathrm{Var}\\left(\\hat{y}_\\mathrm{new}- \\mathbb{E}\\left[y_\\mathrm{new}\\right]\\right)\\) for the following new regression vectors:\n\n\\(x_\\mathrm{new}= (1, 1)^\\intercal\\)\n\\(x_\\mathrm{new}= (1, -1)^\\intercal\\)\n\\(x_\\mathrm{new}= (100, 100)^\\intercal\\)\n\\(x_\\mathrm{new}= (0, 0)^\\intercal\\)\n\nYou may assume that \\(N\\) is large, so that you can apply the CLT to \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\). Even with the CLT approximation your answer will depend on \\(N\\); just make this dependence explicit.\n\n\n(c)\nWhy are some variances in (b) large and some small? Explain each in plain language and intuitive terms.\n\n\n\n3 The sandwich covariance matrix under homoeskedasticity\nFor this problem, make the following assumptions.\n\nThe regressors are non-random, with \\(\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\rightarrow \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\) for positive definite \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\)\nThe responses are \\(y_n = \\boldsymbol{\\beta}^\\intercal\\boldsymbol{x}_n + \\varepsilon_n\\) for some unknown \\(\\boldsymbol{\\beta}\\)\nThe residuals are IID with \\(\\mathbb{E}\\left[\\varepsilon_n\\right] = 0\\) and \\(\\mathrm{Var}\\left(\\varepsilon_n\\right) = \\sigma^2\\) (but not necessarily normal)\n\nUnder these assumptions, show that the sandwich covariance matrix and the standard covariance matrix converge to the same quantity. That is, show that\n\\[\n\\hat\\Sigma_{sand}  =\nN \\left(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1}\n  \\left(\\sum_{n=1}^Nx_n x_n^\\intercal\\hat{\\varepsilon}_n^2\\right)\n  \\left(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1} \\rightarrow \\boldsymbol{S}\n  \\quad\\textrm{and}\\quad\n\\hat\\Sigma_{h}  =\nN \\left(\\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1} \\hat{\\sigma}^2 \\rightarrow \\boldsymbol{S}\n\\]\nfor the same \\(\\boldsymbol{S}\\), where \\(\\hat{\\sigma}^2 := \\frac{1}{N} \\sum_{n=1}^N\\hat{\\varepsilon}_n^2\\)."
  },
  {
    "objectID": "old_assignments/hw5_math_asymptotics_solutions.html",
    "href": "old_assignments/hw5_math_asymptotics_solutions.html",
    "title": "STAT151A Homework 5 Asymptotics Solutions",
    "section": "",
    "text": "\\(\\,\\)\n\n1 Reviewing the distribution of \\(\\hat{\\beta}\\) under different assumptions\nThis homework question will reference the following assumptions.\nRegressor assumptions:\n\nR1: The \\(N \\times P\\) matrix \\(\\boldsymbol{X}\\), which has \\(\\boldsymbol{x}_n^\\intercal\\) in the \\(n\\)–th row, is full column rank\nR2: The regressors \\(\\boldsymbol{x}_n\\) are deterministic, and \\(\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\rightarrow \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\), where \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\) is positive definite\nR3: The regressors \\(\\boldsymbol{x}_n\\) are IID, with positive definite covariance \\(\\mathrm{Cov}\\left(\\boldsymbol{x}_n\\right)\\), and \\(\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\rightarrow \\mathbb{E}\\left[\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\right] = \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\) in probability.\n\nModel assumptions (for all \\(n\\)):\n\nM1: There exists a \\(\\boldsymbol{\\beta}\\) such that \\(y_n = \\boldsymbol{\\beta}^\\intercal\\boldsymbol{x}_n + \\varepsilon_n\\) for all \\(n\\)\nM2: The residuals \\(\\varepsilon_n\\) are IID with \\(\\varepsilon_n \\vert \\boldsymbol{x}_n \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\) for some \\(\\sigma^2\\)\nM3: The residuals \\(\\varepsilon_n\\) are independent with \\(\\mathbb{E}\\left[\\varepsilon_n | \\boldsymbol{x}_n\\right] = 0\\) and \\(\\mathbb{E}\\left[\\varepsilon_n^2 | \\boldsymbol{x}_n\\right] = \\sigma^2\\)\nM4: The residuals \\(\\varepsilon_n\\) are independent with \\(\\mathbb{E}\\left[\\varepsilon_n | \\boldsymbol{x}_n\\right] = 0\\) and \\(\\mathbb{E}\\left[\\varepsilon_n^2 | \\boldsymbol{x}_n\\right] = \\sigma_n^2\\)\nM5: The pairs \\((\\boldsymbol{x}_n, y_n)\\) are IID\nM6: For all finite vectors \\(\\boldsymbol{v}\\), \\(\\frac{1}{N} \\sum_{n=1}^N\\mathbb{E}\\left[(y_n - \\boldsymbol{v}^\\intercal\\boldsymbol{x}_n)^2 \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\right] \\rightarrow \\boldsymbol{V}(\\boldsymbol{v}) &lt; \\infty\\), where each entry of the limiting matrix is finite. (The limit depends on \\(\\boldsymbol{v}\\), but importantly \\(\\boldsymbol{V}(\\boldsymbol{v})\\) is finite for all finite \\(\\boldsymbol{v}\\)).\n\nFor M2, M3, and M4 with \\(\\boldsymbol{x}_n\\) is deterministic, take the conditioning to mean “for that value of \\(\\boldsymbol{x}_n\\).”\nFor this homework, you may use the LLN, the CLT, the continuous mapping theorem, and properties of the multivariate normal distribution.\nThe term “limiting distribution” means the distribution that the quantity approaches as \\(N \\rightarrow \\infty\\).\nAssume R1 for all questions.\n\\[\n\\def\\opone{o_p(1)}\n\\def\\oone{o(1)}\n\\]\nIn this below solutions, as \\(N\\rightarrow \\infty\\), I will use \\(\\oone\\) to denote a term that goes to zero, and \\(\\opone\\) to denote a term that goes to zero in probability . This notation will allow me to make limiting statements one line at a time.\n\nFind the distribution of \\(\\hat{\\beta}\\) under M1, M2, and R2.\n\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}}={}& \\left(\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1} \\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}\n      & \\textrm{(definition)}\\\\\n={}& \\left(\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1}\n      \\frac{1}{N} \\boldsymbol{X}^\\intercal\\left(\\boldsymbol{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}\\right)\n      & \\textrm{(M1)}\\\\\n={}& \\boldsymbol{\\beta}+ \\left(\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1}\n      \\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{\\varepsilon}\n      & \\textrm{(cancellation)}\\\\\n={}& \\boldsymbol{\\beta}+ \\left(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}+ \\oone \\right)^{-1} \\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{\\varepsilon}\n      & \\textrm{(R2) applied to }\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\\\\n={}& \\boldsymbol{\\beta}+ \\left(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}^{-1} + \\oone \\right) \\opone  \n      & \\textrm{(M2) and non-IID LLN applied to } \\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\varepsilon_n \\\\\n\\rightarrow {}& \\boldsymbol{\\beta}.\n\\end{aligned}\n\\]\nSo \\(\\hat{\\boldsymbol{\\beta}}\\rightarrow \\boldsymbol{\\beta}\\), a constant (that is, a degenerate distribution).\n\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M2, and R2.\n\nFrom above, \\[\n\\begin{aligned}\n\\sqrt{N} \\left( \\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}\\right)\n={}& \\left(\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1} \\frac{1}{\\sqrt{N}} \\boldsymbol{X}^\\intercal\\boldsymbol{\\varepsilon}\n   & \\textrm{({previous result})} \\\\\n={}&\n   \\left(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}+ \\oone \\right)^{-1}\n   \\frac{1}{\\sqrt{N}} \\boldsymbol{X}^\\intercal\\boldsymbol{\\varepsilon}\n      & \\textrm{(R2) applied to }\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\\\\n={}&\n   \\left(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}+ \\oone \\right)^{-1}\n   \\mathcal{N}\\left(\\boldsymbol{0}, \\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\sigma^2\\right)\n      & \\textrm{(M2) and normality of } \\frac{1}{\\sqrt{N}}  \\boldsymbol{x}_n \\varepsilon_n \\\\\n={}&\n   \\left(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}+ \\opone \\right)^{-1}\n   \\mathcal{N}\\left(\\boldsymbol{0}, \\left( \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}+ \\oone \\right) \\sigma^2\\right)\n      & \\textrm{(R2) applied to }\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\\\\n\\rightarrow {}&\n   \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}^{-1}\n   \\mathcal{N}\\left(\\boldsymbol{0}, \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\sigma^2\\right)\n      &  \\\\\n\\rightarrow {}&  \n   \\mathcal{N}\\left(\\boldsymbol{0}, \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}^{-1} \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}^{-1} \\sigma^2\\right)\n      & \\textrm{Normality} \\\\\n={}&   \\mathcal{N}\\left(\\boldsymbol{0}, \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}^{-1} \\sigma^2\\right).\n\\end{aligned}\n\\]\n\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M2, and R3.\n\nSame as (2), but\n\\[\n\\begin{aligned}\n\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal=& \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}+ \\opone.  & \\textrm{(R3) and the LLN}\n\\end{aligned}\n\\]\n\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M3, and R2.\n\nSame as (2), but instead of using normality, \\[\n\\frac{1}{\\sqrt{N}}  \\boldsymbol{x}_n \\varepsilon_n = \\mathcal{N}\\left(\\boldsymbol{0}, \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\sigma^2\\right) + \\opone\n\\]\nby the non-IID CLT, using the fact that\n\\[\n\\begin{aligned}\n\\frac{1}{N} \\sum_{n=1}^N\\mathbb{E}\\left[\\boldsymbol{x}_n \\varepsilon_n \\right] ={}& \\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n  \\mathbb{E}\\left[\\varepsilon_n\\right] = \\boldsymbol{0}\n   & \\textrm{(M3)}\\\\\n\\frac{1}{N} \\sum_{n=1}^N\\mathrm{Cov}\\left(\\boldsymbol{x}_n \\varepsilon_n \\right) ={}& \\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n  \\boldsymbol{x}_n^\\intercal\\mathbb{E}\\left[\\varepsilon_n^2\\right] \\\\\n   ={}&  \\sigma^2 \\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n  \\boldsymbol{x}_n^\\intercal\n      & \\textrm{(M3)}\\\\\n   ={}&  \\sigma^2 \\left( \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}+ \\oone \\right).\n      & \\textrm{(R2)}\n\\end{aligned}\n\\]\n\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M3, and R3.\n\nCombine the modifications of (3) and (4); otherwise the same as (2).\n\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M4, M6, and R2.\n\nSame as (4), except we apply the non-IID CLT with\n\\[\n\\begin{aligned}\n\\frac{1}{N} \\sum_{n=1}^N\\mathbb{E}\\left[\\boldsymbol{x}_n \\varepsilon_n \\right] ={}& \\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n  \\mathbb{E}\\left[\\varepsilon_n \\vert \\boldsymbol{x}_n\\right] = \\boldsymbol{0}\n   & \\textrm{(M4)}\\\\\n\\frac{1}{N} \\sum_{n=1}^N\\mathrm{Cov}\\left(\\boldsymbol{x}_n \\varepsilon_n\\right)\n   ={}& \\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n  \\boldsymbol{x}_n^\\intercal\\mathbb{E}\\left[\\varepsilon_n^2 \\vert \\boldsymbol{x}_n\\right] \\\\\n   ={}& \\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n  \\boldsymbol{x}_n^\\intercal\\mathbb{E}\\left[(y_n - \\boldsymbol{\\beta}^\\intercal\\boldsymbol{x}_n)^2\\right]\n      & \\textrm{(M1)}\\\\\n   ={}&  \\boldsymbol{V}(\\boldsymbol{\\beta}) + \\oone\n      & \\textrm{(M6)}\n\\end{aligned}\n\\]\n\nFind the limiting distribution of \\(\\sqrt{N}(\\hat{\\beta}- \\beta)\\) under M1, M4, M6, and R3.\n\nSame as (6), but \\[\n\\begin{aligned}\n\\frac{1}{N} \\sum_{n=1}^N\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal=& \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}+ \\opone.  & \\textrm{(R3) and the LLN}\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n\\frac{1}{N} \\sum_{n=1}^N\\mathrm{Cov}\\left(\\boldsymbol{x}_n \\varepsilon_n\\right)\n   ={}& \\mathbb{E}\\left[\\boldsymbol{x}_n  \\boldsymbol{x}_n^\\intercal(y_n - \\boldsymbol{\\beta}^\\intercal\\boldsymbol{x}_n)^2\\right]\n      & \\textrm{(M1)}\\\\\n   ={}&  \\boldsymbol{V}(\\boldsymbol{\\beta}) + \\oone.\n      & \\textrm{(M6)}\n\\end{aligned}\n\\]\n\nUnder M5, M6, and R3, identify a \\(\\beta^*\\) such that \\(\\sqrt{N}(\\hat{\\beta}- \\beta^*)\\) converges to a nondegenerate, finite random variable, and find the limiting distribution.\n\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}}={}& \\left(\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1} \\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}\n      & \\textrm{(definition)}\\\\\n={}& \\left(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}+ \\opone \\right)^{-1} \\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}\n      & \\textrm{(M5), (R3), the LLN}\\\\\n={}& \\left(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}+ \\opone \\right)^{-1} \\left(\\mathbb{E}\\left[\\boldsymbol{x}_n y_n\\right] + \\opone\\right)\n      & \\textrm{(M5), (M6) with }\\boldsymbol{v}= \\boldsymbol{0}\\textrm{, the LLN}\\\\\n\\rightarrow {}& \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}^{-1} \\mathbb{E}\\left[\\boldsymbol{x}_n y_n\\right] =: \\boldsymbol{\\beta}^*\n\\end{aligned}\n\\]\nWe can then write\n\\[\n\\begin{aligned}\n\\sqrt{N}(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}^*) ={}&\n   \\sqrt{N} \\left(\\left(\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1} \\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}-\n      \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}^{-1} \\mathbb{E}\\left[\\boldsymbol{x}_n y_n\\right] \\right)\n\\\\={}&\n   \\left(\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1}\n   \\sqrt{N} \\left(  \\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{Y}-\n      \\left(\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right) \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}^{-1} \\mathbb{E}\\left[\\boldsymbol{x}_n y_n\\right] \\right)\n\\\\={}&\n   \\left(\\frac{1}{N} \\boldsymbol{X}^\\intercal\\boldsymbol{X}\\right)^{-1}\n   \\frac{1}{\\sqrt{N}} \\sum_{n=1}^N\\left(  \\boldsymbol{x}_n y_n -\n      \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}^{-1} \\mathbb{E}\\left[\\boldsymbol{x}_n y_n\\right] \\right)\n\\\\={}&\n   \\left(\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}+ o_p(1) \\right)^{-1}\n   \\frac{1}{\\sqrt{N}} \\sum_{n=1}^N\\left(  \\boldsymbol{x}_n y_n -\n      \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}^{-1} \\mathbb{E}\\left[\\boldsymbol{x}_n y_n\\right] \\right)\n      & \\textrm{(R3) and the LLN}\n\\end{aligned}\n\\]\nNote that\n\\[\n\\mathbb{E}\\left[\\boldsymbol{x}_n y_n -  \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}^{-1} \\mathbb{E}\\left[\\boldsymbol{x}_n y_n\\right]\\right] =\n\\mathbb{E}\\left[\\boldsymbol{x}_n y_n\\right] -  \\mathbb{E}\\left[\\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\right] \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}^{-1} \\mathbb{E}\\left[\\boldsymbol{x}_n y_n\\right] =\n\\mathbb{E}\\left[\\boldsymbol{x}_n y_n\\right] -  \\boldsymbol{\\Sigma}_{\\boldsymbol{X}}\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}^{-1} \\mathbb{E}\\left[\\boldsymbol{x}_n y_n\\right] =\n0,\n\\]\nso \\[\n\\begin{aligned}\n\\frac{1}{\\sqrt{N}} \\sum_{n=1}^N\\left(  \\boldsymbol{x}_n y_n -\n   \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\boldsymbol{\\Sigma}_{\\boldsymbol{X}}^{-1} \\mathbb{E}\\left[\\boldsymbol{x}_n y_n\\right] \\right)\n={}&\n\\frac{1}{\\sqrt{N}} \\sum_{n=1}^N\\boldsymbol{x}_n  \\left(  y_n -\n   \\boldsymbol{x}_n^\\intercal\\boldsymbol{\\beta}^* \\right)\n   \\rightarrow\n\\\\={}&\n\\mathcal{N}\\left(\\boldsymbol{0}, \\mathbb{E}\\left[ \\boldsymbol{x}_n \\boldsymbol{x}_n^\\intercal\\left(  y_n - \\boldsymbol{x}_n^\\intercal\\boldsymbol{\\beta}^* \\right)^2\\right]\\right).\n\\end{aligned}\n\\] Then rest is like (7).\n\nIn any of the above settings, what is the limiting distribution of \\((\\hat{\\beta}- \\beta)\\)? (The answer is the same no matter which setting you choose.)\n\nThey all converge to a constant."
  },
  {
    "objectID": "old_assignments/midterm_grades.html",
    "href": "old_assignments/midterm_grades.html",
    "title": "STAT151A Mid-semester grades",
    "section": "",
    "text": "SIS.User.ID\noverall_grade\noverall_grade_with_drops\nattendance\nhomework\nquizzes\noverall_score\noverall_score_with_drops\n\n\n\n\n23043880\nF\nF\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n\n\n3032753492\nF\nF\n0.3333333\n0.0000000\n0.0000000\n0.0606061\n0.0606061\n\n\n3032769469\nB\nB\n0.9166667\n0.9638889\n0.7333333\n0.8505051\n0.8872727\n\n\n3034798937\nC\nA\n0.8333333\n0.6638889\n0.8250000\n0.7679293\n0.9631313\n\n\n3035247554\nA-\nA\n0.8333333\n0.9861111\n0.8916667\n0.9154040\n0.9492424\n\n\n3035621796\nF\nC\n0.2500000\n0.4888889\n0.5500000\n0.4732323\n0.7343434\n\n\n3035661147\nB\nA+\n0.7500000\n0.9930556\n0.7500000\n0.8383838\n1.0000000\n\n\n3035700134\nA-\nA\n1.0000000\n0.9972222\n0.8000000\n0.9080808\n0.9595960\n\n\n3035738042\nA+\nA+\n1.0000000\n0.9833333\n1.0000000\n0.9939394\n1.0000000\n\n\n3035762651\nF\nA-\n0.7500000\n0.4986111\n0.5916667\n0.5866162\n0.9030303\n\n\n3035767435\nF\nF\n0.3333333\n0.3277778\n0.3500000\n0.3388889\n0.3919192\n\n\n3035771153\nB\nA\n0.8333333\n0.9902778\n0.8083333\n0.8790404\n0.9689394\n\n\n3035798440\nD\nB\n0.7500000\n0.7597222\n0.4916667\n0.6361111\n0.8335859\n\n\n3035847996\nC\nB+\n0.7500000\n0.9625000\n0.5750000\n0.7477273\n0.8909091\n\n\n3035857512\nB\nA\n0.8333333\n0.9777778\n0.8083333\n0.8744949\n0.9515152\n\n\n3035859475\nB-\nB+\n0.9166667\n0.9888889\n0.6333333\n0.8141414\n0.8982323\n\n\n3035861477\nA\nA\n0.8333333\n0.9888889\n0.9666667\n0.9505051\n0.9824242\n\n\n3035888673\nD\nC-\n0.9166667\n0.3222222\n0.7500000\n0.6247475\n0.7030303\n\n\n3035917572\nD+\nC-\n1.0000000\n0.9125000\n0.4000000\n0.6954545\n0.7151515\n\n\n3036041982\nC\nA\n0.7500000\n0.6611111\n0.8916667\n0.7820707\n0.9477273\n\n\n3036514469\nC\nC\n1.0000000\n0.9486111\n0.4583333\n0.7351010\n0.7781818\n\n\n3036546761\nA\nA\n1.0000000\n0.9736111\n0.9333333\n0.9601010\n0.9672727\n\n\n3036578780\nA\nA+\n0.9166667\n0.9902778\n1.0000000\n0.9813131\n1.0000000\n\n\n3036588816\nC\nC\n1.0000000\n0.9638889\n0.4166667\n0.7217172\n0.7611111\n\n\n3036766978\nA\nA+\n0.9166667\n1.0000000\n1.0000000\n0.9848485\n1.0000000\n\n\n3037009717\nB\nA\n1.0000000\n0.6666667\n0.9333333\n0.8484848\n0.9848485\n\n\n3037109513\nA\nA\n1.0000000\n0.9666667\n0.9000000\n0.9424242\n0.9484848\n\n\n3037126700\nA\nA\n1.0000000\n0.9944444\n0.8583333\n0.9335859\n0.9747475\n\n\n3037135189\nA\nA\n0.7500000\n0.9902778\n0.9666667\n0.9358586\n0.9840909\n\n\n3037187699\nA-\nA\n0.8333333\n0.9763889\n0.8666667\n0.9005051\n0.9545455\n\n\n3037215637\nB\nA\n1.0000000\n0.9847222\n0.7250000\n0.8694444\n0.9487374\n\n\n3037322050\nA\nA+\n0.8333333\n0.9694444\n1.0000000\n0.9585859\n0.9984848\n\n\n3037876883\nC\nC+\n0.9166667\n0.9652778\n0.5333333\n0.7601010\n0.7969697\n\n\n3037878742\nD\nB\n1.0000000\n0.6555556\n0.4666667\n0.6323232\n0.8222222\n\n\n3037976957\nA\nA+\n0.8333333\n0.9958333\n1.0000000\n0.9681818\n1.0000000\n\n\n3038177014\nB\nA\n1.0000000\n0.9694444\n0.7416667\n0.8714646\n0.9472222\n\n\n3038275879\nD+\nC+\n1.0000000\n0.7791667\n0.5000000\n0.6924242\n0.7962121\n\n\n3039214206\nC\nA\n0.9166667\n0.6486111\n0.7666667\n0.7510101\n0.9396465\n\n\n3039716682\nA\nA\n0.8333333\n0.9750000\n0.9583333\n0.9416667\n0.9810606\n\n\n3039763235\nC\nB\n0.7500000\n0.9944444\n0.5416667\n0.7441919\n0.8484848\n\n\n3039811050\nD\nA\n0.7500000\n0.6638889\n0.6333333\n0.6656566\n0.9277778\n\n\n3039812142\nB\nA\n1.0000000\n0.9958333\n0.7583333\n0.8886364\n0.9444444\n\n\n3039815119\nA+\nA+\n1.0000000\n0.9875000\n1.0000000\n0.9954545\n1.0000000\n\n\n3039815392\nA\nA\n1.0000000\n0.9847222\n0.9166667\n0.9565657\n0.9674242\n\n\n3039819500\nF\nF\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n\n\n3039821632\nA\nA+\n0.9166667\n0.9916667\n0.9833333\n0.9742424\n1.0000000\n\n\n3039823751\nA\nA+\n0.9166667\n0.9902778\n0.9833333\n0.9737374\n0.9916667\n\n\n3039823816\nA+\nA+\n1.0000000\n0.9958333\n1.0000000\n0.9984848\n1.0000000\n\n\n3039824921\nA+\nA+\n1.0000000\n0.9902778\n1.0000000\n0.9964646\n0.9992424\n\n\n3039826533\nF\nF\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n\n\n3039828548\nF\nF\n0.1666667\n0.0000000\n0.0000000\n0.0303030\n0.0303030\n\n\n3039830160\nF\nF\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n\n\n3039831525\nB+\nA\n0.5833333\n0.9750000\n0.9500000\n0.8924242\n0.9742424\n\n\n3039831863\nA-\nA\n1.0000000\n0.9722222\n0.8333333\n0.9141414\n0.9285354\n\n\n3039832955\nB-\nA+\n0.5833333\n0.9958333\n0.7500000\n0.8090909\n1.0000000\n\n\n3039849972\nF\nF\n0.3333333\n0.9833333\n0.1333333\n0.4787879\n0.4848485"
  },
  {
    "objectID": "quizzes/e5a0c54b04175cf9f62eabb13dc9aa58fdef9cef982253a8637b4db17f80cb66.html",
    "href": "quizzes/e5a0c54b04175cf9f62eabb13dc9aa58fdef9cef982253a8637b4db17f80cb66.html",
    "title": "STAT151A Final Exam (Spring 2024)",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[0.1in]\\]\n\nYou have 3 hours for this exam.\nYou will be allowed one double-sided cheat sheet.\nPlease turn in your cheat sheet with the exam.\n\nIn the exam, you will find seven questions. From these, please choose exactly four to answer. You will be graded only on the four questions you choose to answer.\nPlease mark the questions you would like graded with an \\(\\times\\) in the box provided. For example, to select problems 2, 4, 5, and 7, your exam should look like this:\nQuestion 1 \\(\\hspace{4em}\\bigcirc \\leftarrow\\) ‘X’ here to grade this question.\nQuestion 2 \\(\\hspace{4em}\\otimes \\leftarrow\\) ‘X’ here to grade this question.\nQuestion 3 \\(\\hspace{4em}\\bigcirc \\leftarrow\\) ‘X’ here to grade this question.\nQuestion 4 \\(\\hspace{4em}\\otimes \\leftarrow\\) ‘X’ here to grade this question.\nQuestion 5 \\(\\hspace{4em}\\otimes \\leftarrow\\) ‘X’ here to grade this question.\nQuestion 6 \\(\\hspace{4em}\\bigcirc \\leftarrow\\) ‘X’ here to grade this question.\nQuestion 7 \\(\\hspace{4em}\\otimes \\leftarrow\\) ‘X’ here to grade this question.\nIf you select more than four questions, we reserve the right to choose which ones we grade.\nNote that each of the seven questions has three parts. Read the question completely and carefully before answering. Make sure to answer every part to receive full credit.\n\nIn this exam, you may use the following results without proof:\n\nThe continuous mapping theorem.\nThe law of large numbers and central limit theorems for independent random variables…\n\nFor univariate random variables,\nFor multivariate random variables,\nFor identically distributed random variables, and\nFor non-identically distributed random variables.\n\nThe OLS estimator is given by \\(\\betavhat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\\) when \\(\\X\\) is full-rank.\nThe OLS estimator satisfies \\((\\X^\\trans \\X) \\betavhat = \\X^\\trans \\Y\\).\n\n\n\n1 Question 1 \\(\\hspace{4em}\\bigcirc \\leftarrow\\) ‘X’ here to grade this question.\nFor this question, we’ll consider the linear model \\(y_n = \\beta_0 + \\beta_1 \\w_n + \\beta_2 \\z_n + \\res_n\\).\n\n\n(1a)\nWrite the set of equations\n\\[\ny_n = \\beta_0 + \\beta_1 \\w_n + \\beta_2 \\z_n + \\res_n\n\\quad\\textrm{for }n \\in \\{1, \\ldots, N\\}\n\\]\nin matrix form. That is, let \\(\\X\\) denote an \\(N \\times 3\\) matrix, \\(\\Y\\) and \\(\\resv\\) length–\\(N\\) column vectors, and \\(\\betav= (\\beta_0, \\beta_1, \\beta_2)^\\trans\\) a length–\\(3\\) column vector. Then express the matrices \\(\\Y\\), \\(\\X\\), and \\(\\resv\\) in terms of the scalars \\(\\y_n\\), \\(\\w_n\\), \\(\\z_n\\), and \\(\\res_n\\) so that \\(\\Y= \\X\\betav+ \\resv\\) is equivalent to the set of regression equations.\n\n\n\n(1b)\nDefine the following quantities: \\[\n\\begin{aligned}\n    \\overline{z} :=& \\meann \\z_n &\n    \\overline{w} :=& \\meann \\w_n &\\\\\n    \\overline{zz} :=& \\meann \\z_n^2 &\n    \\overline{ww} :=& \\meann \\w_n^2 &\n    \\overline{zw} :=& \\meann \\z_n \\w_n & \\\\\n    \\overline{zy} :=& \\meann \\z_n \\y_n &\n    \\overline{wy} :=& \\meann \\w_n \\y_n &\n    \\overline{y} :=& \\meann \\y_n \\\\\n\\end{aligned}\n\\]\nWrite an explict expressions for \\(\\frac{1}{N} \\X^\\trans\\X\\) and \\(\\frac{1}{N} \\X^\\trans\\Y\\) in terms of these quantities.\n\n\n\n(1c)\nFor this part of the question only, assume that\n\\[\\overline{zw} = \\overline{z} = \\overline{w} = 0\\].\nIn terms of the quantites defined in part (b), write a closed–form expression for \\(\\betavhat\\), the OLS estimator of the vector \\(\\betav\\).\nHint: The inverse of a \\(3\\times 3\\) diagonal matrix is given by\n\\[\n\\begin{pmatrix}\nv_1 & 0 & 0 \\\\\n0 & v_2 & 0 \\\\\n0 & 0 & v_3 \\\\\n\\end{pmatrix}^{-1} =\n\\begin{pmatrix}\nv_1^{-1} & 0 & 0 \\\\\n0 & v_2^{-1} & 0 \\\\\n0 & 0 & v_3^{-1} \\\\\n\\end{pmatrix}\n\\]\n\n\n\n\n2 Question 2 \\(\\hspace{4em}\\bigcirc \\leftarrow\\) ‘X’ here to grade this question.\nFor this question, consider the linear models\n\\[\n\\begin{aligned}\ny_n ={} \\betav^\\trans \\xv_n + \\res_n\n&\\quad\\textrm{and}\\quad\ny_n ={} \\gammav^\\trans \\zv_n + \\eta_n\n\\end{aligned}\n\\]\nwith\n\\[\n\\begin{aligned}\n\\xv_n ={}\n\\begin{pmatrix}\n1 \\\\ \\x_n\n\\end{pmatrix}\n&\\quad\\textrm{and}\\quad\n\\zv_n ={} \\begin{pmatrix}\n1 \\\\ \\z_n\n\\end{pmatrix}\n\\textrm{ where }\n\\z_n :={} 10 \\x_n\n\\end{aligned}\n\\]\nAssume that \\(\\x_n\\) is not a constant (i.e., for at least one pair \\(n\\) and \\(m\\), \\(\\x_n \\ne \\x_m\\).).\nLet \\(\\X\\) denote the \\(N \\times 2\\) matrix whose \\(n\\)–th row is \\(\\xv_n^\\trans\\), and \\(\\Z\\) denote the \\(N \\times 2\\) matrix whose \\(n\\)–th row is \\(\\zv_n^\\trans\\).\n\n\n(2a)\nUsing the definitions above, find a \\(2 \\times 2\\) matrix \\(\\A\\) such that \\(\\zv_n = \\A \\xv_n\\). Then, show that \\(\\Z = \\X \\A^\\trans\\) for the same matrix \\(\\A\\).\n\n\n\n(2b)\nSuppose you know that the OLS estimate of \\(\\betav\\) is given by \\(\\betavhat = (4, 30)\\). What is the value of \\(\\gammavhat\\), the OLS estimate of \\(\\gammav\\)? Please use the definitions above and your answer for part (a), and justify your answer.\n\n\n\n(2c)\nNow consider the two models’ prediction on a new datapoint with \\(\\xv_\\new = (1, 50)^\\trans\\) — and so, necessarily, \\(\\zv_\\new = (1, 500)^\\trans\\) — with respective prediction errors\n\\[\n\\res_\\new^\\beta := \\y_\\new - \\betavhat^\\trans \\xv_\\new\n\\quad\\textrm{and}\\quad\n\\res_\\new^\\gamma := \\y_\\new - \\gammavhat^\\trans \\zv_\\new.\n\\]\nPlease select which of (a), (b), (c), or (d) is correct for this particular value of \\(\\xv_\\new\\) and \\(\\zv_\\new\\):\n\nIt is always the case that \\(\\abs{\\res_\\new^\\beta} = \\abs{\\res_\\new^\\gamma}\\)\nIt is always the case that \\(\\abs{\\res_\\new^\\beta} &lt; \\abs{\\res_\\new^\\gamma}\\)\nIt is always the case that \\(\\abs{\\res_\\new^\\beta} &gt; \\abs{\\res_\\new^\\gamma}\\)\nIn general, we cannot determine the relationship between \\(\\abs{\\res_\\new^\\beta}\\) and \\(\\abs{\\res_\\new^\\gamma}\\) using the information provided.\n\nPlease justify your answer carefully.\n\n\n\n\n3 Question 3\\(\\hspace{4em}\\bigcirc \\leftarrow\\) ‘X’ here to grade this question.\nFor this question, assume that you have access to a function\n\\[\n\\Phi(z) = \\prob{\\RV{\\z} \\le z},\n\\]\nas well as its inverse,\n\\[\n\\Phi^{-1}(p) = z \\textrm{ such that }\\prob{\\RV{\\z} \\le z} = p,\n\\textrm{ for }p \\in (0,1),\n\\]\nwhere \\(\\RV{\\z} \\sim \\gauss{0,1}\\) denote a scalar-valued standard normal random variable.\n\n\n(3a)\nSuppose that \\(\\RV{\\y} \\sim \\gauss{\\mu, \\sigma^2}\\), where \\(\\mu\\) and \\(\\sigma\\) are known. Using only the functions \\(\\Phi(\\cdot)\\), \\(\\Phi^{-1}(\\cdot)\\), and the known quantities \\(\\mu\\), and \\(\\sigma\\), find find a quantity \\(\\a\\) such that\n\\[\n\\prob{\\RV{\\y} \\le \\a} = 0.90.\n\\]\nNote that \\(\\Phi(\\cdot)\\) is only for a standard normal random variable. Please do not assume that you have direct access to the distribution and quantile functions of generic normal random variables.\nPlease justify your answer carefully.\n\n\n\n(3b)\nNow, consider the OLS estimator under normal assumptions, so that\n\\[\n\\betavhat \\sim \\gauss{\\betav, \\sigma^2 (\\X^\\trans\\X)^{-1}}.\n\\]\nNote that \\(\\betavhat\\) and \\(\\betav\\) are \\(P\\)–dimensional vectors, \\(\\sigma\\) is a scalar, and \\((\\X^\\trans\\X)^{-1}\\) is a \\(P\\times P\\) matrix.\nAssume that \\(\\betav\\), \\(\\sigma\\), and \\(\\X\\) are all known. In terms of these quantities, find the distribution of \\(\\betavhat_1\\), the first component of \\(\\betavhat\\).\n\n\n\n(3c)\nCombining your answers from parts (a) and (b), find a scalar \\(b\\) such that\n\\[\n\\prob{\\betavhat_1 \\le b} = 0.90.\n\\]\n\n\n\n\n4 Question 4\\(\\hspace{4em}\\bigcirc \\leftarrow\\) ‘X’ here to grade this question.\nFor this question:\n\nLet \\(\\xv_n  = (1, \\z_{n})^\\trans\\), where \\(\\z_n \\sim \\gauss{0, 2}\\).\nAssume that \\(\\y_n = \\betav^\\trans \\xv_n + \\res_n\\) for each \\(n\\) and some \\(\\betav = (\\beta_1, \\beta_2)^\\trans\\), where \\(\\betav\\) is a length 2 column vector.\nAssume the residuals \\(\\res_n\\) are IID with \\(\\expect{\\res_n} = 0\\) and \\(\\expect{\\res_n^2} = 1\\), but not necessarily normal.\nAssume that the residuals \\(\\res_n\\) are all independent of all the \\(\\z_n\\).\n\n\n\n(4a)\nLet \\(\\X\\) denote the \\(N\\times P\\) matrix consisting of the observation \\(\\xv_n^\\trans\\) in the \\(n\\)–th row, and let \\(\\Y\\) denote the \\(N\\)–vector with \\(\\y_n\\) in the \\(n\\)–th entry.\nWrite the matrices \\(\\frac{1}{N} \\X^\\trans \\X\\) and \\(\\frac{1}{N} \\X^\\trans \\Y\\) in terms of \\(\\betav\\), \\(\\res_n\\), \\(\\z_n\\), \\(N\\), and constants. Note that \\(\\X^\\trans \\X\\) is a \\(2 \\times 2\\) matrix and \\(\\X^\\trans \\Y\\) is a \\(2\\)–vector.\n\n\n\n(4b)\nEvaluate the following limits (which will denote convergence in probability):\n\\[\n\\frac{1}{N} \\X^\\trans \\X \\rightarrow ?\n\\quad\\quad\\textrm{and}\\quad\\quad\n\\frac{1}{N} \\X^\\trans \\Y  \\rightarrow ? \\\\\n\\]\nNote that your answers may depend on \\(\\betav\\), but should not depend on \\(\\xv_n\\) or \\(\\y_n\\), since they are limiting quantites that do not depend on the particular dataset.\nJustify your conclusion carefully (state which theorems you use).\n\n\n\n(4c)\nUsing your answers from part (a) and (b), find the limit (convergence in probability) of\n\\[\n\\betavhat = \\left(\\X^\\trans \\X \\right)^{-1}  \\X^\\trans \\Y \\rightarrow ?\n\\]\nNote that your answer may depend on \\(\\betav\\), but should not depend on \\(\\xv_n\\) or \\(\\y_n\\), since they are limiting quantites that do not depend on the particular dataset.\nJustify your conclusion carefully (state which theorems you use).\n\n\n\n\n5 Question 5\\(\\hspace{4em}\\bigcirc \\leftarrow\\) ‘X’ here to grade this question.\nLet \\(\\RV{\\xv}_n\\) denote an IID sequence of random \\(2\\)–dimensional vectors in \\(\\rdom{P}\\) (not necessarily normal), with\n\\[\n\\RV{\\xv_n} =\n\\begin{pmatrix}\n\\RV{\\x}_{n1} \\\\ \\RV{\\x}_{n2}\n\\end{pmatrix}\n\\quad\\quad\\textrm{and}\\quad\\quad\n\\expect{\\RV{\\xv}_n} = \\zerov\n\\quad\\quad\\textrm{and}\\quad\\quad\n\\cov{\\RV{\\xv}_n} =:\n\\Sigmam =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 4\n\\end{pmatrix}.\n\\]\n\n\n(5a)\nFind the limiting distribution of the vector\n\\[\n\\begin{aligned}\n\\frac{1}{\\sqrt{N}} \\sumn \\RV{\\xv}_n \\rightarrow ?\n\\end{aligned}\n\\]\nJustify your conclusion carefully.\n\n\n\n(5b)\nUsing the univariate central limit theorem, find the limiting distributions of the difference between the components of \\(\\RV{\\xv_n}\\):\n\\[\n\\begin{aligned}\n\\frac{1}{\\sqrt{N}} \\sumn \\left(\\RV{\\x}_{n1} - \\RV{\\x}_{n2}\\right) \\rightarrow ?\n\\end{aligned}\n\\]\nJustify your conclusion carefully.\n\n\n\n(5c)\nFind a vector \\(\\vv\\) such that \\(\\vv^\\trans \\RV{\\xv}_n = \\RV{\\xv}_{n1} - \\RV{\\xv}_{n2}\\). Using this vector, show that the solution to (b) also follows from the solution to (a) and the continuous mapping theorem.\nJustify your conclusion carefully.\n\n\n\n\n6 Question 6\\(\\hspace{4em}\\bigcirc \\leftarrow\\) ‘X’ here to grade this question.\nGiven a regression on \\(\\X\\) with \\(P\\) regressors, and the corresponding \\(\\Y\\), \\(\\Yhat\\), and \\(\\reshat\\), define the following quantities: \\[\n\\begin{aligned}\nRSS :={}& \\resvhat^\\trans \\resvhat & \\textrm{(Residual sum of squares)}\\\\\nTSS :={}& \\Y^\\trans \\Y & \\textrm{(Total sum of squares)}\\\\\nESS :={}& \\Yhat^\\trans \\Yhat & \\textrm{(Explained sum of squares)}\\\\\nR^2 :={}& \\frac{ESS}{TSS}.\n\\end{aligned}\n\\]\n\n\n6a\n\nProve that \\(RSS + ESS = TSS\\).\nExpress \\(R^2\\) in terms of \\(TSS\\) and \\(RSS\\).\n\n\n\n\n6b\nFor each of the following questions, please justify your answer. An intuitive explanation is enough; a proof or specific example is not necessary.\n\nWhat is \\(R^2\\) when we include no regressors? (\\(P = 0\\) and \\(\\yhat_n = 0\\) for all \\(n\\))\nWhat is \\(R^2\\) when we include \\(N\\) linearly independent regressors? (\\(P=N\\))\nCan \\(R^2\\) ever decrease when we add a regressor?\nCan \\(R^2\\) ever stay the same when we add a regressor?\nCan \\(R^2\\) ever increase when we add a regressor?\n\n\n\n\n6c\nFor each of the following questions, please justify your answer. An intuitive explanation is enough; a proof or specific example is not necessary.\nThese questions will be about the F-test statistic for the null \\(H_0: \\betav = \\zerov\\),\n\\[\n\\phi = \\frac{\\betahat^\\trans (\\X^\\trans \\X) \\betahat}{P \\sigmahat^2},\n\\]\nwhere \\(\\sigmahat^2 := \\frac{1}{N-P} \\sumn \\reshat_n^2\\).\n\nWrite the F-test statistic \\(\\phi\\) in terms of \\(TSS\\) and \\(RSS\\), and \\(P\\).\nCan \\(\\phi\\) ever decrease when we add a regressor?\nCan \\(\\phi\\) ever stay the same when we add a regressor?\nCan \\(\\phi\\) ever increase when we add a regressor?\n\n\n\n\n\n7 Question 7\\(\\hspace{4em}\\bigcirc \\leftarrow\\) ‘X’ here to grade this question.\nFor this question, we will take\n\\[\n\\a_n \\sim \\gauss{0, 1} \\quad\\textrm{ and }\\quad\nb_n = \\a_n^3.\n\\]\nWe assume the pairs \\((a_n, b_n)\\) are IID, but \\(a_n\\) and \\(b_n\\) are not independent. Assume that, for some \\(\\beta_a\\) and \\(\\beta_b\\),\n\\[\n\\y_n = \\beta_a \\a_n + \\beta_b b_n + \\res_n,\n\\]\nwhere \\(\\res_n\\) are IID with \\(\\expect{\\res_n} = 0\\) and \\(\\var{\\res_n} &lt; \\infty\\). The residuals \\(\\res_n\\) and \\(\\a_n\\) are all independent of one another. Note that the residuals are not necessarily normal.\n\n\n(7a)\nLet \\(\\alphahat\\) denote the OLS estimator of \\(\\y_n \\sim \\alpha a_n\\), that is, of \\(y_n\\) regressed on \\(a_n\\) alone. Note that the regression for \\(\\alphahat\\) does not include a constant, and does not include \\(b_n\\).\nRecall that\n\\[\n\\alphahat = \\frac{\\sumn \\y_n \\a_n}{\\sumn \\a_n^2},\n\\]\nand find the limit\n\\[\n\\alphahat \\rightarrow ?\n\\]\nas \\(N\\rightarrow \\infty\\).\nThe answer may depend on the unknown \\(\\beta_a\\) and \\(\\beta_b\\).\nHint: Standard properties of the normal gives that \\(\\expect{\\a_n^3} = 0\\) and \\(\\expect{\\a_n^4} = 3\\).\n\n\n\n(7b)\nLetting \\(\\yhat_\\new = \\alphahat \\a_\\new\\), find\n\\[\n\\expect{\\y_\\new - \\yhat_\\new \\vert \\a_\\new, \\Y, \\A},\n\\]\nwhere \\(\\A = (\\a_1, \\ldots, \\a_N)^\\trans\\) is the vector of \\(\\a_n\\) observations and \\(\\Y = (\\y_1, \\ldots, \\y_N)^\\trans\\) is the vector of responses in the training set. Note that the expectation is conditional on the training data and on the new regressor, so the only randomness is in \\(\\res_\\new\\).\nThe answer may depend on the unknown \\(\\beta_a\\) and \\(\\beta_b\\).\n\n\n\n(7c)\nAssume that \\(\\beta_b \\ne 0\\), that \\(N\\) is very large.\n\nWhat does your result from (a) imply about using \\(\\alphahat\\) for inference on \\(\\beta_a\\)?\nWhat does your result from (b) imply about using \\(\\alphahat\\) for prediction of \\(\\y_\\new\\)?"
  },
  {
    "objectID": "quizzes/cd882921d17da95eab20d193b962fcae.html",
    "href": "quizzes/cd882921d17da95eab20d193b962fcae.html",
    "title": "STAT151A Quiz 2.5 (Feb 22nd)",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[1in]\\]\nFor this quiz, we’ll consider the linear models\n\\[\n\\begin{aligned}\ny_n ={} \\betav^\\trans \\xv_n + \\res_n\n&\\quad\\textrm{and}\\quad\ny_n ={} \\gammav^\\trans \\zv_n + \\eta_n\n\\end{aligned}\n\\]\nwith\n\\[\n\\begin{aligned}\n\\xv_n ={} (\\x_{n1}, \\x_{n2})^\\trans\n&\\quad\\textrm{and}\\quad\n\\zv_n ={} (\\z_{n1}, \\z_{n2})^\\trans \\textrm{ where}\n\\\\\n\\z_{n1} :={} \\x_{n1} - \\x_{n2}\n&\\quad\\textrm{and}\\quad\n\\z_{n2} :={} \\x_{n1} + \\x_{n2}\n\\end{aligned}\n\\]\nLet \\(\\X\\) denote the \\(N \\times 2\\) matrix whose \\(n\\)–th row is \\(\\xv_n^\\trans\\), and \\(\\Z\\) denote the \\(N \\times 2\\) matrix whose \\(n\\)–th row is \\(\\zv_n^\\trans\\).\nAssume that the matrix \\(\\X\\) is full rank.\nRecall that the inverse of a 2x2 matrix is given by\n\\[\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}^{-1} =\n\\frac{1}{ad - bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}.\n\\]\nYou have 20 minutes for this quiz.\nThere are three parts, (a), (b), and (c), each weighted equally..\n\n\n(a)\nUsing the definitions given on the first page, find a \\(2 \\times 2\\) matrix \\(\\A\\) such that \\(\\Z = \\X \\A\\).\n\n\n\n(b)\nUse the definitions given on the first page of the quiz. Suppose I tell you that the OLS estimate of \\(\\beta\\) is given by \\(\\betahat = (2, 4)\\). What is the value of \\(\\gammahat\\), the OLS estimate of \\(\\gamma\\)?\n\n\n\n(c)\nUse the definitions given on the first page of the quiz. Consider the residual variance estimators for the two regressions:\n\\[\n\\sigmahat^2_\\x := \\meann \\hat\\res_n^2 = \\meann (\\y_n - \\xv_n^\\trans \\betahat)^2\n\\quad\\textrm{and}\\quad\n\\sigmahat^2_\\z := \\meann \\hat\\eta_n^2 = \\meann (\\y_n - \\zv_n^\\trans \\gammahat)^2.\n\\]\nPlease select which of (a), (b), (c), or (d) is correct:\n\nIt is always the case that \\(\\sigmahat^2_\\x &gt; \\sigmahat^2_\\z\\)\nIt is always the case that \\(\\sigmahat^2_\\x &lt; \\sigmahat^2_\\z\\)\nIt is always the case that \\(\\sigmahat^2_\\x = \\sigmahat^2_\\z\\)\nIn general, we cannot determine the relationship between \\(\\sigmahat^2_\\x\\) and \\(\\sigmahat^2_\\z\\) using the information provided.\n\nBriefly justify your answer."
  },
  {
    "objectID": "quizzes/40100a05c0473d483bb078019ade4aee.html",
    "href": "quizzes/40100a05c0473d483bb078019ade4aee.html",
    "title": "STAT151A Quiz 3 (Feb 27th)",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[1in]\\]\nFor this quiz, assume that you have access to a function\n\\[\n\\Phi(z) = \\prob{\\RV{\\z} \\le z},\n\\]\nas well as its inverse,\n\\[\n\\Phi^{-1}(p) = z \\textrm{ such that }\\prob{\\RV{\\z} \\le z} = p,\n\\textrm{ for }p \\in [0,1],\n\\]\nwhere \\(\\RV{\\z} \\sim \\gauss{0,1}\\) denote a scalar-valued standard normal random variable.\nYou have 20 minutes for this quiz.\nThere are three parts, (a), (b), and (c), each weighted equally..\n\n\n(a)\nSuppose that \\(\\RV{\\y} \\sim \\gauss{\\mu, \\sigma^2}\\), where \\(\\mu\\) and \\(\\sigma\\) are known. Using only the functions \\(\\Phi(\\cdot)\\), \\(\\Phi^{-1}(\\cdot)\\), and the known quantities \\(\\mu\\), and \\(\\sigma\\), find find a quantity \\(\\a\\) such that\n\\[\n\\prob{\\RV{\\y} \\le \\a} = 0.90.\n\\]\nNote that \\(\\Phi(\\cdot)\\) is only for a standard normal random variable. Please do not assume that you have direct access to the distribution and quantile functions of generic normal random variables.\nPlease justify your answer carefully.\n\n\n\n(b)\nIn the same setting as (a), please find \\(b_\\ell\\) and \\(b_u\\) such that\n\\[\n\\prob{b_\\ell \\le \\RV{\\y} \\le b_u} = 0.80.\n\\]\nNote the change from \\(0.90\\) in (a) to \\(0.80\\) on the right-hand side in the current problem.\nPlease justify your answer carefully. You may use your result from part (a).\n\n\n\n(c)\nNow suppose that \\(\\xv_\\new \\in \\rdom{P}\\), \\(\\betav\\in \\rdom{P}\\), and \\(\\sigma \\in \\rdom{}\\) all denote known, non-random quantites.\nSuppose that you also know that \\(\\y_\\new = \\betav^\\trans \\xv_\\new + \\res_n\\), where \\(\\res_n \\sim \\gauss{0, \\sigma^2}\\).\nUsing only these known quantities and the functions \\(\\Phi(\\cdot)\\), \\(\\Phi^{-1}(\\cdot)\\), find \\(c_\\ell\\) and \\(c_u\\) such that\n\\[\n\\prob{c_\\ell \\le \\y_\\new \\le c_u} = 0.80.\n\\]\nPlease justify your answer carefully. You may use your results from parts (a) and (b)."
  },
  {
    "objectID": "quizzes/quiz4_retake.html",
    "href": "quizzes/quiz4_retake.html",
    "title": "STAT151A Quiz 4 retake (Due 9pm Mar 22nd)",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[1in]\\]\nThis question will take the residuals of the training data to be random, and will consider variablity under sampling of the training data. The regressors for both the training data and test data will be taken as fixed.\nThe inverse of a 2x2 matrix is given by \\[\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}^{-1} =\n\\frac{1}{ad - bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}.\n\\]\nThe OLS estimator is given by \\(\\betahat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\\).\nYou have 20 minutes for this quiz.\nThere are three parts, (a), (b), and (c), each weighted equally..\n\n\n(a)\nFor this quiz, we will assume that \\(\\y_n = \\beta^\\trans \\xv_n + \\res_n\\) for some \\(\\beta\\), and that the residuals \\(\\res_n\\) are IID with \\(\\expect{\\res_n} = 0\\) and \\(\\expect{\\res_n^2} = 1\\), but not necessarily normal.\nLet \\(\\xv_n  = (1, \\z_{n})^\\trans\\), where \\(\\meann \\z_n = 0\\) and \\(\\meann \\z_n^2 = \\delta &gt; 0\\). That is, assume we are regressing on a constant and a single mean-zero regressor. For this question, take the regressors to be fixed (not random).\nFind the limiting distribution of \\(\\sqrt{N}(\\betahat - \\beta)\\) as \\(N \\rightarrow \\infty\\).\n\n\n\n(b)\nDefine the expected prediction error \\[\n\\begin{aligned}\n\\y_\\new :={} \\beta^\\trans \\xv_{\\new} + \\res_\\new\n\\quad\\textrm{and}\\quad\n\\yhat_\\new :={}  \\betahat^\\trans \\xv_\\new.\n\\end{aligned}\n\\]\nUnder the conditions given in part (a), find the limiting distribution of\n\\[\n\\sqrt{N} (\\yhat_\\new - \\expect{\\y_\\new} )\n\\]\nas \\(N \\rightarrow \\infty\\), as a function of \\(\\xv_\\new\\). That is, the limiting distribution will depend on \\(\\xv_\\new\\), so please make the dependence explicit.\nYou may use your answer from part (a).\n\n\n\n(c)\nAssume the conditions and definitions given in (a) and (b). Assume that \\(\\delta \\ll 1\\) (that is, \\(\\delta\\) is much smaller than \\(1\\).)\nFind the limiting distribution of \\(\\sqrt{N} (\\yhat_\\new - \\expect{\\y_\\new} )\\) when\n\n\\(\\xv_\\new = (1, 0)\\) and\n\\(\\xv_\\new = (1, 1)\\).\n\nWhich of the two is larger?\nYou may use your answer from parts (a) and (b)."
  },
  {
    "objectID": "quizzes/0f4b266f30d33cf873a17296e0481040b7aa0b15c06513c916e9ae5a2e5d0fc5.html",
    "href": "quizzes/0f4b266f30d33cf873a17296e0481040b7aa0b15c06513c916e9ae5a2e5d0fc5.html",
    "title": "STAT151A Quiz 5 (Apr 23rd)",
    "section": "",
    "text": "$$\n\n\\newcommand{\\mybold}[1]{\\boldsymbol{#1}} \n\n\n\\newcommand{\\trans}{\\intercal}\n\\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert}\n\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\newcommand{\\bbr}{\\mathbb{R}}\n\\newcommand{\\bbz}{\\mathbb{Z}}\n\\newcommand{\\bbc}{\\mathbb{C}}\n\\newcommand{\\gauss}[1]{\\mathcal{N}\\left(#1\\right)}\n\\newcommand{\\chisq}[1]{\\mathcal{\\chi}^2_{#1}}\n\\newcommand{\\studentt}[1]{\\mathrm{StudentT}_{#1}}\n\\newcommand{\\fdist}[2]{\\mathrm{FDist}_{#1,#2}}\n\n\\newcommand{\\argmin}[1]{\\underset{#1}{\\mathrm{argmin}}\\,}\n\\newcommand{\\projop}[1]{\\underset{#1}{\\mathrm{Proj}}\\,}\n\\newcommand{\\proj}[1]{\\underset{#1}{\\mybold{P}}}\n\\newcommand{\\expect}[1]{\\mathbb{E}\\left[#1\\right]}\n\\newcommand{\\prob}[1]{\\mathbb{P}\\left(#1\\right)}\n\\newcommand{\\dens}[1]{\\mathit{p}\\left(#1\\right)}\n\\newcommand{\\var}[1]{\\mathrm{Var}\\left(#1\\right)}\n\\newcommand{\\cov}[1]{\\mathrm{Cov}\\left(#1\\right)}\n\\newcommand{\\sumn}{\\sum_{n=1}^N}\n\\newcommand{\\meann}{\\frac{1}{N} \\sumn}\n\\newcommand{\\cltn}{\\frac{1}{\\sqrt{N}} \\sumn}\n\n\\newcommand{\\trace}[1]{\\mathrm{trace}\\left(#1\\right)}\n\\newcommand{\\diag}[1]{\\mathrm{Diag}\\left(#1\\right)}\n\\newcommand{\\grad}[2]{\\nabla_{#1} \\left. #2 \\right.}\n\\newcommand{\\gradat}[3]{\\nabla_{#1} \\left. #2 \\right|_{#3}}\n\\newcommand{\\fracat}[3]{\\left. \\frac{#1}{#2} \\right|_{#3}}\n\n\n\\newcommand{\\W}{\\mybold{W}}\n\\newcommand{\\w}{w}\n\\newcommand{\\wbar}{\\bar{w}}\n\\newcommand{\\wv}{\\mybold{w}}\n\n\\newcommand{\\X}{\\mybold{X}}\n\\newcommand{\\x}{x}\n\\newcommand{\\xbar}{\\bar{x}}\n\\newcommand{\\xv}{\\mybold{x}}\n\\newcommand{\\Xcov}{\\Sigmam_{\\X}}\n\\newcommand{\\Xcovhat}{\\hat{\\Sigmam}_{\\X}}\n\\newcommand{\\Covsand}{\\Sigmam_{\\mathrm{sand}}}\n\\newcommand{\\Covsandhat}{\\hat{\\Sigmam}_{\\mathrm{sand}}}\n\n\\newcommand{\\Z}{\\mybold{Z}}\n\\newcommand{\\z}{z}\n\\newcommand{\\zv}{\\mybold{z}}\n\\newcommand{\\zbar}{\\bar{z}}\n\n\\newcommand{\\Y}{\\mybold{Y}}\n\\newcommand{\\Yhat}{\\hat{\\Y}}\n\\newcommand{\\y}{y}\n\\newcommand{\\yv}{\\mybold{y}}\n\\newcommand{\\yhat}{\\hat{\\y}}\n\\newcommand{\\ybar}{\\bar{y}}\n\n\\newcommand{\\res}{\\varepsilon}\n\\newcommand{\\resv}{\\mybold{\\res}}\n\\newcommand{\\resvhat}{\\hat{\\mybold{\\res}}}\n\\newcommand{\\reshat}{\\hat{\\res}}\n\n\\newcommand{\\betav}{\\mybold{\\beta}}\n\\newcommand{\\betavhat}{\\hat{\\betav}}\n\\newcommand{\\betahat}{\\hat{\\beta}}\n\\newcommand{\\betastar}{{\\beta^{*}}}\n\n\\newcommand{\\bv}{\\mybold{\\b}}\n\\newcommand{\\bvhat}{\\hat{\\bv}}\n\n\\newcommand{\\alphav}{\\mybold{\\alpha}}\n\\newcommand{\\alphavhat}{\\hat{\\av}}\n\\newcommand{\\alphahat}{\\hat{\\alpha}}\n\n\\newcommand{\\omegav}{\\mybold{\\omega}}\n\n\\newcommand{\\gv}{\\mybold{\\gamma}}\n\\newcommand{\\gvhat}{\\hat{\\gv}}\n\\newcommand{\\ghat}{\\hat{\\gamma}}\n\n\\newcommand{\\hv}{\\mybold{\\h}}\n\\newcommand{\\hvhat}{\\hat{\\hv}}\n\\newcommand{\\hhat}{\\hat{\\h}}\n\n\\newcommand{\\gammav}{\\mybold{\\gamma}}\n\\newcommand{\\gammavhat}{\\hat{\\gammav}}\n\\newcommand{\\gammahat}{\\hat{\\gamma}}\n\n\\newcommand{\\new}{\\mathrm{new}}\n\\newcommand{\\zerov}{\\mybold{0}}\n\\newcommand{\\onev}{\\mybold{1}}\n\\newcommand{\\id}{\\mybold{I}}\n\n\\newcommand{\\sigmahat}{\\hat{\\sigma}}\n\n\n\\newcommand{\\etav}{\\mybold{\\eta}}\n\\newcommand{\\muv}{\\mybold{\\mu}}\n\\newcommand{\\Sigmam}{\\mybold{\\Sigma}}\n\n\\newcommand{\\rdom}[1]{\\mathbb{R}^{#1}}\n\n\\newcommand{\\RV}[1]{\\tilde{#1}}\n\n\n\n\\def\\A{\\mybold{A}}\n\n\\def\\A{\\mybold{A}}\n\\def\\av{\\mybold{a}}\n\\def\\a{a}\n\n\\def\\B{\\mybold{B}}\n\\def\\b{b}\n\n\n\\def\\S{\\mybold{S}}\n\\def\\sv{\\mybold{s}}\n\\def\\s{s}\n\n\\def\\R{\\mybold{R}}\n\\def\\rv{\\mybold{r}}\n\\def\\r{r}\n\n\\def\\V{\\mybold{V}}\n\\def\\vv{\\mybold{v}}\n\\def\\v{v}\n\n\\def\\U{\\mybold{U}}\n\\def\\uv{\\mybold{u}}\n\\def\\u{u}\n\n\\def\\W{\\mybold{W}}\n\\def\\wv{\\mybold{w}}\n\\def\\w{w}\n\n\\def\\tv{\\mybold{t}}\n\\def\\t{t}\n\n\\def\\Sc{\\mathcal{S}}\n\\def\\ev{\\mybold{e}}\n\n\\def\\Lammat{\\mybold{\\Lambda}}\n\n\n$$\n\n\n\n\nPlease write your full name and email address:\n\\[\\\\[1in]\\]\nThe OLS estimator is given by \\(\\betahat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\\).\nYou have 20 minutes for this quiz.\nThere are three parts, (a), (b), and (c), each weighted equally..\nThis quiz will use some facts about the standard normal distribution. If \\(z \\sim \\gauss{0, 1}\\), then:\n\n\\(\\expect{z} = 0\\)\n\\(\\expect{z^2} = 1\\)\n\\(\\expect{z^3} = 0\\)\n\\(\\expect{z^4} = 3\\)\n\nRecall that the OLS estimator of \\(\\y_n \\sim \\beta^\\trans \\xv_n\\) is \\(\\betavhat = (\\X^\\trans \\X)^{-1} \\X^\\trans \\Y\\).\n\nFor this quiz, we will take\n\\[\n\\a_n \\sim \\gauss{0, 1} \\textrm{ IID} \\quad\\textrm{and}\\quad\nb_n = \\a_n^2.\n\\]\nNote that the pairs \\((a_n, b_n)\\) are IID, but \\(a_n\\) and \\(b_n\\) are not independent.\nLet \\(\\xv_n = (a_n, b_n)^\\trans\\). Let \\(\\betav = (\\beta_a, \\beta_b)^\\trans\\). Let \\(\\y_n = \\betav^\\trans \\xv_n + \\res_n\\), where \\(\\res_n\\) are IID with \\(\\expect{\\res_n} = 0\\) and \\(\\var{\\res_n} &lt; \\infty\\). The residuals \\(\\res_n\\) and \\(\\a_n\\) are all independent of one another.\nNote that \\(a_n\\), \\(b_n\\), \\(\\beta_a\\), and \\(\\beta_b\\) are all scalars, and both \\(\\xv_n\\) and \\(\\betav\\) are \\(2\\)–vectors.\nLet \\(\\betavhat\\) denote the OLS estimator of \\(\\y_n \\sim \\betav^\\trans \\xv_n\\), that is, of \\(y_n\\) regressed on both \\(a_n\\) and \\(b_n\\). Note that the regression for \\(\\betavhat\\) does not include a constant.\nLet \\(\\alphahat\\) denote the OLS estimator of \\(\\y_n \\sim \\alpha a_n\\), that is, of \\(y_n\\) regressed on \\(a_n\\) alone. Note that the regression for \\(\\alphahat\\) does not include a constant, and does not include \\(b_n\\).\n\n\n(a)\nProve that, as \\(N \\rightarrow \\infty\\),\n\n\\(\\alphahat \\rightarrow \\beta_a\\) and\n\\(\\betavhat \\rightarrow \\betav = \\begin{pmatrix}\\beta_a \\\\ \\beta_b \\end{pmatrix}\\).\n\n\n\n\n(b)\nFor part (b), let \\(\\yhat_\\new = \\beta_a a_\\new\\), and assume that \\(\\beta_b \\ne 0\\). Note that \\(\\yhat_\\new\\) is formed with \\(\\beta_a\\), not \\(\\alphahat\\). Prove that\n\\[\n\\expect{\\y_\\new - \\yhat_\\new} \\ne 0,\n\\]\nwhere the expectation is taken over \\(a_\\new\\), \\(b_\\new\\), and \\(\\res_\\new\\). That is, when you exclude \\(b_n\\) from the regression, the predictions evaluted at the limit \\(\\beta_a\\) are biased.\n\n\n\n(c)\nFor part (c), let \\(\\yhat_\\new' = \\betav^\\trans \\xv_\\new\\). Note that \\(\\yhat_\\new'\\) is formed with \\(\\betav\\), not \\(\\betavhat\\). Prove that\n\\[\n\\expect{\\y_\\new - \\yhat'_\\new} = 0,\n\\]\nwhere the expectation is taken over \\(a_\\new\\), \\(b_\\new\\), and \\(\\res_\\new\\). That is, when you include \\(b_n\\) from the regression, the predictions evaluated at the limit \\(\\betav\\) are unbiased."
  }
]